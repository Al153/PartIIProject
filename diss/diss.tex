% Template for a Computer Science Tripos Part II project dissertation
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{breqn}
\usepackage{float}
\usepackage{framed}

\usepackage{natbib}


\usepackage{svg}
\usepackage{booktabs}

%Highlighter commands. most need to be properly defined.
\newcommand\todo[1]{\textcolor{red}{#1}}
\newcommand\codeName[1]{\texttt{#1}}
\newcommand\term[1]{\textcolor{purple}{#1}}
\newcommand\crossRefNeeded[1]{\textcolor{green}{#1}}
\newcommand\note[1]{\textcolor{cyan}{#1}}
\newcommand\mathName[1]{\textcolor{pink}{#1}}

\newcommand{\reference}{LMDBOptimised}
\newcommand{\batched}{LMDBatched}
\newcommand{\cse}{LMDBCSE}
\newcommand{\postgres}{Postgres}

\newcommand\resultTable[4]{
\begin{center}
	\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|  }
 	\hline
 		Name & \multicolumn{2}{|c|}{#1} \\
 	\hline
 		Iterations & \multicolumn{2}{|c|}{#2} \\
 	\hline
 		Queries & \multicolumn{2}{|c|}{
 		\begin{tabular}{@{}c@{}}
			#3 		
 		\end{tabular}
 	} \\ 	\hline
 		implementation   	& 	time (ms) & /optimised \\ \hline
		#4
 	\hline
	\end{tabular}
\end{center}

}

\newcommand\either[0]{\textbackslash/}
\newcommand\eitherR[0]{\textbackslash/-}
\newcommand\eitherL[0]{-\textbackslash/}

\newcommand{\db}[1]{{\bf [\![}#1{\bf ]\!]}}
\newcommand{\deno}[1]{\db{#1}(v)}
\newcommand{\setComp}[2]{\left\lbrace #1 \mid #2 \right\rbrace}
\newcommand{\clos}[0]{closure(A, v)}
\newcommand{\typeRule}[2]{\Sigma\vdash #1 \colon #2}
\newcommand{\denoRule}[2]{#1 \in \deno{#2}}
\newcommand{\opRule}[3]{#1 \triangleleft_{#2, v} #3}

\newcommand{\phiRule}[3]{\Phi(\Sigma, #1, #2, #3)}
\newcommand{\psiRule}[2]{\Psi(\Sigma, #1, #2)}
\newcommand{\query}[0]{Query(v)}
\newcommand{\queryT}[1]{Query_{#1}(v)}
\newcommand{\projectTitle}[0]{A Purely Functional Approach to Graph Queries on a Database}


\allowdisplaybreaks
{\renewcommand{\baselinestretch}{0.8}\small


\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\bibliographystyle{plain}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\rightline{\LARGE \textbf{Alexander Taylor}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{\projectTitle} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
St John's College \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{p{5cm}p{10cm}}
Name:               & \bf Alexander Taylor                       \\
College:            & \bf St John's College                     \\
Project Title:      & \bf \projectTitle  \\
Examination:        & \bf Computer Science Tripos -- Part II, June 2018  \\
Word Count:         & \bf   \\
Project Originator: & Adapted from a project proposal by Dr Eiko Yoneki                    \\
Supervisor:         & Dr Timothy Jones                    \\ 
\end{tabular}
}

\section*{Original Aims of the Project}

The initial aim of the project was to produce a simple graph database system which wrapped over an SQL database, to expose an API treating the database as a purely functional datastructure. Operations were to be achieved in a monadic fashion, according to well defined DSL semantics. The success criteria were that a user could write a composable query with the DSL and receive correct results with appropriate error checking.

\section*{Work Completed}

All success criteria were met and exceeded. I also implemented several extensions such as adding write functionality and and a collection of bespoke back-ends using the LMDB key-value store. These new back-ends were evaluated by comparing their against the compiled SQL queries generated by the SQL back-end.

\section*{Special Difficulties}

None.
 
\newpage
\section*{Declaration}

I, Alexander Taylor of St John's College, being a candidate for Part II of the Computer
Science Tripos, hereby declare
that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation
does not contain material that has already been used to any substantial
extent for a comparable purpose.

\bigskip
\leftline{Signed [signature]}

\medskip
\leftline{Date [date]}

\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}

\todo{Acknowledgements}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\pagestyle{headings}

\todo{storage space of views - same as a write ahead log?}\\
\todo{references between sections}\\
\todo{requirements analysis - is this required?}\\
\todo{rework of performance test analysis}\\

\chapter{Introduction}
In recent years, alternatives to the relational database model employed by SQL are receiving increasing attention. One such model for data is that of graph databases, which allow queries to pattern match over the graph formed by relations between objects in the the database. Another trend in the industry is a move towards languages with purely functional, strongly typed features. Fields which require the ability to quickly produce non-trivial, correct code have seen the use of high level languages such as Haskell, Ocaml, and Scala progress from a research interest to a serious paradigm. In these use cases, such as automated trading and safety critical systems, an advanced type system is used to  make stronger guarantees about the runtime behaviour of the program and enhances the ability to find bugs at compile time than in more traditional imperative languages.
	
	As the usage of functional languages for real-world code grows, there is an increasing need to integrate common patterns into the functional paradigm. Access to databases in one such example that requires solving. Frequently, when a functional program requires access to a database, such access is implemented ad-hoc for the specific usage case. For example, a program may be set up to read trade data from a database in a finance firm. This approach reduces generality and limits the kinds of abstractions that may be used. For example, the above usage case might only support specific schema required for the types representing the kinds trades that might appear; the wrapper used for the trading system could not be used to access a database containing a social network. In addition to this, while read only databases are trivially purely functional data-structures, it is harder to achieve referential transparency when writes are included into the database specification. This is because most database systems use imperative semantics to best make use of the hardware upon which they are implemented.

	There has been some exploration into generalised libraries and wrappers for accessing databases in a purely functional way. These may allow a means to construct SQL queries in a type-safe manner \cite{HaskellDB}, or provide manipulation of the database’s state in a referentially transparent manner \cite{DBStates}. However, they are typically aimed at relational (SQL) databases and do not allow safe generalisation over the types stored in the database.

In this dissertation, I introduce a general, performant, graph database system, written in Scala, that exposes its underlying graph as a purely functional data-structure. Furthermore, it allows for tighter integration into the client application by allowing the application programmer to store their own types in the database, hence reducing the need for marshalling and unmarshalling. Queries constructed using the library are type-checked by the Scala compiler at compile-time, and execute according to a concrete semantics. In addition to these features, the system is constructed in a modular fashion, allowing for the back-end implementation technologies to be interchanged and the domain specific query language to be extended. My initial goals were to build a read-only database with these features and a single back-end which translates the submitted queries to SQL. However, I have significantly exceeded these goals by adding referentially transparent write operations to the database and writing several variations on a back-end which stores the graph using the LMDB key-value datastore.

\chapter{Preparation}

This chapter discusses the early stages of the project. Once I had settled on a definition of type-safety and settled on using the Scala language for the project, I carried out a brief survey of graph database technologies to construct the language of queries that I intended to implement. Following this, I devised a set of semantics and constructed a system architecture.

\section{The Scala Programming Language}
I quickly decided to use Scala for this project. Scala is a JVM based object oriented and functional language. It provides a very advanced type system, allowing for complex abstractions to be built safely. Furthermore, due to its inter-operability with Java, the latter's libraries can be called from Scala, giving an advantage over other comparable languages such as Haskell and Ocaml, especially when there is a need to take advantage of low-level functionality. Finally, I was also familiar with some of Scala's more advanced features from previous experience.

\section{Definition of Type-Safety}
When we talk about type safety, we mean to say that a correctly typed program will not produce run-time errors of a given class. Breaches of  type safety in typical programs include exceptions (These are inherently not type safe in Scala, since exceptions are unchecked), the usage of primitive data types where specific values are expected, such as passing strings to functions without wrapping or tagging them in more specific types, and unmarshalling data from type-erased sources. If we do not know whether we should expect a value to be an integer or a string and we try to read it as an integer, there is a chance to throw an exception. 
By this definition, accessing an external, imperative database from Scala is not type-safe. The database will have some collection of error modes that often manifest as exceptions and typically stores data in a type-erased form that needs to be unmarshalled. Finally, queries to external database systems are typically constructed as a primitive string that cannot be checked automatically at compile-time. This project addresses these issues by using carefully constructed result container types to correctly handle error cases, including arbitrary exceptions using a pattern-match-able type hierarchy, a DSL that is type checked at compile-time by the Scala type checker, and safe, typeclass based marshalling.


\section{Existing Graph Databases}
This section gives a brief introduction into the main characteristics and classifications of existing graph databases.
\subsection{Classes of Database}
Existing graph databases typically fall into two classes: property graphs and edge-labelled (EL) graphs \cite{QueryLanguageFoundations}. EL graphs typically store only a label on each edge, whereas property graphs can store more attributes on edges and are hence closer semantically to relational databases. EL graphs can be more succinctly modelled mathematically, since they can be represented purely using mathematical relations; they also lead to a cleaner syntax (see the DSL syntax subsection of the query language section). Finally, any data expressible using a property graph can be represented using an EL graph by introducing additional nodes to hold attributes.

\begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{figs/ELGraph.png}
  \caption{An edge-labelled graph encoding information about actors that have acted in movies under different
roles \cite{ELGraphExample}. \todo{Tim and Rob, is using this figure and the next from another paper allowed if correctly cited?}}
  \label{fig:ELGraph}
\end{figure}

\begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{figs/PropertyGraph.png}
  \caption{An edge-labelled graph encoding information about actors that have acted in movies under different
roles \cite{PropertyGraphExample}}
  \label{fig:PropertyGraph}
\end{figure}

\subsection{Schema}
Current graph database systems often do not have rigid schema \cite{QueryLanguageSchema}. Instead, they use dynamic schema. This provides more flexibility in the shape of data that can be stored in the database; it also allows the types of nodes stored to evolve over time. The downside of this is that it requires additional typing checks at run-time. This also fails to fit into our definition of type-safety, since we have no guarantees about any objects extracted from the database. I have instead opted for a rigid schema which is built using Scala objects and is checked by the Scala compiler at compile-time.

\subsection{Mutability}
Current graph databases tend to be mutable. In order to express concepts that require immutability, such as time-dependent data, the user has impose extra constraints, such as adding time-stamps to the schema of  nodes and relations. A lack of immutability also complicates concurrent semantics and implementation of ACID transactions. I decided to introduce immutable data structures  to the database, which yields ACID properties effectively for free. This is explained in section \ref{immutability}.
\subsection{Query Languages}
Typically, database systems have their own query language, the most ubiquitous of which is Neo4j's language, Cypher \footnote{https://neo4j.com/docs/developer-manual/current/cypher/}. This provides ML-style pattern matching of queries against the stored graph, allowing the user to extract arbitrary fields from nodes. In most cases, theses are used by generating a query string and submitting it to the database engine. This is inherently not type-safe. The job of ensuring that absolutely every query a given program could generate is valid is intractable to undecidable for non-trivial programs. I decided to embed the query language in the host language. This allows the query language's type system to be checked by the host compiler, so we can get much stronger guarantees about program correctness.

\section{Immutability}\label{immutability}
In order to implement immutability, I needed a way of creating an immutable snapshot of the database at a given point in time. This is done using a system of views. A \codeName{View} is such an immutable snapshot. When we read from the database, we read from a particular given view. When we write to a particular view of the database, we copy the view, update it, and return the new view. Reads and writes now never interfere with each other. 

\begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{figs/readsWrites.png}
  \caption{An example of reads and writes to an initial view. Separate reads and writes do not interfere with each other.}
  \label{fig:ReadsWrites}
\end{figure}

\section{Query Language}

In this next section, I shall define the query language that I constructed and introduce a set of semantics for evaluating queries.

\subsection{Domain Specific Language Syntax}
I spent some time iterating over a DSL syntax and how I wanted queries to look and be structured. My goals were to have a highly composable and expressive, yet small language. At first I experimented with a Neo4j-style language, with pattern matching syntax, an example of which is below. The query takes a number of fields to fill in, in this case a pair of actors, and finds all valid ways to fill them in subject to the graph constraints.
\begin{framed}
\begin{verbatim}
 val coactor = RelationQuery {
    (x: Pattern[Actor], y: Pattern[Actor]) => {
       val m = ?[Movie]
        (x ~ ActsIn ~> m) && (y ~ ActsIn ~> m)
    }
}
\end{verbatim}
\note{Find two actors such they both act in the same movie}
\end{framed}
This is fairly cluttered syntax and difficult to read. Further more, the Scala type inference system is fairly limited, meaning that users would frequently be required to insert type annotations in this syntax. It would also have been further complicated by adding the property graph features of Neo4j. Furthermore, we would not be able to easily and safely replicate Neo4j's ability to extract an arbitrary number of objects of different types from along a path defined by a query. Doing so would require use of structures such as heterogeneously typed lists.

As a result, I settled upon on an edge-labelled-relation-oriented approach which neatens the above query significantly.

\begin{framed}
\begin{verbatim}
val coactor = ActsIn --><-- ActsIn
\end{verbatim}
\note{Find two actors such they both act in the same movie}
\end{framed}

This is much more readable and concise. With this syntax, and the assumption that \codeName{ActsIn} relates \codeName{Actors} to \codeName{Movies}, Scala's typesystem can infer that \codeName{coactor} relates \codeName{Actors} to other \codeName{Actors}. There is also syntax for repetitions, intersections and unions. \crossRefNeeded{Examples in the appendix}
\subsection{Semantic Definitions}
To correctly implement the DSL, we need to mathematically model its operation. Hence, the above DSL queries should correspond to an intermediate representation. I have picked a set of expression constructors to do this. These fall into two kinds: \codeName{FindPair} and \codeName{FindSingle}, indicating whether they return pairs or individual values.

\paragraph{FindPair queries}
\begin{equation}
\label{PDefinition}
\begin{split}
P  &\rightarrow Rel(R) \mbox{ Find pairs related by the named relation R}\\
&\mid RevRel(R) \mbox{ Find pairs related by the named relation R in the reverse direction}\\
&\mid Chain(P, P) \mbox{   Find pairs related by the first subquery followed by the second}\\
&\mid And(P, P) \mbox{  Find pairs related by both of the sub-queries}\\
&\mid AndRight(P, S) \mbox{  Find pairs related by P where the right value is a result of s}\\
&\mid AndLeft(P, S) \mbox{  Find pairs related by P where the left value is a result of s}\\
&\mid Or(P, P) \mbox{  Find pairs related by either of the sub-queries}\\
&\mid Distinct(P) \mbox{  Find pairs related by P that are not symmetrical}\\
&\mid Id_A \mbox{ Identity relation}\\
&\mid Exactly(\mathit{n}, P) \mbox{  Find pairs related by n repetitions of P}\\
&\mid Upto(\mathit{n}, P) \mbox{  Find pairs related by upto n repetitions of P}\\
&\mid FixedPoint(P) \mbox{  Find the transitive closure of P}\\
\end{split}
\end{equation} 
where $n$ denotes a natural number. These queries look up a set of pairs of objects.\\

\paragraph{FindSingle queries}
\begin{equation}
\label{SDefinition}
\begin{split}
S & \rightarrow Find(F) \mbox{ Find values that match the findable F}\\
&\mid From(S, P) \mbox{ Find values that are reachable from results of S via P}\\
&\mid AndS(S, S) \mbox{ Find values that are results of both subqueries}\\
&\mid OrS(S, S) \mbox{ Find values that are results of either subquery}
\end{split}
\end{equation} 
Which look up sets of single objects.\\

\paragraph{Other Definitions}
In order to model the operation of queries correctly, we need to define the environment in which they operate.

\subparagraph{Object Types} are the ``real world'' types stored in the database. These correspond to the user's Scala classes.

\[ \tau \rightarrow A \mid B \mid C \mid .. \]

\subparagraph{Named relations} are the primitive relations between objects

\[R \rightarrow r_1 \mid r_2 \mid .. \]

\subparagraph{Findables} are names for defined partial functions

\[F_A \rightarrow f_1 \mid f_2 \mid ... \]

\[f \colon A \rightharpoonup \{True, False\} \]

For some given object type A. (i.e. a findable is an index into the database)

\subparagraph{A schema,}$\Sigma$, is made up of three partial functions:

\[\Sigma_{rel}\colon R \rightharpoonup \tau\times\tau \]
\[\Sigma_{findable}\colon F \rightharpoonup \tau \]
\[\Sigma_{table}\colon \tau \rightharpoonup \{True, False\} \]

Which give the types of relations and findables, and validiate the existence of a type. When it is obvious from the context, I shall simply use $\Sigma(x)$ to signify application of the appropriate function.
 
\subparagraph{A view,} $v \in V_\Sigma$, for a given schema $\Sigma$ represents an immutable state of a database.

It represents a pair of partial functions. Firstly the named-relation look up function.

\begin{equation}
v \in V_\Sigma \Rightarrow v_{rel}(r) \in \wp(A \times B)\mbox{ if $\Sigma(r)\downarrow (A, B)$}
\end{equation} 

That is, if a relation r is in the schema, then $v(r)$ is a set of pairs of objects with object type $\Sigma(r)$. Here, and from this point onwards I am using $\wp(s)$ to represent the power-set of a set, and $f(x) \downarrow y$ to mean $f$ is defined at $x$ and $f(x)=y$


The next function of a view is the type-look up function, it returns all objects in the view of a given object type:

\begin{equation}
v \in V_\Sigma \Rightarrow v_{table}(A) \in \wp(A)\indent \mbox{if $\Sigma(A)\downarrow True$}
\end{equation} 

That is, $v(A)$ is a set of objects of type $A$ stored in the view, and $A$ is a member of the schema $\Sigma$. Again I shall overload these two functions where it is clear from the context which is to be used.
 
\subsection{Typing}
In order to ensure the correctness of a given query, there is a type system to verify it. Typing rules take two forms. Firstly typing of pair queries:
\[ \Sigma \vdash P\colon (A, B)\]

Which means ``under the schema $\Sigma$, pair query $P$ returns a subset of $A \times B$''.  The second is for single queries:

\[ \Sigma \vdash S \colon A \]

Which means ``under the schema $\Sigma$ single query returns a subset of $A$''

The rules of the first kind are as follows

\begin{align}
&\displaystyle\mbox{(Rel)}\frac{\Sigma(r)\downarrow(A, B)}{\typeRule{Rel(r)}{(A, B)}} \\[3ex]
&\displaystyle\mbox{(Rev)}\frac{\Sigma(r)\downarrow(B, A)}{\typeRule{Rel(r)}{(A, B)}} \\[3ex]
&\displaystyle\mbox{(Id)}\frac{\Sigma(A)\downarrow True}{\typeRule{Id_A}{(A, A)}} \\[3ex]
&\displaystyle\mbox{(Chain)}\frac{\typeRule{P}{(A, B)} \indent \typeRule{Q}{(B, C)}}{\typeRule{Chain(P, Q)}{(A, C)}} \\[3ex]
&\displaystyle\mbox{(And)}\frac{\typeRule{P}{(A, B)} \indent \typeRule{Q}{(A, B)}}{\typeRule{And(P, Q)}{(A, B)}} \\[3ex]
&\displaystyle\mbox{(Or)}\frac{\typeRule{P}{(A, B)} \indent \typeRule{Q}{(A, B)}}{\typeRule{Or(P, Q)}{(A, B)}} \\[3ex]
&\displaystyle\mbox{(Distinct)}\frac{\typeRule{P}{(A, B)}}{\typeRule{Distinct(P)}{(A, B)}} \\[3ex]
&\displaystyle\mbox{(AndLeft)}\frac{\typeRule{P}{(A, B)} \indent \typeRule{S}{(A)}}{\typeRule{AndLeft(P,S)}{(A, B)}} \\[3ex]
&\displaystyle\mbox{(AndRight)}\frac{\typeRule{P}{(A, B)} \indent \typeRule{S}{B}}{\Sigma \vdash \typeRule{AndRight(P, S)}{(A, B)}} \\[3ex]
&\displaystyle\mbox{(Exactly)}\frac{\typeRule{P}{(A, A)}}{\typeRule{Exactly(n, P)}{(A, A)}} \\[3ex]
&\displaystyle\mbox{(Upto)}\frac{\typeRule{P}{(A, A)}}{\typeRule{Upto(n, P)}{(A, A)}} \\[3ex]
&\displaystyle\mbox{(FixedPoint)}\frac{\typeRule{P}{(A, A)}}{\typeRule{FixedPoint(P)}{(A, A)}} \\[3ex]
\end{align}


The rules for types of Single queries are similar:

\begin{align}
&\displaystyle\mbox{(Find)}\frac{\Sigma(f)\downarrow(A)}{\typeRule{Find(f)}{A}} \\[3ex]
&\displaystyle\mbox{(From)}\frac{\typeRule{P}{(A, B)} \indent \typeRule{S}{A}}{\typeRule{From(S, P)}{B}} \\[3ex]
&\displaystyle\mbox{(AndS)}\frac{  \typeRule{S}{A} \indent  \typeRule{S'}{A}}{\typeRule{AndS(S, S')}{A}} \\[3ex]
&\displaystyle\mbox{(OrS)}\frac{  \typeRule{S}{A} \indent  \typeRule{S'}{A}}{\typeRule{OrS(S, S')}{A}} \\[3ex]
\end{align}

\subsection{Semantics}
    To fully define the query language, we need to define the semantics. I have defined two collections of semantics: operational style and denotational style.
\paragraph{Operational Semantics}   

Now we shall define a set of rules for determining if a pair of objects is a valid result of a query. We're interested in forming a relation $a \triangleleft_A Q$ to mean``a is a valid result of query Q with type A''. This is dependent on the current view $v: View_{\Sigma}$. Hence we define $\opRule{(a, b)}{(A, B)}{P}$ for pair queries $P$ and $\opRule{a}{A}{S}$ for single queries $S$.

\begin{align}
&\displaystyle\mbox{(Rel)}\frac{(a,b) \in v(r)}{\opRule{(a, b)}{(A, B)}{Rel(r)}} \\[3ex]
&\displaystyle\mbox{(Rev)}\frac{(b,a) \in v(r)}{\opRule{(a, b)}{(A, B)}{RevRel(r)}} \\[3ex]
&\displaystyle\mbox{(Id)}\frac{a \in v(A)}{\opRule{(a, a)} {(A, A)} {Id_A}} \\[3ex]
&\displaystyle\mbox{(Distinct)}\frac{\opRule{(a,b)}{(A, B)}{P} \indent a \neq b}{\opRule{(a,b)}{(A, B)}{Distinct(P)}} \\[3ex]
&\displaystyle\mbox{(And)}\frac{\opRule{(a,b)}{(A, B)}{P} \indent \opRule{(a,b)}{(A, B)}{Q}}{\opRule{(a,b)}{(A, B)}{And(P, Q)}} \\[3ex]
&\displaystyle\mbox{(Or1)}\frac{\opRule{(a,b)}{(A, B)}{P}}{\opRule{(a,b)}{(A, B)}{Or(P, Q)}} \\[3ex]
&\displaystyle\mbox{(Or2)}\frac{\opRule{(a,b)}{(A, B)}{Q}}{\opRule{(a,b)}{(A, B)}{Or(P, Q)}} \\[3ex]
&\displaystyle\mbox{(Chain)}\frac{\opRule{(a,b)}{(A, B)}{P} \indent \opRule{(b,c)}{(B, C)}{Q}}{\opRule{(a,c)}{(A, C)}{Chain(P, Q)}} \\[3ex]
&\displaystyle\mbox{(AndLeft)}\frac{\opRule{(a,b)}{(A, B)}{P} \indent \opRule{a}{A}{S}}{\opRule{(a,b)}{(A, B)}{AndLeft(P, S)}} \\[3ex]
&\displaystyle\mbox{(AndRight)}\frac{\opRule{(a,b)}{(A, B)}{P} \indent \opRule{b}{B}{S}}{\opRule{(a,b)}{(A, B)}{AndRight(P, S)}} \\[3ex]
&\displaystyle\mbox{(Exactly_0)}\frac{\opRule{(a,b)}{(A, A)}{Id_A}}{\opRule{(a, b)}{(A, A)}{Exactly(0, P)}} \\[3ex]
&\displaystyle\mbox{(Exactly_{n+1})}\frac{\opRule{(a,b)}{(A, A)}{P} \indent \opRule{(b, c)}{(A, A)}{Exactly(n, P)}}{\opRule{(a, c)}{(A, A)}{Exactly(n + 1, P)}} \\[3ex]
&\displaystyle\mbox{(Upto_0)}\frac{\opRule{(a,b)}{(A, A)}{Id_A}}{\opRule{(a, b)}{(A, A)}{Upto(0, P)}} \\[3ex]
&\displaystyle\mbox{(Upto_n)}\frac{\opRule{(a, b)}{(A, A)}{Upto(n, P)}}{\opRule{(a, b)}{(A, A)}{Upto(n + 1, P)}} \\[3ex]
&\displaystyle\mbox{(Upto_{n+1})}\frac{\opRule{(a,b)}{(A, A)}{P} \indent \opRule{(b, c)}{(A, A)}{Upto(n, P)}}{\opRule{(a, c)}{(A, A)}{Upto(n + 1, P)}} \\[3ex]
&\displaystyle\mbox{(fix1)}\frac{\opRule{(a,b)}{(A, A)}{Id_A}}{\opRule{(a, b)}{(A, A)}{FixedPoint(P)}} \\[3ex]
&\displaystyle\mbox{(fix2)}\frac{\opRule{(a,b)}{(A, A)}{P} \indent \opRule{(b, c)}{(A, A)}{FixedPoint(P)}}{\opRule{(a, b)}{(A, A)}{FixedPoint(P)}} \\[3ex]
\end{align}

And the FindSingle rules

\begin{align}
&\displaystyle\mbox{(Find)}\frac{a \in v(A) \indent f(a)\downarrow True}{\opRule{a}{A}{Find(f)}} \\[3ex]
&\displaystyle\mbox{(From)}\frac{\opRule{a}{A}{S} \indent \opRule{(a,b)}{)(A, B)}{P}}{\opRule{b}{A}{From(S, P)}} \\[3ex]
&\displaystyle\mbox{(AndS)}\frac{\opRule{a}{A}{S} \indent \opRule{a}{A}{S'}}{\opRule{a}{A}{And(S, S')}} \\[3ex]
&\displaystyle\mbox{(OrS1)}\frac{\opRule{a}{A}{S}}{\opRule{a}{A}{Or(S, S')}} \\[3ex]
&\displaystyle\mbox{(OrS1)}\frac{\opRule{a}{A}{S'}}{\opRule{a}{A}{Or(S, S')}} \\[3ex]
\end{align}
\paragraph{Denotational Semantics}
The operational semantics clearly demonstrate membership of a query, but do not give a means to efficiently generate the results of query. To this end, we introduce denotations $\llbracket P \rrbracket$ and $\llbracket S \rrbracket$ such that \[\Sigma \vdash P \colon (A, B) \Rightarrow\llbracket P \rrbracket \colon View_{\Sigma} \rightarrow \wp(A \times B)\]

and \[\Sigma \vdash S \colon A \Rightarrow\llbracket S \rrbracket \colon View_{\Sigma} \rightarrow \wp(A)\] Such denotations should be compositional and syntax directed, whilst still corresponding to the operational semantics.

\begin{align}
& \deno{Rel(r)} = v(r) \\[3ex]
& \deno{RevRel(r)} = swap(v(r)) \\[3ex]
& \deno{Id_A} = dup(v(A)) \\[3ex]
& \deno{Chain(P, Q)} = join(\deno{P}, \deno{Q}) \\[3ex]
& \deno{And(P, Q)} = \deno{P} \cap \deno{Q} \\[3ex]
& \deno{Or(P, Q)} = \deno{P} \cup \deno{Q} \\[3ex]
& \deno{AndLeft(P, S)} = filterLeft(\deno{P}, \deno{S}) \\[3ex]
& \deno{AndRight(P, S)} = filterRight(\deno{P}, \deno{S}) \\[3ex]
& \deno{Distinct(P)} = distinct(\deno{P}) \\[3ex]
& \deno{Exactly(n, P)} = (\lambda pairs. join(\deno{P}, pairs))^n \deno{Id_A} \\[3ex]
& \deno{Upto(n, P)} = (\lambda pairs. join(\deno{P}, pairs) \cup pairs)^n \deno{Id_A} \\[3ex]
& \deno{FixedPoint(P)} = fix (\lambda pairs. join(\deno{P}, pairs) \cup pairs)  \mbox{ in the domain $\clos$}\\[3ex]
\end{align}

And similarly with single queries

\begin{align}
&\deno{Find(f)} = \setComp{a \in v(A)}{f(a)\downarrow True} \mbox{ for $\Sigma(f) = A$} \\[3ex]
&\deno{From(S, P)} =  \setComp{b}{(a, b) \in \deno{P} \&\wedge a \in \deno{S}}   \\[3ex]
&\deno{AndS(S, S')} = \deno{S} \cap \deno{S'} \\[3ex]
&\deno{OrS(S, S')} = \deno{S} \cup \deno{S'} \\[3ex]
\end{align}
with the following definitions:
\[ swap(s) = \setComp{(b,a)}{(a, b) \i s}\]
\[ dup(s) = \setComp{(a,a)}{a \in s}\]
\[ join(p, q) = \setComp{(a, c)}{ \exists b. (a, b) \in p \wedge (b, c) \in q}\]
\[ distinct(s) = \setComp{(a, b) \in s} { a \neq b} \]
\[ filterLeft(p, s) = \setComp{(a, b) \in p}{a \in s}\]
\[ filterRight(p, s) = \setComp{(a, b) \in p}{b \in s}\]
    
   I have also proved in the appendix the correspondence of these two formulations of semantics.
\subsection{Commands}
In addition to the query construction language, there are five commands which make use of the queries.

\todo{format this correctly}
\[\label{CommandSemantics}
\begin{split}
findPairs(P, v) &= \deno{P}\\
find(S, v) &=  \deno{S}\\
shortestPath(a, a', P, v) &= \mbox{Shortest path from $a$ to $a'$ in the graph defined by $\deno{P}$}\\
allShortestPaths(a, P, v) &= \mbox{The shortest path to each element reachable from $a$ in the graph defined by $\deno{P}$}\\
\end{split}\]

The definition of the write command is slightly more complicated. It needs to return an updated view with its look up functions now returning the inserted objects. Write takes a collection, $rs$, of correctly typed named relations instances.


\[rs = \setComp{(a_i, r_i, b_i) \in (A \times R \times B)}{0 < i \leqslant n} \mbox{for some $n$ being the size of the set.}\]
and updates the view as so: 
\begin{equation}
\begin{split}
write(rs, v) = &v\left[A \mapsto v_{table}(A) \cup \setComp{a_i}{0 < i \leqslant n}\right]\\
& \left[B \mapsto v_{table}(B) \cup \setComp{b_i}{0 < i \leqslant n}\right]\\
& \left[r \mapsto v_{rel}(R) \cup \setComp{(a_i, b_i)}{0 < i \leqslant n}\right] 
\end{split}
\end{equation}

\subsection{Summary}
To summarise, this past section defines a query DSL, a type system for checking validity of queries, and a set of semantics for executing queries. Furthermore, I have then defined a set of commands which allow queries to executed and views to be generated.


\section{Starting Point}
In addition to Scala, I have made use of several libraries and tools, some of which were not forseen in my project proposal. Some were used for the actual construction of the project, whereas others were used for auxiliary tasks such as testing and debugging or  generating datasets. All of these tools have licenses allowing free usage (The exact license for each is given in the footnotes.) and no modifications to these libraries were performed.

\begin{itemize}
\item[\textbf{SBT}]\footnote{//www.Scala-sbt.org/} Scala Build Tool (SBT) is a package manager and build tool for Scala which allows access to Java libraries. \footnote{https://github.com/sbt/sbt/blob/1.x/LICENSE}

\item[\textbf{PostgreSQL}]\footnote{//www.postgresql.org/} An open-source, multi-platform SQL implementation.\footnote{https://opensource.org/licenses/postgresql}

\item[\textbf{LMDB}]\footnote{//symas.com/lmdb/} An open source, highly optimised key-value datastore. \footnote{http://www.openldap.org/software/release/license.html}

\item[\textbf{Scalaz}]\footnote{https://github.com/Scalaz/Scalaz} A library for Scala providing typeclasses and syntax to aid advanced functional programming.\footnote{https://github.com/scalaz/scalaz/blob/series/7.3.x/LICENSE.txt}

\item[\textbf{JDBC}]\footnote{http://www.oracle.com/technetwork/java/javase/jdbc/index.html} Java's standard library support for SQL connections. Used to interact with a postgres database. I had originally planned to use the higher level wrapper scalikeJDBC,\footnote{http://scalikejdbc.org/} however, this did not allow such fine control over queries as I would have liked. \footnote{https://jdbc.postgresql.org/about/license.html}

\item[\textbf{LMDBJava}]\footnote{//github.com/lmdbjava/lmdbjava} A JNI library allowing access to the LMDB datastore. Previously, I experimented with another library, LMDBJni. However, upon discovery of a bug in the JNI code, I switched to LMDBJava.\footnote{https://github.com/lmdbjava/lmdbjava/blob/master/LICENSE.txt}

\item[\textbf{Junit}]\footnote{https://junit.org/junit5/} A unit testing library for JVM languages.\footnote{https://junit.org/junit4/license.html}

\item[\textbf{SLF4J}]\footnote{https://www.slf4j.org/} A logging library for JVM languages. \footnote{https://www.slf4j.org/license.html}

\item[\textbf{Spray-JSON}]\footnote{https://github.com/spray/spray-json} A JSON library for Scala, used to generate benchmarking datasets. \footnote{https://github.com/spray/spray-json/blob/master/LICENSE}

\item[\textbf{Python}]\footnote{https://www.python.org/} A scripting language used to generate test datasets. \footnote{https://docs.python.org/3/license.html}


\end{itemize}


\section{Software Engineering}

\begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{figs/Architecture.png}
  \caption{A map of the architecture of the system}
  \label{fig:Architecture}
\end{figure}
The system was designed to be as modular and re-implementable as possible. To this end, it consists of several interchangeable components; a map of this is provided in figure \ref{fig:Architecture}. The front-end mostly consists of interfaces and syntax for building queries and the schema for the database. The front-end's DSL translates into the underlying typed ADT. This then has its compile-time type information erased to become an un-typed query ADT, which is easier and cleaner to interpret. Finally, there are three interfaces that a back-end system must implement: DBBackend, DBInstance and DBExecutor. These specify a root back-end object that opens an instance which represents a database connection. Each DBInstance has an executor that allows us to execute commands. The results of these commands are then un-erased to return them as the correct type.

\section{Scala Techniques}
I have made use of several advanced programming techniques which are specific to the Scala language.

\subsection{type-enrichment}
The type-enrichment feature of the Scala language allows retroactive addition of methods to previously defined types. type-enrichment is performed by creating an implicit class, taking a single underlying value of some given type. The methods defined in the implicit class can now be called as if they were methods of the underlying class, provided the implicit class is in scope.


\begin{framed}
\begin{verbatim}
object Outer {
    implicit class EnrichInt(i: Int){
        def neg: Int = -i
        def  succ: Int = i + 1
        def mod(n: Int) = i - (i / n)*n
    }
}
\end{verbatim}
When Outer is imported, we can now do

\begin{verbatim}
5.neg == -5
6.succ == 7
10 mod 4 == 2
\end{verbatim}
\end{framed}






This allows us to add syntax to types where a small number of core methods have been defined.

\subsection{Implicit Parameters}
Another advanced feature of Scala that  I have used is that of implicit  parameters to functions and class constructors. Values such as vals, functions, and classes can be declared with an implicit tag. Functions and classes can declare additional parameters as implicit. When these functions are called, the implicit parameter can be omitted if and only if there is an unambiguous implicit value of the correct type in scope.
\begin{framed}
\begin{framed}
\begin{verbatim}
object foo {
    implicit val i = 5
}

object bar {
    def plusImpl(j: Int)(implicit k: Int): Int = j + k
}
\end{verbatim}
\end{framed}

\begin{framed}
\begin{verbatim}
import foo._

bar.plusImpl(4) == 9
\end{verbatim}
\end{framed}
\end{framed}
This is typically used for purposes such as passing around values that are defined once and used many times in a program, such as an execution context, or a logging framework instance.

Functions can also be implicit and take implicit values
\begin{framed}
\begin{verbatim}
implicit def f(implicit a: A): B = ...
\end{verbatim}
When an implicit value of type A is available, then an implicit value of type B is also available.
\end{framed}

This can be made more interesting when we include parameterised generic types. This allows us to get the compiler to do work in the manner of an automated theorem prover (by the curry-howard correspondence) at compile-time.

\begin{framed}
	\begin{framed}
		\begin{verbatim}
		
object Defs {
    implicit val s: String = "foo"
    implicit val i: Int = 2
    Implicit def toList[A](implicit a: A): List[A] = List(a, a, a)

    def getImplicit[A](implicit a: A): A = a
}
		\end{verbatim}
	\end{framed}
	\begin{framed}
		\begin{verbatim}
		import Defs._

getImplicit[List[Int]] == List(2, 2, 2)
getImplicit[List[List[String]] == 
        List(List(“foo", “foo", "foo"),
        List("foo", "foo", "foo"), 
        List("foo", "foo", "foo"))
		\end{verbatim}
	\end{framed}
\end{framed}


\subsection{Typeclass Pattern}
A further combination of these two patterns is the typeclass pattern. We define a typeclass for a type by defining as methods on a trait the operations we want on the type. We can then define implicit objects which work as type class instances for the types we want. We can also use implicit functions to generate typeclass instances in a manner similar to a proof tree. We can then use the type-enrichment feature to add methods to values of a type that is a member of the typeclass.


\begin{framed}
	\begin{framed}
		\begin{verbatim}
trait Monoid[A] {
     def id: A
     def op(a1: A, a2: A): A
}
		\end{verbatim}
	\end{framed}
	
	Definition of a monoid typeclass instance using F-type polymorphism
	
	\begin{framed}
		\begin{verbatim}
object Instances {
     implicit object IntMonoid extends Monoid[Int] {
        override val id: Int = 0
        override def op(a1: Int, a2: Int): Int = a1 + a2
     }

    implicit def ListMonoid[A] = new Monoid[List[A]] {
            override val id: List[A] = List()
            override def op(a1: List[A], a2:  List[A):  List[A= a1 ++ a2
      }
 

    implicit def PairMonoid[A, B](implicit MA: Monoid[A], MB: Monoid[B]) =
     new Monoid[(A, B)]{
      override val id: (A, B) = (MA.id, MB.id)
      override def op(p: (A, B), q: (A, B)) =
      	(MA.op(p._1, q._1), MB.op(p._2, q._2))
    }
}
		\end{verbatim}
	\end{framed}
	
	Definition of several monoid instances, including a pair monoid that combines monoids.
	
	\begin{framed}
		\begin{verbatim}
		
object Syntax {
        implicit class MonoidOps[A](a: A)(implicit ma: Monoid[A]) {
          def *(n: Int): A = if (n == 0) ma.id else ma.op(a, *(n-1))
        }
}
		\end{verbatim}
	\end{framed}
Definition of a multiplication operation on instances of a monoid.
	\begin{framed}
		\begin{verbatim}
			 (5, List("f")) * 3 == (15, List("f", "f", "f"))
		\end{verbatim}
	\end{framed}
Usage of the multiplication operation on a tuple.
\end{framed}

It is clear that these patterns allow for very expressive structures and abstractions to be built in Scala. I use these frequently within the project to neaten code and to achieve type-safety.



\chapter{Implementation}

In this section, I shall first focus on common techniques when building the database, then explain construction of the front- and middle-ends. After this, I shall look at common structures shared by the various backend implementations and finally, I shall conclude with an examination of each of the backend implementations that I have written. 
\section{Note on Purity and Concurrency}
All of the back-ends aim to preserve the global immutability of the database. The immutable semantics of the database also mean that queries generally do not interfere with each other. This means that we can avoid keeping locks or creating large transactions. Hence, the backends do not require much work to maintain correct concurrency.
\section{Functional Programming Techniques}
	\subsection{Monadic Compilation}
	At several points in building a backend, it becomes necessary to transform one algebraic type to another. This is typically done by walking over a tree, whilst keeping some mutable state representing parts of the tree that are relevant. One example is converting an intermediate representation to SQL output code. Here, we may want to extract common subexpressions into a dictionary, or pick out all the database tables that need to be accessed by the query. This can be encoded by folding a State Monad instance over the tree.

The state monad is an abstraction over functions that chain an immutable state through successive computations.

{\renewcommand{\baselinestretch}{0.8}\small
\begin{verbatim}
class State[S, A](r: S => (S, A)) {
      def runState(s: S): (S, A) = r(s)
}


object Example {
    Implicit def StateMonadInstance[S] = new Monad[State[S, _]] {
         def point[A](a: A) = new State(s => (s, a))
         def bind[A, B](sa: State[S, A], f: A => State[S, B]) = new State(
            s => {
                  val (s', a) = sa.runState(s)
                  f(a).runState(s')
              }
                
         )
    }
}
\end{verbatim}

\todo{\crossRefNeeded{Do I want to link to this instead?}}

To define a monadic compiler, we define a recursive function which chains state monad objects together.

{\renewcommand{\baselinestretch}{0.8}\small
\begin{verbatim}
def compile(adt: ADT): State[S, Res] = adt match {
   case basis => ...
   case Pattern(a, b) => for {
      ca <- compile(a)
      cb <- compile(b)
   } yield foo(ca, cb) 
}
\end{verbatim}


	\subsection{Constrained Future Monad}
	Part of the goal of type-safety is the recovery of error cases. Typically, this would be done in a JVM program through the use of exceptions. However the presence of unchecked exceptions on the Java platform makes it difficult to ensure that all error cases are accounted for. A more functional approach is the use of the exception monad \cite{ExceptionMonad}. In Scala, this manifests as the built in Try monad and Scalaz's \codeName{\either} (\codeName{Either}) monad. \codeName{Try[A]} has two case classes: \codeName{Failure(e: Throwable)} and \codeName{Success(a: A)}, while \codeName{E \either A} has the case classes \codeName{\eitherL(e: E)} and \codeName{\eitherR(a: A)} (\codeName{Left} and \codeName{Right}). Since \codeName{Try}'s failure case is the unsealed \codeName{Throwable} trait, we do not really have a way to ensure we have handled all error cases at compile-time, whereas \codeName{Either} has a parametrised error type, which can be a sealed type hierarchy, which is then checked by the Scala compiler at run-time. For example, consider the simple interpreter below. All error cases are proved to be handled by the type-system.
	
\begin{framed}
	\begin{framed}
		\begin{verbatim}
			sealed trait Error
			case object DivByZero extends Error
			case object Underflow extends Error

			sealed trait NoError
		\end{verbatim}
	\end{framed}
	
	A simple pair of error hierarchies. One signifying errors, and NoError indicating a lack of error. Note that $NoError$ cannot be instantiated or sub-classed.


	\begin{framed}
		\begin{verbatim}
def eval(e: Expr): Error \/ Int = 
    e match {
         case Div(a, b) => for {
               aRes <- eval(a)
               bRes <- eval(b)
               r <- if (bRes == 0) DivByZero.left else (aRes/bRes).right
            } yield r

        case Sub(a, b) => for {
            aRes <- eval(a)
            bRes <- eval(b)
            r <- if(bRes > aRes) Underflow.left else (aRes-bRes).right
    } yield r
}
		\end{verbatim}
	\end{framed}
	
	A simple interpreter for a simple algebraic datatype for expressions that returns an error or a result
	
	\begin{framed}
		\begin{verbatim}
def evalAndPrint(e: Expr): NoError \/ String = 
    eval(e) match {
        case \/-(i) =>i.toString.right
        case -\/(e) => e match {
                 case DivByZero => “division by zero”.right
                 case UnderFlow => “underflow”.right
}}
		\end{verbatim}
	\end{framed}
	A simple result printer that correctly handles all error cases and reduces its error parameter to the un-instantiable $NoError$ type
		
\end{framed}


Typically, we're dealing with cases which might take a significant amount of time to return. So rather than using a simple \codeName{Either} monad, we lift it into an asynchrony monad. There are several options to choose from for an asynchrony monad. I chose the built in \codeName{Future} over more exotic alternatives, such as the Scalaz \codeName{Task}, since it is relatively widely used, and I have some familiarity with it from past projects. \codeName{Futures} also capture thrown unchecked exceptions, which makes handling them a little easier. Hence we're interested in passing around \codeName{Future[E \either A]} around, for a sealed type hierarchy \codeName{E}. There is also the issue of the java libraries used (for SQL and LMDB access) throwing exceptions, and unexpected exceptions turning up in code. Fortunately, the  Future container catches these, acting like an asynchronous \codeName{Try[E \either A]}. This causes issues as we do not have the sealed trait property of errors as we have above. To solve this, I introduced the \codeName{ConstrainedFuture[E, A]} monad, which has the requirement that the error case type parameter \codeName{E} implements the \codeName{HasRecovery} typeclass for converting any \codeName{Throwable} to an \codeName{E}.

\begin{framed}
	\begin{verbatim}
trait HasRecovery[E] {
 def recover(t: Throwable): E
}
	\end{verbatim}
\end{framed}

The underlying future is kept private, and can only be accessed via the run method, which calls the recover method on any errors (tail recursively, so any exceptions thrown during execution are also handled). By this construction \crossRefNeeded{appendix} we ensure all non-fatal error cases are contained in a type-safe way.

	\subsection{Operation Monad}
	As stated, \crossRefNeeded{Can I link to it?} typical database operations take a view as a parameter, inspect the view, return a value and may also insert a new view. This requires interplay between the \codeName{ConstrainedFuture} (To handle failure and asynchrony) and \codeName{State} (to chain together updates to the view representing current state) monads. Hence we use the \codeName{Operation[E, A]} monad, which wraps a function \codeName{$(ViewId \Rightarrow ConstrainedFuture[E, (ViewId, A)])$} in a similar way to how \codeName{State} monad chains together functions \codeName{$S \Rightarrow (S, A)$}. Each of the commands on a database yields an operation.

	
	\subsection{Local and Global State}
Although it would be preferable to only use purely functional folds, maps, and immutable data structures everywhere within the project, for certain, high frequency, performance critical tasks, using purely immutable structures slows us down. Recursive functions (though my functional style does not make heavy use of them anyway) tend to use more stack space than the JVM has available in many situations. Hence for tasks such as retrieving values for result sets,  pathfinding across large relations, and building indices, I have opted to make use of mutable data structures locally, using builders \footnote{https://docs.scala-lang.org/overviews/core/architecture-of-scala-collections.html\#builders} for collections such as sets. This leads to a dramatic increase in speed, especially for when large numbers of elements are added sequentially to collections that are not lists. In these cases, the mutable state never leaks out of the functions that make use of the mutability.	
	
\section{Schema Implementation}
	One of the goals of the project was to allow close to arbitrary user objects (assuming that they are finite) to be inserted and retrieved from the database. This was achieved using the typeclass pattern.
	\subsection{Schema Hierarchy}
	In order to work with the database, a class needs to have an instance of the \codeName{SchemaObject} typeclass.
	\begin{framed}
		\begin{verbatim}
sealed trait SchemaObject[A] {
// components for constructing DB
 def schemaComponents: Vector[SchemaComponent]  tables
 def any: Pattern[A] // Findable that matches any A
 def findable(a: A): DBTuple[A] // convert an object to a findable
 def name: TableName // name of the table
 def fromRow(row: DBObject): ExtractError \/ A // unmarshall an A
}

		\end{verbatim}
		
	\end{framed}
	
	\codeName{SchemaObject} is sealed, so can only be implemented by implementing one of its subclasses. Currently, for the sake of simplicity, there are five: \codeName{SchemaObject0} .. \codeName{SchemaObject4}, with the number indicating the number of underlying database fields required. These simplify the construction of a type-class instance to marshalling values of the object type to tuples of  \codeName{Storable}"  (read: primitive) types.
	
	\crossRefNeeded{To go in the appendix: The definition of a schema object subclass}
	
	\subsection{DBObjects}
	
	To store objects in the database under various backends, we need to have a type erased (at compile-time, but not run-time) version of the objects. Once we have a primitive representation of an object from the \codeName{SchemaObject}, we can fully unerase it by converting it to a \codeName{DBObject}. A \codeName{DBObject} is simply a collection of type tagged fields (\codeName{DBCell}). These can now be easily inserted or retrieved from a database.
\begin{framed}
	\begin{verbatim}
type DBObject = Vector[DBCell]

type DBCell -> DBInt of Int 
			|  DBString of String
			| DBBool of Boolean
			| DBDouble of Double
	\end{verbatim}
	
	\note{Pseudo-ml definitions of \codeName{DBObject} and its components}
\end{framed}
	
	\subsection{Unerasure}
	In order to correctly retrieve values from the database, we need to be able to undo the erasure process. This can be done using the unmarshalling methods derived from the \codeName{SchemaObject}  for an object type.
	\subsection{Relations}
	    In order for type checking to work, relations need to have type parameters for the object types they link. Hence, to define relations in the schema, the user needs to define objects that extend the relation interface.
	    
\begin{framed}
\begin{verbatim}
abstract class Relation[A, B](implicit val sa: SchemaObject[A], val sb: SchemaObject[B])
   extends FindPairAble[A, B]
\end{verbatim}
\end{framed}



	
	\subsection{SchemaDescription}
	
In order to build the database structures the backends need a definitive collection of schema to include. This is performed using a \codeName{SchemaDescription} object, which simply holds a collection of \codeName{SchemaObjects} and \codeName{Relations} which need to be used by the database.	
	
	\subsection{Findables}
	For the sake of simplicity, I have only implemented findables which test if fields of objects match particular values. This allows us to look for specific objects or match particular fields. This also makes indexing easier to implement.
	
\section{Query ADT}
The intermediate representation terms described in the preparation section \crossRefNeeded{link}	are implemented in Scala by a pair of ADT hierachies: a typed and type-erased equivalent for each. The typed ADTs are parameterised by the object types which they look up. Using this parameterisation, I have encoded in type of the ADTs the inductive type rules in the semantics such that the Scala compiler checks the type of any generated query and does type inference for us. \crossRefNeeded{Link to Appendix "The definitions of these types can be found in the appendix..."} The only rules that cannot be checked at compile-time are whether the schema description contains the relation in instances of the (\codeName{Rel}) rule, the type \codeName{A} for the (\codeName{$Id_A$}) rule, and the (\codeName{Find}) rule, as we cannot predict the contents of the SchemaDescription at compile-time without dependent types.  The typed ADTs are erased, with respect to a schema,  into their unsafe equivalents in order to be executed. If an AST node makes reference to a non-existent table or relation, a run-time error is created in the \codeName{Either} return type. The constructors of the type-erased ADT nodes are private to the enclosing package, meaning that they can only be created by erasing a typed ADT.
	
\section{Commands}
As specified in the previous chapter, each backend needs to implement five commands: 
\begin{itemize}
\item \codeName{find(S): Operation[E, Set[A]]}
\item \codeName{findPairs(P): Operation[E, Set[(A, B)]]}
\item \codeName{ShortestPath(start, end, P): Operation[E, Option[Path[A]]}
\item \codeName{allShortestPaths(start, P): Operation[E, Set[Path[A]]}
\item \codeName{insert(relations): Operation[E, Unit]}.
\end{itemize}
Each command should return an \codeName{Operation} of the correct type.
\section{DSL}
The DSL mostly consists of syntactic sugar to make queries easier to read. It is implemented using the type-enrichment pattern. Both \codeName{Relation} and \codeName{FindPair} implement the trait (interface) \codeName{FindSingleAble}, so we can use type-enrichment to write new DSL opertors. The main thing to note in the DSL is the use of arrows to chain relations together. (See \codeName{RelationSyntax.Scala} in the appendix for examples of DSL). \crossRefNeeded{Put in appendix}

\section{Common Generic Algorithms}
During construction of the database, several common patterns of problems emerged with slight differences. Hence, I have written relatively optimised generic versions of these algorithms such that different backends can make use of them regardless of the underlying types. These algorithms are found in \codeName{core.utils.algorithms}


	\subsection{Simple Traversal}
	The first set of generic algorithms to look at are the \codeName{SimpleFixedPointTraversal} algorithms. These compute the repetitions of a function for execution of the \codeName{FixedPoint}, \codeName{Upto}, and \codeName{Exactly} expressions of the ADT. They are labelled as "Simple" because they do the search from a single root. They carry out search mutably, and convert their output to an immutable set upon returning.
	
		\paragraph{Exactly}
The simplest algorithm is for computing \codeName{Exactly} query nodes. Here, we simply expand a fringe set of values outwards, by applying the search step to every value in the fringe to get the new fringe. We also memoise the search step function in the case of repetitions. After the required number of repetitions, the remaining fringe is returned.

\begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{figs/Exactly.png}
  \caption{Execution of a simple \codeName{Exactly} query}
  \label{fig:ExactlySingle}
\end{figure}
    		\paragraph{Upto}
The next algorithm is for computing \codeName{Upto}. This is computed in a similar way by flat-mapping the functions over the fringe repeatedly to calculate an expanding fringe. The major differences here are that we keep an accumulator of all the found values. When a new fringe is calculated, we subtract the accumulator set from it to reduce the number of nodes that need to be searched to those that have not yet been searched, and then union the remaining fringe with the accumulator to get the new accumulator. After the required number of repetitions, the accumulator is returned.

\begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{figs/Upto.png}
  \caption{Execution of a simple \codeName{Upto} query}
  \label{fig:UptoSingle}
\end{figure}
	
		\paragraph{FixedPoint}
The  final algorithm is to calculate \codeName{FixedPoint}. This works slightly differently. As before, we keep an accumulator of reached nodes, but unlike before, the generation number of each node is not important, only that a node is reachable is important. Hence, the fringe is a queue rather than a set, and we iterate until the fringe is empty. In each iteration, we pop off the top value of the fringe queue, compute all immediately reachable nodes. This reachable set is diffed (define diff) with the accumulator to find the newly reached nodes. These are now added to the fringe queue and the accumulator. When the fringe is empty, we return the accumulator.
	
	\subsection{Full traversal}
		The next set of algorithms build on the \codeName{SimpleFixedPoint} algorithms to return not just those nodes reachable from a single root, but the set of all reachable pairs with the left hand pair derived from a root set (using the left-hand optimisation). As such, the algorithms need to do some more work to reconstruct which nodes are reachable from each root, while still eliminating redundancies.
		\paragraph{Exactly}
The first such algorithm is for computing \codeName{Exactly}. This is similar to the original version, except we now store a fringe for each root in a map of \codeName{Root => Set[Node]}. We also memoise the search function in a Map to avoid computing it redundantly. In each iteration, we simply expand the fringe for each root as before by mapping the fringe expansion loop body over the values of the fringe map. 

\begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{figs/ExactlyPairs.png}
  \caption{Execution of an \codeName{Exactly} query from multiple roots}
  \label{fig:ExactlyPairs}
\end{figure}
		
		\paragraph{Upto}
The next algorithm is to compute \codeName{Upto}. This is again done like before, but with a map of root to accumulator set as the accumulator. As with \codeName{Exactly}, the fringes of each root are expanded simultaneously, sharing redundant results via the memo, while the accumulators are unioned with the fringe of the appropriate root with each iteration. \begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{figs/UptoPairs.png}
  \caption{Execution of an \codeName{Upto} query from multiple roots}
  \label{fig:UptoPairs}
\end{figure}
		
		\paragraph{FixedPoint}
Finally, the FixedPoint take a departure from the parallel implementations above. The reachable set of each node is calculated sequentially using a similar algorithm to above. However the memo now contains all nodes reachable from previously processed roots, allowing for fast convergence of dense graphs.	
	
	\subsection{Pathfinding}
	The \codeName{ShortestPath} and \codeName{AllShortestPaths} commands require us to search a subgraph generated by a relation. Since all edges have unit weight, the pathfinding algorithm reduces to breadth-first-search, which I have implemented in an imperative format while wrapping up error cases in an Either. These functions take, as a parameter, the search step \codeName{ A\ $\Rightarrow$ E \either Set[A]} which represents the edges going out of a node. 
	
	\subsection{Joins}
	The problem of joining two sets of pairs based on shared intermediate values is a requirement for any backend. 
\[
A\mbox{ }join\mbox{ }B = \setComp{(a, c)}{\exists b. (a, b) \in A \wedge (b, c) \in B}
\]	
I have implemented a simple hash-join \cite{HashJoins}. This operates by first assuming that the size of distinct leftmost elements in the right set is smaller (known to be a subset of, due to the left-optimisation of backends) than the rightmost elements of the left set. We then build a mutable map to index leftmost values to rightmost values. We then traverse the left set and for each pair generate all new joined pairs, using a \codeName{flatmap} to collect the values into one set. An issue with this approach is that we have to build the index upon each join call, an issue that is addressed later.
	
\begin{framed}
\begin{verbatim}
  def joinSet[A, B, C](
      leftRes: Set[(A, B)],
      rightRes: Set[(B, C)]
  ): Set[(A, C)] = {
    // build an index of all 
    // values to join on right, since Proj_B(right)
    // is a subset of Proj_B(Left)
    val collectedRight = mutable.Map[B, mutable.Set[C]]()
    for ((b, c) <- rightRes) {
      val s = collectedRight.getOrElseUpdate(b, mutable.Set())
      s += c
    }
    
    for {
      (left, middle) <- leftRes
      right <- collectedRight.getOrElse(middle, Set[C]())
    } yield (left, right)
  }
\end{verbatim}
\end{framed}

\section{Views and Commits}
In the memory backend, as will be explained later, views are easy implement as a map of \codeName{ViewId} to \codeName{MemoryTree} and simply updating the \codeName{MemoryTree}, allowing Scala's immutable collections to handle sharing of data in an efficient way. In the other backends, backed by non-functional technologies, we need other methods of sharing and inserting to immutable structures. A first observation is that with the operation monad model, each view only has one direct predecessor, forming a tree where we're only interested in the path back to the root from a given node. From this insight, we can think of each new view only needing to store a difference against its parent. Since the current design of the database only allows the addition of values and not deletion, this difference is only going to be positive. Hence, we can store all the added values between two views in a container called a \codeName{Commit}. As an optimisation, we can disregard the parent view, and simply store each view as a collection of its commits. This would also allow us to implement deletion if desired. This would be done by removing commits in a view that are to be modified, and replacing them with a new commit containing the contents of those commits, excluding the deleted values.

\section{Memory Backend}
The first backend that I have implemented is a simple, naive memory-based backend. This backend follows the denotational semantics, and makes very few attempts to improve performance. This backend serves as a test-bench backend, used to create unit tests to test other backends. It also allowed me to practice implementation of type erasure and unerasure according to the schema in a controllable environment (that is, without interference from other languages or libraries as in the SQL and LMDB backends.)
	\subsection{Table Structure}
	A memory instance stores a concurrent map of \codeName{ViewId} to \codeName{MemoryTree}, which itself is a map of \codeName{TableName} (derived from the \codeName{SchemaDescription}) to \codeName{MemoryTable}. There is a \codeName{MemoryTable} for each object class in the \codeName{SchemaDescription}. A \codeName{MemoryTable} provides lookups using maps for \codeName{DBObjects} and \codeName{Findable}s, in the form of an index to set of objects for each column value. These lookups return objects containing a \codeName{DBObject} and the outgoing and incoming relations for the object, indexed by \codeName{RelationName}. 

\begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{figs/MemoryHierarchy.png}
  \caption{The lookup procedure for objects in the memory implementation}
  \label{fig:MemoryHierarchy}
\end{figure}

	
	\subsection{Reads}
Reads occur by simply walking over the ADTs recursively, following the denotational semantics \crossRefNeeded{Link to appendix}. The only real departures from the denotational semantics are the Left-optimisation and use of the generic fixed point algorithms to compute \codeName{Exactly(n, P)}, \codeName{Upto(n, P)}, and \codeName{FixedPoint(P)}. Results are also computed in the \codeName{Either} monad to allow for error checking (for cases such as missing tables).
	
	\subsection{Left Optimisation}
	One of the few optimisations here is the Left optimisation. When we compute the result set of a \codeName{FindPair}, we pass in the subset of left hand side variables we want to compute from. This mostly has an effect when we compute joins (\codeName{Chain}s). Consider joining a query of maybe a few dozen unique rightmost values to a large query with several million unique leftmost values. The pairs of the right relation only need to be joined if their leftmost value is in the set of rightmost values of the left relation. Hence it makes sense to pre-limit our search to pairs with leftmost values in the right most value. 
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{figs/LeftHandOptimisation.png}
	\caption{Joining a large right hand side to a small left hand side without pruning the right hand side search results in redundant calculations. \todo{Rob, Tim, Would it be better to give explicit result tables? Does this figure make sense?}}
	\label{fig:LeftHandOptimisation}
\end{figure}
This pattern also makes an appearance in the original LMDB implementation.
	\subsection{Writes}
	Thanks to Scala's immutable collections library, updating the database is fairly easy When inserting a collection of relations, we simply add, relation by relation, each object in the relation, if it does not exist, and update the outgoing and incoming relation map of each object. This update creates new immutable object tables and a new memory tree. This is stored to the map of \codeName{ViewId}  to \codeName{MemoryTree} as a new view.

	\subsection{Storage}
	Objects are stored as \codeName{DBObject}s ad their incoming and outgoing relations in wrapper \codeName{MemoryObject}s. \codeName{MemoryObject}s  are hashed and compared by their \codeName{DBObject}s.
	
	\subsection{Mutability}
	The only mutability in the Memory implementation is for the views counter and the views lookup table. Access is kept transactional using a lock.

	\subsection{Pathfinding and fixed point traversal}
	The pathfinding and fixed point traversal methods are simply implemented by calls to the appropriate generic algorithms using the interpreted findPairs function as the search function.


\section{PostgreSQL backend}
Upon completion of the initial memory backend, I started work on a PostgreSQL based backend. This compiles the ADT intermediate representation into SQL queries that are then executed by a Postgres database.

	\subsection{Table Structure}
	The construction of the underlying database uses several SQL tables. These can be partitioned into a set of control tables that will be present in all database instances and a set  of schema defined tables.

		\paragraph{Control Tables}
		\begin{center}
			\begin{tabular}{|p{4cm}||p{4cm}|p{4cm}|} \hline
			Name & Schema & Description \\ \hline
			Default & (ViewId) & Holds the mutable default ViewId \\ \hline
			Commits Registry & (CommitId$^{primary}$, Dummy) & Stores which commits are valid, allows us to give each commit a unique id \\ \hline
			Views Registry & (ViewId$^{primary}$, Dummy) & Stores which views are valid, allows us to give each view a unique id \\ \hline
			Views Table & (ViewId, CommitId) & Stores which commits belong to which view. \\ \hline
			\end{tabular}
		\end{center}
\note{Note: \codeName{ViewId} is a foreign key into the views registry, \codeName{CommitId} is a foreign key into Commits Registry. The Dummy column is needed to fix postgresql syntax when we try to get the next key for a table with one column.}		
		
		\paragraph{Schema defined Tables}
				\paragraph{Control Tables}
		\begin{center}
			\begin{tabular}{|p{4cm}||p{4cm}|p{4cm}|} \hline
			Name & Schema & Description \\ \hline
			Object Table & (ObjectId$^{primary}$, generated) & Generated per schema object in schema description. A schema column is generated per schema object field, with an appropriate SQL type. This allows indexing by findables to get object Ids for queries, and extraction of objects from the database. \\ \hline
			Auxiliary Table & (ObjectId, CommitId) & One auxiliary table is associated with each object table. It stores the object Ids associated with each CommitId, which is a many-many relation \\ \hline
			Relation & (LeftId, CommitId, RightId) & Generated per relation in schema description. Each is associated with a commit.  \\ \hline
			\end{tabular}
		\end{center}
		\note{Note: Object-, Left-, and Right- Ids are foreign keys to the \codeName{ObjectTable}'s ObjectId column.}
	\subsection{Query Structure}
	n order to manage the complexity of queries, I have made use of SQL’s Common Table Expressions feature. CTEs act like let expressions in ML \cite{CTEs}.

	The first CTE we use selects all the valid CommitIds.
\begin{framed}
	\begin{verbatim}
WITH RECURSIVE VIEW_CTE AS 
	(SELECT $commitId FROM $viewTable WHERE $viewId = $id)
	\end{verbatim}
\end{framed}

The next collection of CTEs select the valid instances of each relation from the relation tables.

\begin{framed}
\begin{verbatim}
WITH	Relation0 AS (SELECT left_id, right_id 
					FROM REL_0_0 JOIN VIEW_CTE ON REL_0_0.CommitId = VIEW_CTE.CommitId)
\end{verbatim}
\end{framed}

We then select the valid instances from the auxiliary tables the values we use for \mathName{$Id_A$} queries.

\begin{framed}
\begin{verbatim}
AuxTable2 AS (SELECT DISTINCT obj_id AS left_id, obj_id AS right_id FROM AUXUSERSPACE_People_0 JOIN VIEW_CTE ON AUXUSERSPACE_People_0.CommitId = VIEW_CTE.CommitId)
\end{verbatim}
\end{framed}

We then create CTEs for any repeated subqueries (ie those used by \mathName{FixedPoint}, \mathName{Exactly}, and \mathName{Upto})

\begin{framed}
\begin{verbatim}
View1 AS (SELECT * FROM Relation0),
\end{verbatim}
\end{framed}

We then create a CTE for the main query, which exposes a \codeName{leftId} and \codeName{rightId}  column

\begin{framed}
\begin{verbatim}
main_query AS (SELECT left_id, right_id FROM View3 )
\end{verbatim}
\end{framed}

Finally, we join the main query CTE to queries to extract the left and rights .
\begin{framed}
\begin{verbatim}
(SELECT left_table.col_0 AS left_col_0, right_table.col_0 AS right_col_0
	FROM (
		(USERSPACE_People_0 AS left_table JOIN main_query ON left_table.obj_id = main_query.left_id)
		JOIN USERSPACE_People_0 AS right_table
		ON right_id = right_table.obj_id)
)
\end{verbatim}
\end{framed}

The \codeName{main} query itself is built compositionally, in that each subquery exposes a \codeName{leftId} and \codeName{rightId},  allowing queries to be composed to form larger queries in a standard form. A full query might look like this:
	\todo{Colour code this query, make it fit}
\begin{framed}
\begin{verbatim}
WITH RECURSIVE
	VIEW_CTE AS (SELECT CommitId FROM VIEWS_TABLE WHERE ViewId = 1),
	Relation0 AS (SELECT left_id, right_id 
					FROM REL_0_0 JOIN VIEW_CTE ON REL_0_0.CommitId = VIEW_CTE.CommitId),
	AuxTable2 AS (SELECT DISTINCT obj_id AS left_id, obj_id AS right_id FROM AUXUSERSPACE_People_0 JOIN VIEW_CTE ON AUXUSERSPACE_People_0.CommitId = VIEW_CTE.CommitId),
	View1 AS (SELECT * FROM Relation0),
	View3 AS (
		(SELECT left_id, right_id FROM AuxTable2 )
		UNION
		SELECT View3.left_id AS left_id, View1.right_id AS right_id
		FROM
			View3 INNER JOIN View1 ON View3.right_id = View1.left_id
	),
	main_query AS (SELECT left_id, right_id FROM View3 )
(SELECT left_table.col_0 AS left_col_0, right_table.col_0 AS right_col_0
	FROM (
		(USERSPACE_People_0 AS left_table JOIN main_query ON left_table.obj_id = main_query.left_id)
		JOIN USERSPACE_People_0 AS right_table
		ON right_id = right_table.obj_id)
	)
\end{verbatim}
\end{framed}
	\subsection{Monadic Compilation}
	In order to generate these SQL queries, we need to convert the ADT query to a lower level intermediate representation \crossRefNeeded{In appendix}, which maps one-to-one to SQL. While doing this, we need to gather and rename all of the relation and auxiliary tables that we extract from so that we can form the CTE queries, we also need to find repeated queries to hoist out. In order to do this, we use the monadic compiler pattern described above. The compiler state is shown below \todo{name the figure}. When the compilation is done, depending on the context of the command, we append different extraction queries. For find pairs we need to extract the fields of both the left hand side and the right hand side objects, whereas for pathfinding operations, we only need to extract the \codeName{ObjectId}s along the path as opposed to whole  objects. \todo{format diagrams here}
	
	\begin{framed}
	\begin{verbatim}
case class CompilationContext(
        varCount: Int, // Unique identifier for variable names
        relations: Map[ErasedRelationAttributes, VarName], // Relations used by the query
        requiredTables: Map[TableName, VarName], // tables needed for Findables in the query
	    requiredAuxTables: Map[TableName, VarName], // the aux (commit) tables needed for the query
        commonSubExpressions: Map[SubExpression, VarName]
)

	\end{verbatim}
	\end{framed}
	\subsection{Writes}
	There are several steps in the implementation of writes, though I have not expended  a great deal of effort making them fast. A significant part of this is the "insert or get" SQL query, which looks up an object in the relevant table, returning its \codeName{ObjectId} if it exists and creating the object and returning the new ID if it does not.
	
	\begin{framed}
		\begin{verbatim}
WITH insertOrGetTemp AS (
  INSERT INTO $objectTable ($columnNames)
  SELECT $values WHERE NOT EXISTS (SELECT 0 FROM $objectTable WHERE $ValueConditions)
  RETURNING $objId
) SELECT * FROM insertOrGetTemp  UNION ALL (SELECT $objId FROM $name WHERE $ValueConditions)
		\end{verbatim}
	\end{framed}
	
	We create a new view and commit, then we run a memoised \codeName{InsertOrGet} over all the leftmost objects to be inserted, then all the right objects to be inserted, yielding tagged relations between \codeName{ObjectIds}. For each inserted relation, the existing relation instances are removed from those to insert, and the remaining inserted to the relevant \codeName{RelationTable} with the correct \codeName{CommitId}. The auxiliary tables are now updated and, on success, the views table is updated.

	\subsection{Mutability}
	As with the memory backend, all mutability except for the availability of views and the default view is hidden from the user. The SQL backend uses commits to manage view mutability.
	\subsection{Pathfinding and Fixed Point Traversal}
Pathfinding is implemented by constructing an SQL query to generate right hand side \codeName{ObjectIds} for a relation given a left hand side \codeName{ObjectId}. This query is used as the search function for the generic pathfinding algorithms. Once paths have been found, their \codeName{ObjectIds} are looked up in the database to find the full values along each path.

Fixed point traversal and repetitions are done natively in SQL. \codeName{FixedPoint} and \codeName{Upto} are done using a recursive CTE, while \codeName{Exactly} is done by explicitly joining together the required number of repetitions of the sub-relation's query.	
	\begin{framed}
	\begin{verbatim}
	WITH RECURSIVE
    RecursiveCTE AS (
    	(SELECT *, 0 AS Counter FROM BasisCase )
    	UNION
    	SELECT 
    		RecursiveCTE.left_id AS left_id,
    		InductiveCase.right_id AS right_id,
    		Counter + 1 AS Counter
    	FROM RecursiveCTE INNER JOIN InductiveCase
    	ON RecursiveCTE.right_id = InductiveCase.left_id
    	WHERE Counter < Limit
    )
	\end{verbatim}
	\note{An example of a recursive CTE computing \codeName{Upto}. \codeName{FixedPoint} would omit the counter variable and the limit.}
	\end{framed}
	
	\subsection{Object Storage}
	Objects are stored as in the appropriate \codeName{ObjectTable} in a manner derived from the \codeName{DBObject} of each object. Each \codeName{DBCell} is converted to appropriate SQL type.

\section{LMDB Backends}
The final family of backends are the LMDB backends. LMDB is a simple, efficient memory mapped file based Key-Value datastore. \footnote{https://symas.com/lmdb/}.

	\subsection{Common}
	Although I have written several versions of the LMDB backend, there are several overarching similarities.
		\paragraph{LMDB API}
			\subparagraph{Terminology}
			LMDB terminology differs slightly from that of more mainstream systems. What would be called a database in SQL is known as an \term{Env} and a table known as a \term{DBI}.
			\subparagraph{Transactions}
				LMDB's transactional model differs from more traditional systems. In short, we need to create a transaction to do reads but not writes. This stems from LMDB delegating as much work as possible to the OS kernel's memory-mapped file system. All writes are immediately written to the database, whereas starting a read transaction gives a view of the \term{Env} as it was at the start of  the transaction, ensuring that concurrent writes don't affect what is read from the database. Writes may be included in transactions to allow atomic get-and-sets. In practice the transactions we actually use are very short lived.
			\subparagraph{JVM API}
			The LMDB backend uses the \codeName{LMDBJava} API \footnote{//github.com/lmdbjava/lmdbjava} to allow access to an LMDB Database from JVM languages (such as Scala). For each \term{DBI}, we have a byte array to byte array key-value map, much like a \codeName{Map[Array[Byte], Array[Byte]]}.
			
		\paragraph{Storage and Keys}
			In order to write to the database using the LMDBJava API, we need to convert the key and value to \codeName{ByteBuffer}. To generify this process I have introduced two type classes: \codeName{Keyable} and \codeName{Storable}.
			
\begin{framed}
\begin{verbatim}
trait Keyable[K] { def bytes(k: K): Array[Byte]}

trait Storable[A] {
 def bufferLength(a: A): Int
 final def toBuffer(a: A): ByteBuffer = 
    {/*Allocates a buffer and fills it using declared methods*/}
 def writeToExistingBuffer(a: A, buf: ByteBuffer): Unit
 def fromBuffer(buf: ByteBuffer): LMDBError \/ A
}
\end{verbatim}
\end{framed}			
\codeName{Keyable} is a type class which shows that an object can be converted into an array of bytes representing a component of a key (Can be concatenated with others to make a full key). \codeName{Storable} is a more general typeclass that shows that objects can be converted into a \codeName{ByteBuffer}. \codeName{Storable} objects are typically more complex than objects used as keys, and have a higher marshalling throughput, hence the lower level, faster, \codeName{ByteBuffer} API. Typeclass instances exist to allow sets and lists of \codeName{Storable} objects to be stored.	
			
		\paragraph{Table Structure}
	
 Similarly to the SQL implementation, there are several tables, some generated based on the \codeName{SchemaSescription} and other, control, tables exist regardless. Each table is represented in memory by a subclass of the \codeName{impl.lmdb.common.tables.interfaces.LMDBTable} trait, which provides utility methods such as transactional reads and computations.
 
\begin{center}
	\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}||p{3cm}|}
 	\hline
 		
	Name & Key Type & Value Type  & Description \\ \hline
	Object Counter & Singleton & ObjId & Atomic counter holds the next ObjectId \\ \hline
	Commit Counter & Singleton & CommitId & Atomic counter holds the next CommitId \\ \hline
	View Counter & Singleton & ViewId & Atomic counter holds the next ViewId \\ \hline
	Default table & Singleton & ViewId & Mutably stores the default view \\ \hline
	Available Views & Singleton & Set[ViewId] & Mutably stores a set of available views \\ \hline
	Views Table & ViewId & List[CommitId] & Maps a view to its constituent commits \\ \hline
	Relations & (ObjectId, CommitId, RelationName) & Set[ObjId] & Single table stores all forward relations in a database \\ \hline
	Reverse Relations & (ObjectId, CommitId, RelationName) & Set[ObjId] & Single table stores the reverse of all relations in the database \\ \hline
	\end{tabular}
\end{center} 
 
 \note{Singleton Keys denote tables with single key.}
 \begin{center}
	\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}||p{3cm}|}
 	\hline
	Name & Key Type & Value Type  & Description \\ \hline
	Retrieval Table & ObjectId & DBObject & Used to retrieve object data at end of a query \\ \hline
	Empty Index & CommitId & ObjectId & Stores object Ids accessible a view. Used to compute Id_A. \\ \hline
	Column Index &  (Commit, DBCell) & ObjectId & Provides an index over a field of a SchemaObject \\ \hline
	\end{tabular}
\end{center}

For each \codeName{SchemaObject} in the \codeName{Schemadescription}, there exists a \codeName{RetrievalTable}, \codeName{EmptyIndexTable}, and a \codeName{ColumnIndexTable} per column in the \codeName{SchemaObject}.
		
		
		\paragraph{Writes}
		Writes occur in a similar way to the SQL implementation, again using commits to manage views. First we look up all the left and right hand side objects to find their \codeName{ObjectIds} (If needed, we create new entries in the retrieval table and update the index tables to insert the objects.), we then look up existing instances of the relations to insert and only insert new ones to the relation table with the new \codeName{CommitId}. Upon success, we create a new view and insert it to the views table and the available views table.

	\subsection{Original LMDB Implementation}
	\note{impl.lmdb.original}

The original LMDB backend implementation executed queries in much the same way as the in-memory backend, the only difference being the flat LMDB table structure. Interpretation occurs by a very similar to function that of the memory backend, instead passing around a list of Commits to search rather than the memory tree. At the end of a query's execution, the resulting \codeName{ObjectId}s are looked up in the relevant Retrieval table to extract the actual objects and convert them into user objects. Pathfinding and the repetition operators are handled in the same way as the memory backend by calling the relevant generic algorithms.
 

	\subsection{Batched}
	\note{impl.lmdb.fast}
	This is the first variant upon my simple original LMDB backend, it focuses on making small local optimisations rather than significant algorithmic changes.
	
		\paragraph{Read Batching}
		Firstly, the original implementation made suboptimal uses of reads. For example, when extracting objects at the end of query execution, it would look up each object individually with its own read transaction. This is sped up by batching together reads to separate keys and commits in the same transaction, allowing for a small speed up.
		
		\paragraph{Pre-Caching}
		The result of executing a \codeName{FindSingle} is constant throughout the whole query, so by lifting out and pre-emptively executing any \codeName{FindSingle}s, we speed up execution by removing redundancy. More importantly, this helps enable the next operation.
		
		\paragraph{FindFrom}
		For the pathfinding commands, the transitive queries and \codeName{From} \codeName{FindSingle} query, we're not actually interested in finding all pairs matching a query, or interested in which pairs are related, but actually the reachability function \codeName{$A \Rightarrow Set[B]$} (this is a kleisli arrow), which takes a node and finds the directly reachable neighbours. Since we do not have to do any bookwork to maintain the left hand side of any relation, this greatly simplifies our search. Hence, in these cases, we interpret the query using a new \codeName{FindFrom} method, which is an alternative interpreter. For example, joins now become a concatenation of these arrows (\codeName{flatMap}  in Scala parlance), \codeName{Id} becomes the return function \codeName{($x \Rightarrow Set(x)$)}, \codeName{Distinct} becomes set subtraction as opposed to calling the \codeName{Set.filter} function, and, finally, we can make use of the \codeName{SimpleFixedPointTraversal} algorithms described above. The pre-caching above is useful here, since due to the \codeName{flatMap}s, the computation of \codeName{FindSingle}s would be repeated many times otherwise.

	\subsection{Common Subexpression Elimination}
	\note{impl.lmdb.cse}
	A common performance issue in the previous implementation was that of redundancy and lack of general common subexpression elimination (CSE). When a common sub expression is repeated outside of an \codeName{Upto} or \codeName{Exactly} block, then it is repeatedly recalculated, leading to poor performance. In addition, there is lots of redundancy exposed by the above \codeName{FindFrom} optimisation. Consider calculation $R join (S join T)$. If a node $n$ appears in the results of $S(m)$, $S(m')$ for distinct $\opRule{(?, m)}{A, B}{R}$, $\opRule{(?, m')}{A, B}{R}$ ($m$, $m'$ are right hand results of R), then $T(n)$ is computed redundantly in the overall calculation. These issues of redundancy can be solved using a combination of global CSE and memoization.

		\paragraph{The Memoisation Problem}
		When looking at a graph data structure, memoization is harder than when looking to optimise pure functions. To optimise a pure function $f\colon A \Rightarrow B$, we only need to store a hash table \codeName{Map[A, B]}. To compute $f(a)$, we simply look up $a$ in the map. If it is in the map, we return the mapped value else, we call $f$, and add the result to the map before returning. In a large graph, computing and storing all pairs generated by a query is extremely wasteful, since many of those pairs might never be used (effectively, we would have ignored the left-optimisation). Conversely, memoizing the full left-optimised function, $interpret(Q)\colon Set[A] \Rightarrow Set[(A, B)]$ is also wasteful, since there may be overlap in the sets used as keys. Instead, we memoise the \codeName{FindFrom} function of a query, $findFrom(Q)\colon A \Rightarrow Set[B]$, and reconstruct results from the pairs. This allows for the best overlap.

To make the best use of memoization, and to overusing memory by memoizing the same query twice, we need to also apply CSE to group together instances of the same query.

		\paragraph{Retrievers}
		A solution to these parameters is the use of an object called a \codeName{Retriever}. This exposes two methods which mirror those from interpreting a query. 
		
		\begin{framed}
			\begin{verbatim}
trait RelationRetriever extends Logged {
     def find(from: Set[ObjId]): LMDBEither[Set[(ObjId, ObjId)]]
     def findFrom(from: ObjId): LMDBEither[Set[ObjId]]
}
			\end{verbatim}
		\end{framed}
		For each subquery node in a query(e.g. \mathName{And(Id_A, Or(R_1, R_2 ))}), we generate a retriever. Retrievers for primitive relations (e.g. \mathName{Id_A} , \mathName{Rel(R)}) are uncached, as LMDB allows for very fast re-computations of these. In the other cases, we use a CachedRelationRetriever, which memorises an underlying lookup function. Using type-enrichment, I have implemented methods such as \codeName{join}, \codeName{and}, \codeName{or}, \codeName{exactly}, \codeName{fixedPoint} on \codeName{RelationRetrievers}, allowing them to be composed to mirror the query they are generated from. This memoises every node of the query to be executed, yielding a significant reduction of redundancy.
	
		
		\paragraph{Monadic Compilation}
		To avoid storing multiple \codeName{RelationRetriever}s for the same subexpression, it is useful to reuse the same retriever for every occurrence of a subquery. Hence when building a retriever for a query, we want to keep track of all subqueries we've seen before. This can be done using the monadic compilation pattern above. The compilation state stores a hashmap of all the subtrees that have already been computed (A bit like when constructing a binary decision diagram in automated theorem proving). 
		
\begin{framed}
\begin{verbatim}
class QueryMemo(
    val pairs: Map[UnsafeFindPair, RelationRetriever], 
    val singles: Map[UnsafeFindSingle, SingleRetriever]
)
\end{verbatim}
\note{Compilation state}
\end{framed}
		The compilation function checks the memo for precomputed values, and if they are not already computed, recurses to compute subtrees, then composes the subtree results to get a new retriever for the current node, which is added to the compilation state.
		
	\subsection{Complex Common Subexpression Elimination}
	\note{impl.lmdb.fastjoins}
Despite these optimisations, there are still sources of redundancy.

		\paragraph{Index Building}
			Firstly, the original CSE implementation uses the excessively generic join function to join result sets. This function wastes time by computing an index map on every call. So instead we can change the definition of a \codeName{Retriever}'s public functions to mitigate the need to build indices.
			\begin{framed}
			\begin{verbatim}
			trait RelationRetriever extends Logged {
  def find(from: Set[ObjId]): LMDBEither[Map[ObjId, Set[ObjId]]]
  def findFrom(from: ObjId): LMDBEither[Set[ObjId]]
}
			\end{verbatim}
			\end{framed}
			Indeed, the function to join a pair of retrievers becomes a simpler \codeName{flatmap} of one map to another. Functions such as the \codeName{union} and \codeName{intersection} of retrievers also become simpler, \note{(see impl.lmdb.fastjoins.retrievers.RelationRetriever.RelationRetrieverOps)}

		\paragraph{Exactly}
		This implementation also explores other formulations of the \mathName{Exactly(n, P)} query. Exactly effectively joins together n repetitions of the underlying query in a sequential fashion. This makes relatively poor reuse of the join function, since each join has different operand subqueries, so it cannot be memoised. This formulation does make good reuse of \mathName{P}, however.
		

\begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{figs/SequentialJoins.png}
  \caption{Sequential joins provide little common substructure to eliminate.}
  \label{fig:SequentialJoins}
\end{figure}
		
		
		As joins over the set of results of query form a \mathName{Monoid} \crossRefNeeded{Link to appendix}, we can use associativity to change the order in which the joins are evaluated, aiming for as much reuse as possible. This can be done in a manner similar to square-and-multiply exponentiation.

We calculate retrievers for $P^{2^i}$ for each $i$ less than the bitlength of $n$. We then join the relevant retrievers to to get a retriever for $P^n$. This only uses $O(Log_2(n))$ distinct joins, and makes very good reuse of the join functions. 

\begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{figs/CommonJoins.png}
  \caption{By reordering joins, we gain significant common substructure to eliminate.}
  \label{fig:CommonJoins}
\end{figure}

\begin{framed}
\begin{verbatim}
    def exactly(n: Int): RelationRetriever = repeat(n, outer, IdRetriever)

    private def repeat(
        n: Int,
        acc: RelationRetriever,
        res: RelationRetriever
    ): RelationRetriever = if (n > 0) {
      val newRes = if((n & 1) == 1)  (res join acc) else res
      val newAcc = acc join acc
      repeat(n >> 1, newAcc, newRes)
    } else res
\end{verbatim}
\end{framed}

		\paragraph{Upto Optimisation}
			We can also re-formulate an \mathName{Upto(n, P)} as an \mathName{$Exactly(n, Or(P, Id_A))$} \crossRefNeeded{link to Proof in appendix}, and hence use the same optimisation as above on \mathName{Upto}.  The same cannot be said for \mathName{FixedPoint}, as we do not know statically at what point the underlying relation will converge, so cannot pick an n to compute \mathName{Upto(n, P)}.
			
			\begin{framed}
			\begin{verbatim}			
    def upto(n: Int): RelationRetriever =
        repeat(n, outer or IdRetriever, IdRetriever)
			\end{verbatim}
			\end{framed}



\chapter{Evaluation}
\section{Unit Tests}
The correctness of each backend is tested using a suite of 45 unit tests which verify adherence to semantics. These have  wide range, testing over all the commands, all the possible ADT nodes, and the correct usage and separation of views. I have been using the regression test model, in that discovering a non-trivial bug, I've written a test case to target that bug, ensuring that it is not leaked into production again.
\section{Performance Tests}
	In order to evaluate the effectiveness of the various optimisations to the LMDB backends described above, it was necessary to run performance tests over wide range of queries. The various LMDB backends were tested against each other as a control and particularly against the SQL backend as a standard to beat. The in-memory backend was omitted as initial tests indicated that it runs with approximately the same algorithmic characteristics as the Original LMDB implementation. The LMDB implementation was able to do indexing faster than Scala's tree based hash maps, which are used by the in-memory implementation. Further more, the original LMDB implementation is typically around 3x slower than the batched version for most jobs, so we also omit it for most tests, since most of the algorithms it uses are the same.
	\subsection{Hardware}
	Tests were run on the oslo machine belonging to Timothy Jones' group. Its specifications are shown below. All of the backends ran off of an SSD.
	\todo{Oslo Specs}
	\subsection{Datasets}
	To evaluate tests on non-trivial examples, I sought to construct datasets over which large queries were feasible.

		\paragraph{IMDB and TMDB}
		The first and most used collection of tests are derived from the TMDB movie database,the most popular 5000 of which were collected from kaggle \cite{TMDB5000}

The CSV data was processed using a python script into a simplified JSON format. A separate, larger dataset derived from IMDB was constructed from tests for another graph database \cite{IMDB} and manually (in python) converted to the same JSON format. I then wrote a Scala script which reads the JSON and writes the relations to a given database instance.

Slightly different parameters were given when generating each dataset.

\begin{center}
	\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}||p{3cm}|  }	
	\hline
        Name & Movies Size & People Size & Notes \\ \hline
        smallest & 100 & 10351 & 100 movies with most actors \\ \hline
        small & 250 & 18167 & 250 movies with most actors \\ \hline
        small _sparse & 250 & 1029 & 250 movies with fewest actors \\ \hline
        medium & 1000 & 35626 & 1000 movies with most actors \\ \hline
        medium_sparse & 1000 & 6310 & 1000 movies with fewest actors \\ \hline 
       large & 12862 & 50134 & Largest dataset \\ \hline
	\end{tabular}
\end{center}

The objects in the database are as follows:
\paragraph{Objects}
\begin{center}
	\begin{tabular}{ |p{3cm}| p{3cm}|}
	\hline
	Name & Fields \\ \hline
	Person & Name: String \\ \hline
	Movie & Name: String Language: String \\ \hline
	Date & Time: Long \\ \hline
	Place & Place: String \\ \hline
	Genre &	Genre: String \\ \hline
	\end{tabular}
\end{center}

\paragraph{Relations}
\begin{center}
	\begin{tabular}{ |p{3cm}|| p{3cm}| p{3cm}|}
	\hline
		Name &  From & To \\ \hline
		ActsIn & Person & Movie \\ \hline
		Directed & Person & Movie\\ \hline
		HasBirthday &Person & Date\\ \hline
		BornIn & Person & Place\\ \hline
		HasGenre & Movie & Genre\\ \hline
	\end{tabular}
\end{center}

		\paragraph{UFC}
		
		A second dataset, built from UFC fight data \cite{UFC}, was constructed in the same way to produce one JSON dataset.
		
		\paragraph{Objects}
\begin{center}
	\begin{tabular}{ |p{3cm}| p{9cm}|}
	\hline
		Name & Fields \\ \hline
		Person & Name: String, Height: Int, Weight: Int\\ \hline
	\end{tabular}
\end{center}

\paragraph{Relations}
\begin{center}
	\begin{tabular}{ |p{3cm}|| p{2cm}| p{2cm}||p{3cm}|}
	\hline
		Name &  From & To & notes \\ \hline
		Beat & Person & Person & \\ \hline
		ShorterThan &  Person & Person & Transitive reduction of the shorter than relation \\ \hline
		LighterThan & Person & Person & Transitive reduction of the lighter than relation \\ \hline

	\end{tabular}
\end{center}
		
		The \codeName{ShorterThan} and \codeName{LighterThan} relations produce extremely sparse graphs which take a long time to converge under transitive closure. \todo{Size of graph.}
	\subsection{Test Harness}
	In order to run tests against each other, I have written a typesafe test harness to run on oslo. This standardises the interface that individual test instances must implement. Each Test must have a \codeName{setup} method and a \codeName{test} method which is run on a \codeName{DBInstance} with the test method indexed by a \codeName{TestIndex}. The test specification must give a maximum index and a mask to avoid running inappropriate backends (For example, those that might take too long on a large test.) The benchmarks that I have run test only the read speed. The time taken to construct the database is not included.
	
	\subsection{Results}
		\paragraph{Overall Picture}
		An overall view of the results is that, as might be expected, the SQL implementation is the fastest over short, indexing heavy, queries, with the most aggressively optimised LMDB implementation typically performing slightly worse than less aggressively optimised LMDB implementations due to the overheads of optimisation. However, when queries expose the redundancies noted in the preceding sections, significant speed ups can be seen, both over the other LMDB implementations and over SQL. This can particularly be seen in the cases of \codeName{FindPair} transitive queries.

		A point to note is that I am ignoring evaluation of pathfinding queries. For simplicity, the pathfinding implementation re-evaluates the underlying search function query at each stage of the breadth first search. This seems to particularly punish the SQL implementation, to the extent that it is around 2000x slower than the LMDB reference implementation. Stateful solutions to speed this up might include creating a temporary table in the database to store the subgraph being searched, but this complicates the SQL implementation significantly (clearing up on error becomes a lot harder). Hence, I am ignoring the pathfinding tests for the purpose of backend comparison.


		\paragraph{Redundancy}
			This test is to demonstrate the removal of redundancies in \codeName{FindSingle} commands. There is clearly a major increase in speed due to the introduction of CSE and memoisation. Since this is a \codeName{FindSingle} query, there is no usage of the optimised join formulations, so there is no speed gain by the most optimised LMDB backend. Postgres fails to fully optimise away the redundancy, even with use of the \note{DISTINCT} modifier of queries, and ends up running out of temporary space on this query.
				
\resultTable{Redundancy}{4 each}{
\texttt{KevinBacon >> coactor}\\
\texttt{KevinBacon >> (coactor -->--> (coactor -->--> coactor))}\\
\texttt{KevinBacon >> (coactor -->--> (coactor -->--> (coactor -->--> 	coactor)))}\\

Where

\texttt{def coactor = ActsIn --><-- ActsIn}
}{ \reference   	& 	381,972 & 1\\
 \cse 			&   379,280 & 0.993 \\
 \batched 		&	9,318,741 & 24.4 \\
 \postgres    	&	Did not finish & Did not finish \\
} 

\begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{scripts/Redundancy.png}
  \caption{Read performance for redundancy queries}
  \label{fig:RedundancyResult}
\end{figure}

		\paragraph{Conjunctions and Disjunctions}
	These two tests make use of the \mathName{And} and  \mathName{Or} operators to combine several subqueries with some underlying common structure. Both resulted in similar performance patterns. The postgres implementation significantly outperformed all LMDB implementations, while the CSE related optimisations appeared to add overheads greater than the redundancy they removed. I expect the reason for this is that there is still not a large amount of redundancy exposed by the underlying sub queries, and that the implementation of unions and intersection in Scala sets is slower than the optimised C implementation used by Postgres. In addition, constructing a conjunction and disjunction over the memoisation in the LMDB implementation also adds some overhead and reduces locality of reference.	

\resultTable{Conjunctions}{40 each}{
\texttt{coactorWith(KevinBacon))}       \\
\texttt{coactorWith(KevinBacon) \& coactorWith(TomCruise))}\\
\texttt{coactorWith(KevinBacon) \& coactorWith(TomCruise) \& coactorWith(TomHanks))}\\
Where\\
\texttt{def coactorWith(a: Person) = ActsIn --> (a >> ActsIn) <-- ActsIn} \\
}{ 
 \reference 			&   879,984 & 1 \\
 \cse 					&	827,734 & 0.941	\\
 \batched 				&   791,678 & 0.900 \\
 \postgres    			&	106,165 & 0.121 \\
}
\begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{scripts/Conjunctions.png}
  \caption{Read performance for conjunction queries}
  \label{fig:ConjunctionResults}
\end{figure}

					\resultTable{Disjunctions}{40 each}{\texttt{coactorWith(KevinBacon))}       \\
\texttt{coactorWith(KevinBacon) | coactorWith(TomCruise))}\\
\texttt{coactorWith(KevinBacon) | coactorWith(TomCruise) | coactorWith(TomHanks))}\\
Where\\
\texttt{def coactorWith(a: Person) = ActsIn --> (a >> ActsIn) <-- ActsIn} \\
}{ 
 \reference 			&   853,664 & 1 \\
 \cse 					&	824,799 & 0.966	\\
 \batched				&   800,455 & 0.938 \\
 \postgres    			&	112,488 & 0.132 \\
 }	
 
 \begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{scripts/Disjunctions.png}
  \caption{Read performance for disjunction queries}
  \label{fig:DisjunctionResults}
\end{figure}

		\paragraph{Tests that involve repetitions}
		These are a suite of benchmarks for testing the optimisations and performance on repetitive queries. A general overview is that the join-reordering optimisations strongly sped up benchmarks which ran \codeName{FindPairs} queries, but in \codeName{FindSingles} queries, the overhead of this formulation did not compete as strongly with the fast \codeName{SimpleFixedPointTraversal} methods. SQL performed strongly in \mathName{Upto} queries which use built in, optimised, recursive CTEs to calculate results, but less strongly in \mathName{Exactly} queries which do not have a built-in formulation.

			\subparagraph{Exactly Test}
			This test runs \codeName{FindSingle} \mathName{Exactly} queries of varying lengths over the small movies database. As explained, the \codeName{SimpleFixedPointTraversal} \codeName{exactly} method is faster than any gains by the join optimisations in the optimised LMDB version. Postgres also performed poorly due to the lack of a recursive CTE for self joins.
	\resultTable{Exactly}{50}{
	\texttt{TomHanks >> (}\\
	\texttt{((ActsIn -->(KevinBacon >> ActsIn))<-- ActsIn) * (index.i \% 10)}	\\
	\texttt{)}
}{ 
 \reference 			&  	1,752 	& 1 \\
 \cse 					&	1,798 	& 1.03	\\
 \batched				&   1,513 	& 0.834 \\
 \postgres    			&	102,461 & 58.5 \\
 }
 
 \begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{scripts/ExactlyTest.png}
  \caption{Read performance for \codeName{Exactly} queries}
  \label{fig:ExactlyResults}
\end{figure}

 \begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{scripts/ExactlyTestLog.png}
  \caption{Log-Read performance for \codeName{Exactly} queries}
  \label{fig:ExactlyTestLog}
\end{figure}
			\subparagraph{Exactly Pairs}
This test instead runs a \codeName{FindPair} variable length \mathName{Exactly} query over the small movies database. Since this test involves keeping track of the root for each pair found, the \mathName{Exactly}-optimised LMDB implementation significantly outperforms the other LMDB implementations based on generic algorithm. Postgres also performs well in this test, will minimal slowdown from the find single queries. The LMDB implementations, however, do experience a slowdown.
	\resultTable{ExactlyPairs}{50}{
	\texttt{((ActsIn -->(KevinBacon >> ActsIn))<-- ActsIn) * (index.i \% 10))}}{ 
 \reference 			&  	83,269 		& 1 \\
 \cse 					&	1,792,388 	& 21.5 \\
 \batched				&   1,787,390 	& 21.5 \\
 \postgres    			&	107,797 	& 1.29 \\
 }	

\begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{scripts/ExactlyPairs.png}
  \caption{Read performance for exactly \codeName{FindPair} queries}
  \label{fig:ExactlyPairsResults}
\end{figure}

			\subparagraph{UptoTest}
			
				This test runs \mathName{Upto} based \codeName{FindPair} queries over the large IMDB database. Here the upto-optimisation of the final LMDB backend provides significant speed-ups, such as 225x on the other LMDB backends and 1.5x over postgres.
				\resultTable{Upto}{10}{
				\texttt{((ActsIn -->(KevinBacon >> ActsIn))<-- ActsIn).*(0 --> index.i)}
}{ 
 \reference 			&  	126,448 	& 1 \\
 \cse 					&	28,491,910 	& 225 \\
 \batched				&   28,423,658 	& 225 \\
 \postgres    			&	197,880 	& 1.56 \\
 }	
 
\begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{scripts/Upto.png}
  \caption{Read performance for \codeName{Upto} queries}
  \label{fig:UptoTestResult}
\end{figure}

 \begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{scripts/UptoLog.png}
  \caption{Log-Read performance for \codeName{Upto} queries}
  \label{fig:UptoLogResult}
\end{figure}
			\subparagraph{UptoLarge}
			
				Since the more naive LMDB instances took seven hours each to complete the above test, I re-ran the benchmark for only the reference and Postgres implementations for a longer period of time, yielding a similar ratio of performance.
				\resultTable{Upto}{10}{
				\texttt{((ActsIn -->(KevinBacon >> ActsIn))<-- ActsIn).*(0 --> (index.i \% 10))}}{ 
 \reference 			&  	1,059,091 	& 1 \\
 \postgres    			&	1,557,314 	& 1.47 \\
 }
 
 \begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{scripts/UptoLarge.png}
  \caption{Read performance for large \codeName{Upto} queries}
  \label{fig:UptoLargeResults}
\end{figure}
	
			\paragraph{JoinSpeed}
				This test is  to demonstrate the cost of joins, we interleave queries that calculate two subqueries and then combine them using, firstly, a relatively fast And and, secondly, instead, a higher complexity join. In theory, the difference between these two types of queries should give us an idea of the cost of a join.
			\resultTable{Join Speed}{150 each}{
			\texttt{(ActsIn --><-- ActsIn) \& (ActsIn --><-- ActsIn)}\\
\texttt{(ActsIn --><-- ActsIn) -->--> (ActsIn --><-- ActsIn)}
			}{
			\reference   	& 	368,320 & 1 	\\
 			\cse 			&   525,992 & 1.43   	\\
			\batched 		&	543,909 & 1.48	\\
			\postgres    	&	289,352 & 0.786	\\}
			
\begin{figure}[ht]
\centering
  \includegraphics[width=\textwidth]{scripts/JoinSpeed.png}
  \caption{Read performance for Join-Speed queries}
  \label{fig:JoinSpeedResults}
\end{figure}
						\todo{More detail}
			\paragraph{Closing Thoughts}
			These benchmarks provide some insights into the strengths and weaknesses of the various optimisations I have written. An option to produce a performant back-end might be to produce heuristics for estimating graph density and the relative performance of the various LMDB backends upon it to create an adaptive backend. Since all the LMDB backends use the same underlying database format (indeed the code for constructing and writing to the database remains the same), a backend could choose at query time which strategy it wants to use to execute each individual query.


	
\chapter{Conclusion}

\section{Successes}
To the best of my knowledge, the graph database system I have built over the last seven months is one of the, if not the, first explicitly purely functional graph database systems in existence. This project is a demonstration that complex, performant software projects can be built using purely functional languages, whilst keeping the benefits of strong type systems. I feel that despite having written the project in Scala, a high level, often inefficient language I have achieved an acceptable level of performance for graph traversal. Although the system would need significant work in areas of security, networked access, and tests and tweaks on real world use cases to become commercially viable, it would be interesting to see if companies known for their use of functional programming, especially finance firms, might find use cases for graph databases

\section{Further extensions}
Interesting extensions to the core project that, given more time, I would have been interested to explore include
\begin{itemize}
 \item An increase in the use of laziness to allow larger queries over larger graphs. Currently, the size of queries are limited by how large the result sets of sub-queries. However, the Postgres implementation typically runs out of temporary file space before this happens.
 \item The implementation of deletions and garbage collection of unused views of the database. Garbage collection in particular might be difficult, as the database is not required to store any state about client instances.
 \item Exploration of  improvement of the schema system using a dependent types library, such as shapeless.
 \item Rewriting the lowest layer of the LMDB backends' interpreters in C or another low level language to extract maximum performance from the LMDB system, without having to construct Scala collections at every step.
\end{itemize}

\section{Lessons learned}
	In hindsight, I might aimed to build a narrower project, focusing on a single backend and comparing  performance against a widely used graph database such as Neo4j. Although I very much enjoyed the chance to build a large project in a purely functional style, implementing the large number of moving parts in the project distracted from implementing more interesting optimisations and really digging down to extract all redundancy.

\section{Concluding Thoughts}
I hope for another chance to explore this space as I feel there are still many areas worth investigating.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{diss}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix
\addcontentsline{toc}{chapter}{Appendix}
\chapter{The Domain \clos}

For the various semantics proofs it is necessary to define the Scott domain $\clos$ for some object type $A$ and $View_{\Sigma}$ $v$. This domain is the set of subsets $x$ such that $\deno{Id_A} \subseteq x \subseteq A \times A$ with bottom element $\bot = \deno{Id_A}$ and partial order $x \sqsubseteq y \Leftrightarrow x \subseteq y$. From this point on, I shall use $x \subseteq y$ to mean $y \sqsubseteq x$.

\hfill\begin{minipage}{\dimexpr\textwidth-1cm}
\paragraph{$\clos$ is a domain}


Firstly, by definition, \[\forall x. x \in \clos \Rightarrow x \supseteq \deno{Id_A}\] hence $\deno{Id_A}$ is the bottom element.

Secondly for any chain $ x_1 \subseteq x_2 \subseteq x_3 \subseteq ...$, $x_i \in \clos$, there exists a value $\bigsqcup_n x_n \in \clos$ such that $\forall i. \bigsqcup_n x_n \supseteq x_i$ and $\forall y. (\forall i. x_i \subseteq y) \Rightarrow y \supseteq \bigsqcup_n x_n$

\paragraph{proof:}

Take $\bigsqcup_n x_n = \bigcup_n x_n$. This is in $\clos$, since both $\bigcup_n x_n \supseteq \deno{Id_A}$ due to $\forall i. x_i \supseteq \deno{Id_A}$ by definition and $\bigcup_n x_n \subseteq A \times A$, by \[\forall a. (a \in \bigcup_n x_n \wedge \neg (a \in A \times A)) \Rightarrow (\exists i. a \in x_i \wedge \neg a \in A \times A ) \Rightarrow (\exists i. \neg x_i \subseteq A \times A)\] yielding a contradiction if $\bigcup_n x_n \subseteq A \times A$ does not hold.
$\\ $
We know $\forall i. \bigcup_n x_n \supseteq x_i$ by definition, so it is an upper bound. $\\ $
To prove it is a least upper bound, consider $y$ such that $(\forall i.) y \supseteq x_i$ 

\begin{equation}
\label{LeastUpperBound}
\begin{split}
\forall a. (\exists i. a \in x_i) & \Rightarrow a \in y \\
\forall a. \bigvee_n (a \in x_n) & \Rightarrow a \in y \\
\forall a. (a \in \bigcup_n x_n) & \Rightarrow a \in y \\
\therefore \bigcup_n x_n & \subseteq y\\
\square
\end{split}
\end{equation}

\xdef\tpd{0pt}
\end{minipage}

\prevdepth\tpd

\chapter{Correspondence of operational and denotational semantics}
\todo{neaten}
In order to use the denotational semantics to construct an interpreter or compiler we need to prove they are equivalent to the operational semantics. Namely:

For any pair query $P$, schema $\Sigma$ and $View_{\Sigma}$ $v$:
$$
\typeRule{P}{(A, B)}\Rightarrow \opRule{(a, b}{(A, B)}{P} \Leftrightarrow \denoRule{(a, b)}{P})
$$

And for any single query $S$, schema $\Sigma$ and $View_{\Sigma}$ $v$: 
$$
\typeRule{S}{A}\Rightarrow \opRule{(a}{A}{S} \Leftrightarrow \denoRule{a}{S})
$$

In order to prove these two propositions, we define two induction hypotheses



$$
\phiRule{P}{A}{B} \Leftrightarrow (\typeRule{P}{(A, B)} \Rightarrow \forall v \in View_{\Sigma}, (a, b) \in A \times B. (\opRule{(a, b)}{(A, B)}{P}) \Leftrightarrow \denoRule{(a, b)}{P})))
$$

$$
\psiRule{S}{A} \Leftrightarrow (\typeRule{S}{A} \Rightarrow \forall v \in View_{\Sigma}, a \in A. (\opRule{a}{A}{S}) \Leftrightarrow (\denoRule{a}{S}))
$$

Now we shall induct over the structures of $P$ and $S$ starting with the SingleQuery cases


\paragraph{Case $S = AndS(S', S'')$}
\begin{equation} \label{case AndS}
\begin{split}
\denoRule{a}{S} & \Leftrightarrow a \in (\deno{S'} \cap \deno{S''}) \\
				& \Leftrightarrow (\denoRule{a}{S'} \wedge \denoRule{a}{S''}) \\
				& \Leftrightarrow (\opRule{a}{A}{S'} \wedge \opRule{a}{A}{S''}) \mbox{ by $\psiRule{S'}{A}$, $\psiRule{S''}{A}$}\\
				& \Leftrightarrow \opRule{a}{A}{AndS(S', S'')} \mbox{ by inversion of (AndS)}
\end{split}
\end{equation}

\paragraph{Case $S = OrS(S', S'')$}
\begin{equation} \label{case OrS}
\begin{split}
\denoRule{a}{S} & \Leftrightarrow a \in (\deno{S'} \cup \deno{S''}) \\
				& \Leftrightarrow (\denoRule{a}{S'} \vee \denoRule{a}{S''}) \\
				& \Leftrightarrow (\opRule{a}{A}{S'} \vee \opRule{a}{A}{S''}) \mbox{ by $\psiRule{S'}{A}$, $\psiRule{S''}{A}$}\\
				& \Leftrightarrow \opRule{a}{A}{OrS(S', S'')} \mbox{ by inversion of (OrS)}
\end{split}
\end{equation}

\paragraph{Case $S = From(S', P)$}
\begin{equation} \label{case From}
\begin{split}
\denoRule{b}{S} & \Leftrightarrow \exists a \in A. \indent (\denoRule{a}{S'} \wedge \denoRule{(a, b)}{\deno{P}}) \\
				& \Leftrightarrow \exists a \in A. \indent (\opRule{a}{A}{S'} \wedge \opRule{(a, b)}{(A, B)}{P}) \mbox{ by $\psiRule{S'}{A}$, $\phiRule{P}{A}{B}$}\\
				& \Leftrightarrow \opRule{b}{B}{From(S', P)} \indent\mbox{ by inversion of (OrS)}
\end{split}
\end{equation}

\paragraph{Case $S = Find(f)$}
\begin{equation} \label{case Find}
\begin{split}
\denoRule{a}{S} & \Leftrightarrow a \in v(A) \wedge f(a) \downarrow True  \indent \mbox{ by inversion of the type rule (Find)}\\
				& \Leftrightarrow \opRule{a}{A}{Find(f)} \mbox{ by definition}\\
\end{split}
\end{equation}

Now, looking at the FindPair queries

\paragraph{Case $P = Rel(r)$}
\begin{equation} \label{case Rel}
\begin{split}
\denoRule{(a, b)}{P} & \Leftrightarrow (a, b) \in v(r)\\
				& \Leftrightarrow \opRule{(a, b)}{(A, B)}{Rel(r)} \mbox{ by definition}\\
\end{split}
\end{equation}

\paragraph{Case $P = RevRel(r)$}
\begin{equation} \label{case RevRel}
\begin{split}
\denoRule{(a, b)}{P} & \Leftrightarrow (b, a) \in v(r)\\
				& \Leftrightarrow \opRule{(a, b)}{(A, B)}{RevRel(r)} \mbox{ by definition}\\
\end{split}
\end{equation}

\paragraph{Case $P = Id_A$}
\begin{equation} \label{case Id_A}
\begin{split}
\denoRule{(a, b)}{P} & \Leftrightarrow a \in v(A) \wedge a = b\\
				& \Leftrightarrow \opRule{(a, b)}{(A, B)}{Id_A} \mbox{ by definition}\\
\end{split}
\end{equation}

\paragraph{Case $P = Chain(P', Q)$}
$\\ \indent$We have by inversion of the (Chain) type rule $\typeRule{P}{(A, C)} \Leftrightarrow \exists B. \indent\typeRule{P'}{(A, B)} \wedge \typeRule{Q}{(B, C)}$

\begin{equation} \label{case Chain(P', Q)}
\begin{split}
\denoRule{(a, c)}{P} & \Leftrightarrow (a, c) \in join(\deno{P'}, \deno{Q})\\
					& \Leftrightarrow \exists b \in B. \indent \denoRule{(a,b)}{P'} \wedge \denoRule{(b, c)}{Q}\\
					&\Leftrightarrow \exists b \in B. \indent \opRule{(a,b)}{(A, B)}{P'} \wedge \opRule{(b, c)}{(B, C)}{Q}\indent\mbox{by $\phiRule{P'}{A}{B}$, $\phiRule{Q}{B}{C}$} \\
				& \Leftrightarrow \opRule{(a, c)}{(A, C)}{Chain(P', Q)} \mbox{ by definition}\\
\end{split}
\end{equation}

\paragraph{Case $P = And(P', Q)$}
\begin{equation} \label{case And(P', Q)}
\begin{split}
\denoRule{(a, b)}{P} & \Leftrightarrow (a, b) \in (\deno{P'} \cap \deno{Q})\\
					& \Leftrightarrow \denoRule{(a, b)}{P'} \wedge \denoRule{(a, b)}{Q}\\
					&\Leftrightarrow \opRule{(a,b)}{(A, B)}{P'} \wedge \opRule{(a,b)}{(A, B)}{Q}\indent\mbox{by $\phiRule{P'}{A}{B}$, $\phiRule{Q}{A}{B}$} \\
					& \Leftrightarrow \opRule{(a, b)}{(A, B)}{And(P', Q)} \mbox{ by inversion of (And)}\\
\end{split}
\end{equation}

\paragraph{Case $P = Or(P', Q)$}
\begin{equation} \label{case Or(P', Q)}
\begin{split}
\denoRule{(a, b)}{P} & \Leftrightarrow (a, b) \in (\deno{P'} \cup \deno{Q})\\
					& \Leftrightarrow \denoRule{(a, b)}{P'} \vee \denoRule{(a, b)}{Q}\\
					&\Leftrightarrow \opRule{(a,b)}{(A, B)}{P'} \vee \opRule{(a,b)}{(A, B)}{Q}\indent\mbox{by $\phiRule{P'}{A}{B}$, $\phiRule{Q}{A}{B}$} \\
					& \Leftrightarrow \opRule{(a, b)}{(A, B)}{Or(P', Q)} \mbox{ by inversion of (Or1), (Or2)}\\\end{split}
\end{equation}

\paragraph{Case $P = AndLeft(P', S)$}
\begin{equation} \label{case AndLeft(P', Q)}
\begin{split}
\denoRule{(a, b)}{P} & \Leftrightarrow \denoRule{(a, b)}{P'} \wedge \denoRule{a}{S}\\
					&\Leftrightarrow \opRule{(a,b)}{(A, B)}{P'} \wedge \opRule{a}{A}{S}\indent\mbox{by $\phiRule{P'}{A}{B}$, $\psiRule{S}{A}$} \\
					& \Leftrightarrow \opRule{(a, b)}{(A, B)}{AndLeft(P', S)} \mbox{ by inversion of (AndLeft)}\\\end{split}
\end{equation}

\paragraph{Case $P = AndRight(P', S)$}
\begin{equation} \label{case AndRight(P', Q)}
\begin{split}
\denoRule{(a, b)}{P} & \Leftrightarrow \denoRule{(a, b)}{P'} \wedge \denoRule{b}{S}\\
					&\Leftrightarrow \opRule{(a,b)}{(A, B)}{P'} \wedge \opRule{b}{B}{S}\indent\mbox{by $\phiRule{P'}{A}{B}$, $\psiRule{S}{B}$} \\
					& \Leftrightarrow \opRule{(a, b)}{(A, B)}{AndRight(P', S)} \mbox{ by inversion of (AndRight)}\\\end{split}
\end{equation}

\paragraph{Case $P = Distinct(P')$}
\begin{equation} \label{case Distinct(P)}
\begin{split}
\denoRule{(a, b)}{P} & \Leftrightarrow \denoRule{(a, b)}{P'} \wedge a \neq b \\
					&\Leftrightarrow \opRule{(a,b)}{(A, B)}{P'} \wedge a \neq b\indent\mbox{by $\phiRule{P'}{A}{B}$} \\
					& \Leftrightarrow \opRule{(a, b)}{(A, B)}{Distinct(P')} \mbox{ by inversion of (AndRight)}\\\end{split}
\end{equation}

\paragraph{Case $P = Exactly(n, P')$}
$\\ \indent$We have by inversion of the (Exactly) type rule $\typeRule{P}{(A, A)} \wedge \typeRule{P'}{(A, A)}$

\begin{equation}
\label{fDef}
\mbox{let $f = (\lambda pairs. join(\deno{P'}, pairs))$}\end{equation}
$$\mbox{then}$$
\begin{equation} \label{case Exactly(n, P)}\denoRule{(a, b)}{P} \Leftrightarrow (a, b) \in  f^n \deno{Id_A}\end{equation}
$$\mbox{hence it suffices to prove}$$
\begin{equation}
\label{exactly Deno} (a, b)\in f^n \deno{Id_A} \Leftrightarrow \opRule{(a, b)}{(A, A)}{Exactly(n, P')}
\end{equation}

\paragraph{Case $Exactly(0, P'):$}
$$f^0\deno{Id_A} = \deno{Id_A}$$
$$\mbox{so by $\phiRule{(a, b)}{A, A)}{Id_A}$}$$
\begin{equation}
\begin{split}
(a, b) \in f^0\deno{Id_A} & \Leftrightarrow \denoRule{(a,b)}{Id_A}\\
& \Leftrightarrow \opRule{(a, b)}{(A, A)}{Id_A}\\
& \Leftrightarrow \opRule{(a, b)}{(A, A)}{Exactly(0, P')}
\end{split}
\end{equation}

\paragraph{Case $Exactly(n+1, P')$, assuming $\phiRule{Exactly(n, P')}{A}{A}$:}
$$f^{n+1}\deno{Id_A} = f(f^n(\deno{Id_A}))$$
$$\mbox{so by $\phiRule{Exactly(n, P')}{A}{A}$}$$
\begin{equation}
\begin{split}
(a, b) \in f^{n+1}\deno{Id_A} & \Leftrightarrow \exists a'. \denoRule{(a,a')}{P'} \wedge (a', b) \in f^n(\deno{Id_A})\indent\mbox{by definition of $join$ and $f$}\\
& \Leftrightarrow \opRule{(a, a')}{(A, A)}{P} \wedge \opRule{(a', b)}{(A, A)}{Exactly(n, p)}\\
& \Leftrightarrow \opRule{(a, b)}{(A, A)}{Exactly(n+1, P')}\mbox{ by (Exactly n+1)}\\
\end{split}
\end{equation}

\paragraph{Case $P = Upto(n, P')$}
$\\ \indent$We have by inversion of the (Upto) type rule $\typeRule{P}{(A, A)} \wedge \typeRule{P'}{(A, A)}$

\begin{equation}
\label{fDef}
\mbox{let $f = (\lambda pairs. join(\deno{P'}, pairs) \cup pairs)$}\end{equation}
$$\mbox{then}$$

\begin{equation}
\deno{P} = f^n\deno{Id_A}\\
\mbox{We now case split on n}
\end{equation}

\paragraph{Case $Upto(0, P')$}

\begin{equation}
\begin{split}
\denoRule{(a,b)}{Upto(0, P')} & \Leftrightarrow (a, b) \in f^0 \deno{Id_A} \\
							  & \Leftrightarrow (a, b) \in \deno{Id_A}\\ 
							  & \Leftrightarrow \opRule{(a, b)}{(A, A)}{Id_A} \mbox{    by $\phiRule{Id_A}{A}{A}$}\\ 
							  & \Leftrightarrow \opRule{(a, b)}{(A, A)}{Upto(0, P')} \mbox{    by (Upto0)}\\ 
\end{split}
\end{equation}

\paragraph{Case $Upto(n+1, P')$}

\begin{equation}
\begin{split}
\denoRule{(a,b)}{Upto(m + 1, P')} & \Leftrightarrow (a, b) \in f^{m+1} \deno{Id_A} \\
							  & \Leftrightarrow (a, b) \in (join(\deno{P'}, f^{m} \deno{Id_A}) \cup f^{m} \deno{Id_A})\\ 
							  & \Leftrightarrow (a, b) \in join(\deno{P'}, \deno{Upto(m, P)}) \vee (a, b) \in \deno{Upto(m, P')}\\ 
							  & \Leftrightarrow (\exists a'.\denoRule{(a, a')}{P'} \wedge \denoRule{(a', b)}{Upto(m, P')}) \vee \opRule{(a, b)}{(A, A)}{Upto(m, P')}\\ 
							  & \Leftrightarrow (\exists a'.\opRule{(a, a')}{(A, A)}{P'} \wedge \opRule{(a', b)}{(A, A)}{Upto(m, P')}) \vee \opRule{(a, b)}{(A, A)}{Upto(m, P')}\\ 
							  & \Leftrightarrow \opRule{(a, b)}{(A, A)}{Upto(m+1, P')}\mbox{   by (Upto n+1), (Upto n)}
\end{split}
\end{equation}


\paragraph{case $P = FixedPoint(P')$}
$\\ \indent$We have by inversion of the (FixedPoint) type rule $\typeRule{P}{(A, A)} \wedge \typeRule{P'}{(A, A)}$

\begin{equation}
\label{fDef}
\mbox{let $f = (\lambda pairs. join(\deno{P'}, pairs) \cup pairs)$}\end{equation}
$$\mbox{then}$$
$$
\deno{P} = fix(f) \mbox{   In the domain $\clos$}\\
$$
\paragraph{Lemma:  f is continuous in the domain $\clos$}
\setlength{\leftskip}{1cm}
\paragraph{Firstly, f is monotonous.} $\\ $
Let $x \subseteq y$
\begin{equation}
\label{continuousLemma}
\begin{split}
(a, b) \in f(x) & \Rightarrow (a, b) \in x \vee (\exists a'. \denoRule{(a, a')}{P'} \wedge (a', b) \in x)\\
& \Rightarrow (a,b) \in y \vee(\exists a'. \denoRule{(a, a')}{P'} \wedge (a', b) \in y)\\
& \Rightarrow (a, b) \in f(y)\\
\therefore & f(x) \subseteq f(y)
\end{split}
\end{equation}
\setlength{\leftskip}{0pt}


\setlength{\leftskip}{1cm}
\paragraph{Secondly, $f$ preserves the $lub$s of chains.} $\\ $
$\indent\indent$ Consider a chain $x1 \subseteq x2 \subseteq ... $ in $\clos$
Since $\clos$ is a domain, the $lub$, $\bigcup_nx_n$ is also in $\clos$

 
\begin{equation}
\begin{split}
	\forall m. x_m & \subseteq \bigcup_n x_n \\
	\forall m. f(x_n) & \subseteq f(\bigcup_n(x_n))\\
	\therefore \bigcup_nf(x_n) & \subseteq f(\bigcup_n(x_n))
\end{split}
\end{equation}

To get the inverse relation,

\begin{equation}
(a, b) \in f(\bigcup_n(x_n)) \Rightarrow ((\exists n. (a,b) \in x_n ) \vee (\exists m, a'. \denoRule{(a, a')}{P'} \wedge (a', b) \in x_m)
\end{equation}

let $n' = max(n, m)$ so $x_n \subseteq x_{n'}  \wedge x_m \subseteq x_{n'}$

\begin{equation}
\begin{split}
\exists n'. ((a,b) & \in x_{n'} ) \vee (\exists a'. \denoRule{(a, a')}{P'} \wedge (a', b) \in x_{n'})\\
\therefore\exists n'. (a, b) &\in f(x_{n'})\\
\therefore\exists n'. f(\bigcup_nx_n) &\subseteq f(x_{n'}\\
\therefore f(\bigcup_nx_n) &\subseteq \bigcup_nf(x_{n'})\\
\end{split}
\end{equation}


\setlength{\leftskip}{0pt}
So $f$ is Scott-continuous.

Now, by Tarski's fixed point theorem

$$\deno{FixedPoint(P')} = fix(f) = \bigsqcup_nf^n(\bot)$$


\paragraph{Lemma $\opRule{(a, b)}{(A, A)}{FixedPoint(P')} \Leftrightarrow \exists n. (a, b) \in f^n(\bot)$}

Firstly, in the forwards direction, $\opRule{(a, b)}{(A, A)}{FixedPoint(P')} \Rightarrow \exists n. (a, b) \in f^n(\bot)$

By inversion of the operational rules (FixedPoint0), (FixedPoint n) and $\opRule{(a, b)}{(A, A)}{FixedPoint(P')}$, we get two cases.



\paragraph{Case $\opRule{(a,b)}{A, A}{Id_A}$:\\}
by $\phiRule{Id_A}{A}{A}, \denoRule{(a, b)}{Id_A} = \bot$\\so $n = 0$


\paragraph{Case $\opRule{(a, b)}{(A,A)}{P'} \wedge \opRule{(b, c)}{(A,A)}{FixedPoint(P')}\\ $}
(hence $\opRule{(a, c)}{(A, A)}{FixedPoint(P')}$)

by $\phiRule{P'}{A}{A}, and \opRule{(b,c)}{(A,A)}{FixedPoint(P')}$

$$\denoRule{(a, b)}{P'} \wedge \exists n. (b, c) \in f^n(\bot)$$

Instantiating with $m = n$ gives
$$\denoRule{(a, b)}{P'} \wedge (b, c) \in f^m(\bot)$$

so $(a,c) \in f(f^m(\bot)) = f^{m+1}(\bot)$

Hence $\opRule{(a, b)}{(A, A)}{FixedPoint(P')} \Rightarrow \exists n. (a, b) \in f^n(\bot)\\\\ $
To go the other way, we need to prove $(a, b) \in \bigsqcup_nf^n(\bot) \Rightarrow \opRule{(a, b)}{(A, A)}{FixedPoint(P')}$

$(a, b) \in \bigsqcup_n f^n(\bot)$ Means that either:

\paragraph{Case $(a, b) \in \bot$}
\begin{equation}
\label{Case Bottom}
\begin{split}
& \therefore \denoRule{(a,b)}{Id_A}\\
& \therefore \opRule{(a,b)}{(A, A)}{Id_A} \mbox{ By $\phiRule{Id_a}{A}{A}$}\\
& \therefore \denoRule{(a,b)}{Id_A} \mbox{ By (Fix1)}
\end{split}
\end{equation}

\paragraph{Case $\exists n \geq 0. (a, b) \in f^{n+1}(\bot) \wedge \neg ((a, b) \in f^n(\bot))$}
\begin{equation}
\label{Case n+1}
\begin{split}
(\exists a'. \denoRule{(a, a')}{P'} \wedge (a', b) \in f^{n}(\bot)) & \vee (a, b) \in f^{n}(\bot) \wedge \neg ((a, b) \in f^n(\bot))\\
&\therefore \exists a'. \denoRule{(a, a')}{P'} \wedge (a', b) \in f^{n}(\bot)\\
&\therefore \opRule{(a, a')}{(A, A)}{P'} \wedge \opRule{(a', b)}{(A, A)}{FixedPoint(P')}\\
&\therefore \opRule{(a,b)}{(A,A)}{FixedPoint(P')}
\end{split}
\end{equation}

so we have $\opRule{(a, b)}{(A, A)}{FixedPoint(P')} \Leftrightarrow \exists n. (a, b) \in f^n(\bot) \Leftrightarrow \denoRule{(a, b)}{FixedPoin(P')}$

$$\square$$

\chapter{Joins on $\queryT{A}$ as a monoid}

In order to justify some of our optimisations, we need to know certain properties of join denotation.

Firstly, we want to define establish closure and associatiavity of the $join$ function defined in the denotational semantics.

We want to define these for queryable subsets of a view $v \in View_{\Sigma}$
\begin{equation}
	(v) = \setComp{s}{\exists P, A, B. s = \deno{P} \wedge \typeRule{P}{A, B}}
\end{equation}

\section{Lemma: Join is closed on $\query$}
Proof:
For $s, t \in Query(v)$ there exists $P, Q, A, B, C, D$ such that $\typeRule{P}{A, B}$ and $\typeRule{Q}{C, D}$ hold and $s = \deno{P} \wedge t = \deno{Q}$

\paragraph{Case $B \neq C$}
	Then the join is empty, since no $b \in B = c \in C$, so 
	\begin{equation}
		join(\deno{P},\deno{Q}) = \emptyset = \deno{Distinct(Id_T)}
	\end{equation}
	For some type $T \in \Sigma$ (We know $\Sigma$ at least one type, since the typings of $P$ and $Q$ hold.)

 
\paragraph{Case $B = C$}
	Then the join is not empty.
	By the correspondence of denotational and operational semantics, we have
	\begin{equation}
		join(\deno{P}, \deno{Q}) = \deno{Chain(P, Q)}
	\end{equation}
	
	with the following typing
	\begin{equation}
	\typeRule{Chain(P, Q)}{A, D}
	\end{equation}
	 
\section{Lemma: Join is associative on $\query$}
Proof: For $r, s, t \in \query$ there exist queries $P, Q, R$ and object types $A, B, C, D, E, F$ such that $\typeRule{P}{A, B}$, $\typeRule{Q}{C, D}$, and $\typeRule{R}{E, F}$  hold and $r = \deno{P} \wedge s = \deno{Q} \wedge t = \deno{R}$
\\We want to prove that $join(r, join(s, t)) = join(join(r, s), t)$
\paragraph{Case $B \neq C$}
	Then 
	\begin{equation}\label{Case B != C}
	\begin{split}
		join(r, join(s, t)) & = join(r, u) \mbox{ For some $u$, either $u = \emptyset$ or $u$ has a left type of $C$}\\
							& = \emptyset \mbox{ hence equals the empty set}\\
							& = join(\emptyset, t)\\
							& = join(join(r, s), t) \\
	\end{split}
	\end{equation}
	
\paragraph{Case $D \neq E$}
Then similarly
	\begin{equation}\label{Case D != E}
	\begin{split}
		join(join(r, s), t) & = join(u, t) \mbox{ For some $u$, either $u = \emptyset$ or $u$ has a right type of $D$}\\
							& = \emptyset \mbox{ hence equals the empty set}\\
							& = join(r, \emptyset)\\
							& = join(r, join(s, t)) \\
	\end{split}
	\end{equation}
	
\paragraph{Case $B = C$ and $D = E$}
Then
\begin{equation}\label{Well typed}
\begin{split}
(a, f) \in join(r, join(s, t)) & \Leftrightarrow (a, f) \in join(\deno{P}, join(\deno{Q},\deno{R}))\\
							  & \Leftrightarrow \opRule{a,f}{A, F}{Chain(P, Chain(Q, R))} \mbox{ by deno-oper correspondence}\\
							& \Leftrightarrow \exists b, d. \opRule{(a, b)}{A, B}{P} \wedge \opRule{(b,d)}{B,D}{Q} \wedge \opRule{(d,f)}{D,F}{Q}\\
							& \Leftrightarrow \opRule{(a, f)}{A, F}{Chain(Chain(P, Q), R)}\\
							& \Leftrightarrow \denoRule{(a, f)}{Chain(Chain(P, Q), R)}\\
							& \Leftrightarrow (a, f) \in join(join(\deno{P}, \deno{Q}), \deno{R})\\
							& \Leftrightarrow (a, f) \in join(join(r, s), t)\\							
\end{split}
\end{equation}

Hence $\query$ is a monoid with $join$. However, it doesn't particularly stick to a nice typescheme.
	
\section{Joins on $\queryT{A}$ as a monoid}
If we now take the more useful subset $\queryT{A}$ of $\query$

\begin{equation}
\queryT{A} = \setComp{s \subseteq A \times A}{\exists P. \typeRule{P}{A, A} \wedge s = \deno{P}}
\end{equation}

Trivially, from the above, $join$ over $\queryT{A}$ also forms a monoid.

This enables several optimisations.

Firstly, writing $p$ for $\deno{P}$ and with the binary representation of $n = \sum_i{b_i * 2^{i}}$
\begin{equation}\label{ExactlyReordering}
\begin{split}
\deno{Exactly(n, P)} & = p^{n} \mbox{ In $\queryT{A}$}\\
					& = \prod_i {p^{b_i * 2^i} }
\end{split}
\end{equation}

\section{Joins distribute over Or}

It is useful to know how $join$ interacts with other queries. Specifically we can do some re-writing if it is the case that join distributes over $Or$

\begin{equation}
\deno{Chain(P, Or(Q, R))} = \deno{Or(Chain(P,Q), Chain(P, R))}
\end{equation}

Firstly, we have by inversion of the type rules for $Chain$ and $Or$ that:
\begin{equation}\label{typeEquality}
\begin{split}
	\typeRule{Chain(P, Or(Q, R))}{A, C} & \Leftrightarrow \typeRule{P}{A, B} \wedge \typeRule{Q, R}{B, C}\\
									   & \Leftrightarrow \typeRule{Chain(P,Q)}{A, C} \wedge \typeRule{Chain(P, R)}{A, C}\\
									   & \Leftrightarrow \typeRule{Or(Chain(P,Q), Chain(P, R))}{A, C}\\
\end{split}
\end{equation}

\begin{equation}\label{denotationalEquality}
\begin{split}
\deno{Chain(P, Or(Q, R))} & = \setComp{(a, c)}{\exists b. \denoRule{(a, b)}{P} \wedge (b,c) \in (\deno{Q} \cup \deno{R}})\\
						 & = \setComp{(a, c)}{\exists b. (\denoRule{(a, b)}{P} \wedge \denoRule{(b,c)}{Q}) \vee (\denoRule{(a, b)}{P} \wedge \denoRule{(b,c)}{R})}\\
						 & = \deno{Chain(P, Q)} \cup \deno{Chain(P, R)}\\
						 & = \deno{Or(Chain(P, Q), Chain(P, R))}
\end{split}
\end{equation}


\section{$Upto(n, P)$ expressed as $Exactly(n, P')$}
Thanks to the previous section, we can now rewrite the denotation of $upto$

\begin{equation}
\label{UptoToExactly}
\begin{split}
\deno{Upto(n, P)} & = (\lambda pairs. join(\deno{P}, pairs) \cup pairs)^n \deno{Id_A}\\
				  & = (\lambda pairs. join(\deno{P} \cup \deno{Id_A}, pairs)^n \deno{Id_A} \mbox{ Since $Id_A$ is the identity of join}\\
				  & = \deno{Exactly(n, Or(P, Id_A))}
\end{split}
\end{equation}

This means that we can evaluate $Upto$ queries as $Exactly$ queries, and apply the binary-representation driven construction above to evaluate queries using fewer unique joins.


\section{Joins do not distribute over And}

Consider the schema containing object types $A$, $B$, $C$ and relations $R_1 \colon A,B$, $R_2 \colon B,C$,  and $R_3 \colon B,C$ and a view $v$ in this schema, with relations $R_1 \mapsto \lbrace(a, b1), (a, b2)\rbrace$, $R_2 \mapsto \lbrace(b1, c)\rbrace$, and $R_3 \mapsto \lbrace(b2, c)\rbrace$.

Clearly 
\begin{equation}
\denoRule{(a,c)}{And(Chain(R_1, R_2), Chain(R_1, R_3))}
\end{equation}

But 
\begin{equation}
 \deno{And(R_2, R_3)} = \emptyset
\end{equation}

So
\begin{equation}
 \deno{Chain(R_1, And(R_2, R_3))} = \emptyset
\end{equation}
 
\chapter{Scala Algebraic Data Type Definitions}
Here, I have attached the definitions of the Scala typed intermediate representation. Note how the use of generic type parameters causes the Scala level type checking to also do the query-language type checking and inference. Also note how the use of \codeName{SchemaObject[A]} implicit parameters forces membership of the type-class of types that can be stored in the database.
\begin{framed}
\begin{verbatim}
sealed abstract class FindPair[A, B](
    implicit val sa: SchemaObject[A],
    val sb: SchemaObject[B]
) {
  /**
    * Reverse the relation
    */
  def reverse: FindPair[B, A]

  /**
    * Erase the type of the object, wrt SchemaDescription
    */
  def getUnsafe(sd: SchemaDescription):  MissingRelation \/ UnsafeFindPair
}
\end{verbatim}
\end{framed}

\begin{framed}
\begin{verbatim}
sealed abstract class FindSingle[A](implicit val sa: SchemaObject[A]) {
  /**
    * Convert to unsafe
    */
  def getUnsafe(sd: SchemaDescription): MissingRelation \/ UnsafeFindSingle
}
\end{verbatim}
\end{framed}

Definitions of \codeName{FindPair} case classes.
For the sake of brevity and clarity, I have removed the implementations of \codeName{reverse} and \codeName{getUnsafe}. In short, they were defined straightforwardly to recursively process the tree.
\begin{framed}
\begin{verbatim}
/**
  * Search for pairs related by a [[Relation]]
  */
case class Rel[A, B](r: Relation[A, B])
    (implicit sa: SchemaObject[A], sb: SchemaObject[B])
    extends FindPair[A, B]

/**
  * Search for pairs related in reverse by a [[Relation]]
  */
case class RevRel[A, B](r: Relation[B, A])
    (implicit sa: SchemaObject[A], sb: SchemaObject[B])
    extends FindPair[A, B]

/**
  * Search for pairs that appear in the result of both subexpressions
  */

case class And[A, B](left: FindPair[A, B], right: FindPair[A, B])
    (implicit sa: SchemaObject[A], sb: SchemaObject[B])
    extends FindPair[A, B]

/**
  * Search pairs in the result of the left subexpression,
  * such that the right of the pair is in result of the
  * right subexpression
  */
case class AndRight[A, B](left: FindPair[A, B], right: FindSingle[B])
    (implicit sa: SchemaObject[A], sb: SchemaObject[B])
    extends FindPair[A, B]

/**
  * Search for pairs of the left sub expression
  * such that the left of the pair is in the result
  * of the right subexpression
  */
case class AndLeft[A, B](left: FindPair[A, B], right: FindSingle[A]                        )
    (implicit sa: SchemaObject[A], sb: SchemaObject[B])
    extends FindPair[A, B]


/**
  * Search for pairs that appear in the result of either subexpression
  */
case class Or[A, B](left: FindPair[A, B], right: FindPair[A, B])
    (implicit sa: SchemaObject[A], sb: SchemaObject[B])
    extends FindPair[A, B]

/**
  * search for pairs (a, c) such that there
  * exists b, (a, b) is in the result of left, (b, c)
  * is in the result of right
  */

case class Chain[A, B, C](
                           left: FindPair[A, B],
                           right: FindPair[B, C]
                         )
    (implicit sa: SchemaObject[A], sb: SchemaObject[B], sc: SchemaObject[C])
    extends FindPair[A, C]

/**
  * return a set of repeated pairs of the type
  *
  * Chain(a, id) = Chain(id, a) = a
  */

case class FindIdentity[A]()(implicit sa: SchemaObject[A])
extends FindPair[A, A]

/**
  * filter the result of the sub expression for those
  * pairs which do not equal each other
  */

case class Distinct[A, B](rel: FindPair[A, B])
    (implicit sa: SchemaObject[A], sb: SchemaObject[B])
extends FindPair[A, B]

/**
  * Find pairs that are related by n repetitions
  * of the underlying subexpression
  */

case class Exactly[A](n: Int, rel: FindPair[A, A])
    (implicit sa: SchemaObject[A])
    extends FindPair[A, A]

/**
  * Find pairs that are related by up to n
  * repetitions of the underlying subexpression
  */

case class Upto[A]( n: Int, rel: FindPair[A, A])
    (implicit sa: SchemaObject[A])
    extends FindPair[A, A]

/**
  * Find pairs that are related by the transitive closure
  * of the underlying subexpression
  */
case class FixedPoint[A](rel: FindPair[A, A])
    (implicit sa: SchemaObject[A])
    extends FindPair[A, A]

\end{verbatim}
\end{framed}
Definition of \codeName{FindSingle} case classes. Again, I have ommitted the implementations of \codeName{getUnsafe} for the sake of brevity
\begin{framed}
\begin{verbatim}
/**
  * Lookup a findable
  */
case class Find[A](pattern: Findable[A])
    (implicit sa: SchemaObject[A])
    extends FindSingle[A]
/**
  * Narrow down a findSingle query
  */
case class AndS[A
    (left: FindSingle[A], right: FindSingle[A])(implicit sa: SchemaObject[A])
    extends FindSingle[A]
/**
  * Find objects b: B, such that there exists a: A
  * in the result of start that (a, b) is in the result of rel
  */
case class From[A, B](start: FindSingle[A], rel: FindPair[A, B])
    (implicit sa: SchemaObject[A], sb: SchemaObject[B])
    extends FindSingle[B]

/**
  * Broaden a findSingle query
  */
case class OrS[A](left: FindSingle[A], right: FindSingle[A])
    (implicit sa: SchemaObject[A])
    extends FindSingle[A]
\end{verbatim}
\end{framed}

\chapter{Denotational Semantics Based Memory Implementation}
\section{FindPairs}
The simplistic denotational semantics based Memory interpreter
\begin{framed}
\begin{verbatim}
def findPairsSetImpl(
    t: UnsafeFindPair,
    left: Set[MemoryObject],
    tree: MemoryTree
): MemoryEither[Set[RelatedPair]] = {


    def recurse(
        t: UnsafeFindPair,
        left: Set[MemoryObject]
    ) = findPairsSetImpl(t, left, tree)


    t match {
      case USAnd(l, r) =>
        for {
        leftRes <- recurse(l, left)
        rightRes <- recurse(r, left)
      } yield leftRes.intersect(rightRes)


      case USAndRight(l, r) => for {
        leftRes <- recurse(l, left)
        rightRes <- findSingleSetImpl(r, tree)
      } yield leftRes.filter{case (a, b) => rightRes.contains(b)}

      case USAndLeft(p, s) => for {
        rightRes <- findSingleSetImpl(s, tree)
        pairRes <- recurse(p, rightRes intersect left)
      } yield pairRes


      case USOr(l, r) => for {
        leftRes <- recurse(l, left)
        rightRes <- recurse(r, left)
      } yield leftRes.union(rightRes)

      case USChain(l, r) => for {
        lres <- recurse(l, left)
        rres <- recurse(r, lres.map(_._2))
      } yield algorithms.Joins.joinSet(lres, rres)

      case USDistinct(r) => for {
        rres <- recurse(r, left)
      } yield rres.filter{case (a, b) => a != b}

      case USId(_) =>
        left.map(x => (x, x)).right

      case USRel(rel) =>
        left.map(_.getRelatedMemoryObjects(rel, tree)).flattenE

      case USRevRel(rel) =>
        left.map(_.getRevRelatedMemoryObjects(rel, tree)).flattenE

      case USUpto(n, rel) =>
        val stepFunction =
          left => findPairsSetImpl(rel, Set(left), tree).map(_.mapProj2)
        upTo(stepFunction, left, n)

      case USFixedPoint(rel) =>
        // find a fixed point
        val stepFunction =
          left => findPairsSetImpl(rel, left, tree).map(_.mapProj2)
        for {
          res <- fixedPoint(stepFunction, left)
        } yield res

      case USExactly(n, rel) =>
        val stepFunction =
          left => findPairsSetImpl(rel, Set(left), tree).map(_.mapProj2)
        FixedPointTraversal.exactly(stepFunction, left, n)

    }
  }
\end{verbatim}
\end{framed}

\section{FindSingle}
\begin{framed}
\begin{verbatim}
def findSingleSetImpl(
    t: UnsafeFindSingle,
    tree: MemoryTree
): MemoryEither[Set[MemoryObject]] = {

    def recurse(t: UnsafeFindSingle) = findSingleSetImpl(t, tree)


    t match {
      case USFind(pattern) =>
        tree
          .getOrError(
              pattern.tableName,
              MemoryMissingTableName(pattern.tableName)
          ).flatMap(_.find(pattern).map(_.toSet))
      case USFrom(start, rel) => for {
        left <- recurse(start)
        res <- findPairsSetImpl(rel, left, tree).map(_.mapProj2)
      } yield res
      case USAndS(left, right) => for {
        r1 <- recurse(left)
        r2 <- recurse(right)
      } yield r1 intersect r2

      case USOrS(left, right) => for {
        r1 <- recurse(left)
        r2 <- recurse(right)
      } yield r1 union r2
    }
  }
\end{verbatim}
\end{framed}

\end{document}
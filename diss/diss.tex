
% Template for a Computer Science Tripos Part II project dissertation
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{breqn}

%Highlighter commands. most need to properly defined.
\newcommand\todo[1]{\textcolor{red}{#1}}
\newcommand\codeName[1]{\textcolor{blue}{#1}}
\newcommand\term[1]{\textcolor{purple}{#1}}
\newcommand\code[1]{\textcolor{green}{\begin{verbatim}#1\end{verbatim}}}
\newcommand\note[1]{\textcolor{cyan}{#1}}
\newcommand\mathName[1]{\textcolor{pink}{#1}}

\newcommand{\reference}{RefImpl}
\newcommand{\batched}{LMDBatched}
\newcommand{\cse}{LMDBCSE}
\newcommand{\postgres}{Postgres}

\newcommand\resultTable[4]{
\begin{center}
	\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|  }
 	\hline
 		Name & \multicolumn{2}{|c|}{#1} \\
 	\hline
 		Iterations & \multicolumn{2}{|c|}{#2} \\
 	\hline
 		Queries & \multicolumn{2}{|c|}{#3} \\
 	\hline
 		implementation   	& 	time (ms) & /optimised \\
		#4
 	\hline
	\end{tabular}
\end{center}

}

\newcommand\either[0]{\textbackslash/}
\newcommand\eitherR[0]{\textbackslash/-}
\newcommand\eitherL[0]{-\textbackslash/}

\newcommand{\db}[1]{{\bf [\![}#1{\bf ]\!]}}
\newcommand{\deno}[1]{\db{#1}(v)}
\newcommand{\setComp}[2]{\left\lbrace #1 \mid #2 \right\rbrace}
\newcommand{\clos}[0]{closure(A, v)}
\newcommand{\typeRule}[2]{\Sigma\vdash #1 \colon #2}
\newcommand{\denoRule}[2]{#1 \in \deno{#2}}
\newcommand{\opRule}[3]{#1 \triangleleft_{#2, v} #3}

\newcommand{\phiRule}[3]{\Phi(\Sigma, #1, #2, #3)}
\newcommand{\psiRule}[2]{\Psi(\Sigma, #1, #2)}
\newcommand{\query}[0]{Query(v)}
\newcommand{\queryT}[1]{Query_{#1}(v)}
\newcommand{\projectTitle}[0]{A Purely Functional Approach to Graph Queries on a Database}


\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\rightline{\LARGE \textbf{Alexander Taylor}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{\projectTitle} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
St John's College \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Alexander Taylor                       \\
College:            & \bf St John's College                     \\
Project Title:      & \bf \projectTitle  \\
Examination:        & \bf Computer Science Tripos -- Part II, June 2018  \\
Word Count:         & \bf   \\
Project Originator: & Adapted from a project proposal by Dr Eiko Yoneki                    \\
Supervisor:         & Dr Timothy Jones                    \\ 
\end{tabular}
}

\section*{Original Aims of the Project}

The initial aim of the project was to produce a simple graph database system which wrapped over an SQL database, to expose an API treating the database as a purely functional datastructure. Operations were to be achieved in a monadic fashion, according to well defined DSL semantics. The success criteria were that a user could write a composable query with the DSL and receive correct results with appropriate error checking.

\section*{Work Completed}

All success criteria were met and exceeded. I also implemented several extensions such as adding write functionality and and a collection of bespoke backends using the LMDB key-value store. These new backends were evaluated by comparing their against the compiled SQL queries generated by the SQL backend.

\section*{Special Difficulties}

None.
 
\newpage
\section*{Declaration}

I, Alexander Taylor of St John's College, being a candidate for Part II of the Computer
Science Tripos, hereby declare
that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation
does not contain material that has already been used to any substantial
extent for a comparable purpose.

\bigskip
\leftline{Signed [signature]}

\medskip
\leftline{Date [date]}

\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}

\todo{Acknowledgements}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\pagestyle{headings}

\chapter{Introduction}
This project has been to build a new graph database management system. written a purely functional style in the language Scala. I have successfully built a modular, composable read oriented DBMS which has a range of commands and a domain specific read query language. 
\section{Progress}

I have achieved all of my initial success criteria, written several extensions (Writes, an extra backend, and optimisations to the backend), and achieved performance comparable to and sometimes exceeding that of postgreSQL across a range of benchmarks.

\section{Motivations}

Graph databases becoming of more interest recently, as the relational model of data is found not to match certain datasets of interest, such as social networks.

Similarly, purely functional, strongly typed programming is also progressing from a research interest to a serious paradigm used in industry. This is especially the case in fields where the ability to rapidly produce high quality code which has a high probability of correctness is vital, such as in automated trading. In such cases, an advanced type system can be used to make guarantees about the behaviour of the program at runtime and enhances the ability to find bugs at compile time rather than in the field, while the purely functional paradigm allows code to be written far more expressively and concisely than in lower level languages.

There exist several attempts to make access to relational databases purely functional or monadic in nature, but there is very little application to  graph databases.
    
    \todo{https://hackage.haskell.org/package/haskelldb
		  http://www.bcs.org/upload/pdf/ewic_dp95_paper14.pdf}


\section{Type safety}
When we talk about type safety, we mean to say that a correctly typed program will not produce run-time errors of a given class. Sources of a lack of  type safety in typical programs include exceptions (These are inherently not type safe in scala, since exceptions are unchecked), the usage of primitive data types where specific values are expected, such as passing strings to function without wrapping or tagging them in more specific types, and unmarshalling data from a type-erased source. If we don’t know whether we’re expecting a value to be an integer or a string and we try to read it as an integer, there is a chance to throw an exception. 
By this definition, accessing an external, imperative, database from scala is not type safe. The database will have some collection of error modes that often manifest as exceptions, and the database system typically stores data in a type erased form that needs to be unmarshalled. Finally, queries to external database systems are typically constructed as a primitive string that cannot be checked automatically at compile time. This project addresses these issues by using carefully constructed result container types to correctly handle error cases, including arbitrary exceptions using a pattern match-able type hierarchy, a DSL that is checked at compile time by the scala type checker, and typeclass based marshalling.

\chapter{Preparation}

\section{Existing Graph Databases}
\subsection{Classes of Database}
Existing graph databases typically fall into two classes: property graphs and edge labelled graphs. \todo{https://arxiv.org/pdf/1610.06264.pdf} . Edge labelled graphs typically store only a label on any given edge, whereas property graphs can store more attributes on edges, hence being closer to close to relational databases. Edge labelled graphs can be more succinctly modelled mathematically, since they can be represented purely using mathematical relations. Edge labelled graphs also lead to a cleaner syntax (we shall cover this later). Finally, any data expressible using a property graph can be represented using an edge labelled graph by introducing additional nodes to hold attributes.

\todo{Insert and acknowledge edge-labelled vs property graph examples in GDrive}

\subsection{Schema}
Current graph database systems often do not have rigid schema \todo{https://arxiv.org/pdf/1602.00503.pdf, introduction}. Instead they use dynamic schema. This requires additional typing checks at runtime. This also fails to fit into our definition of type safety, since we have no guarantees about any objects extracted from the database. Instead, I’ve opted for a rigid schema which is built using scala objects and hence is checked by the scala compiler at compile time.
\subsection{Mutability}
Current graph databases tend to be mutable. In order to express concepts that require immutability, such as time-dependent data, the user has impose extra constraints, such as time stamping nodes and relations. A lack of immutability also complicates concurrent semantics and implementation of ACID transactions. The introduction of immutable data structures gives us these effectively for free.
\subsection{Query Languages}
Typically, database systems have their own query language, such as cypher for neo4j and gremlin. These are typically used by generating a query string and submitting it to the database engine. This is inherently un-typesafe. The job of ensuring that absolutely every query that a given program could generate is valid is intractable to undecidable for non trivial programs. By encoding our query language in the host language and aligning the type system to be checked by the host compiler, we can get much stronger guarantees about program correctness. \todo{links to cypher and gremlin}

\section{Mutability}
In order to implement immutability, we need a way of creating an immutable snapshot of the database at a given point in time. This is done using a system of views. A view is an immutable view of the database. When we read from the database, we read from a particular given view. When we write to a particular view of the database, we copy the view, update it, and return the new view. Reads and writes now never interfere with each other. \todo{Read/Write diagram}

\section{Query Language}
\subsection{DSL Syntax}
I spent some time iterating over a DSL syntax and how I wanted queries to look and be structured. My goals we to have a highly composable and expressive, yet small language. At first I experimented with a neo4j style language, with pattern matching syntax, an example of which is below. The query takes a number of fields to fill in, in this case a pair of actors, and finds all valid ways to fill them in subject to the graph constraints.
\todo{Ugly syntax figure}
However, the scala type inference is fairly localised so would need some type parameterisations filled out, yielding a more accurate syntax below.
\todo{Even uglier syntax}
This is fairly cluttered syntax and difficult to read. It would also have been complicated by a desire to implement property graph features. Furthermore, we would not be able to easily and safely replicate Neo4j’s ability to extract an arbitrary number of objects of different types from a path. Doing so would require use of structures such as heterogeneously typed lists.

As a result, I settled upon on an edge-labelled-relation oriented approach which neatens the above query significantly.
\todo{reword above;}
\todo{neat syntax}

This is much more readable and concise. With this syntax, and the assumption that \codeName{ActsIn} relates \codeName{Actors} to \codeName{Movies}, Scala’s typesystem can infer that \codeName{coactor} relates \codeName{Actors} to other \codeName{Actors}. There is also syntax for repetitions, intersections and unions. \todo{Examples in the appendix}
\subsection{Algebraic Datatypes}
These DSL queries should correspond to an intermediate representation. I have picked a set of expression constructors. These fall into two kinds: \codeName{FindPair} and \codeName{FindSingle}, indicating whether they return pairs or individual values.

\paragraph{FindPair queries}
\begin{equation}
\label{PDefinition}
\begin{split}
P  &\rightarrow Rel(R) \mbox{ Find pairs related by the relation R}\\
&\mid RevRel(R) \mbox{ Find pairs related by the relation R in the reverse direction}\\
&\mid Chain(P, P) \mbox{   Find pairs related by the first subquery followed by the second}\\
&\mid And(P, P) \mbox{  Find pairs related by both of the sub-queries}\\
&\mid AndRight(P, S) \mbox{  Find pairs related by P where the right value is a result of s}\\
&\mid AndLeft(P, S) \mbox{  Find pairs related by P where the left value is a result of s}\\
&\mid Or(P, P) \mbox{  Find pairs related by either of the sub-queries}\\
&\mid Distinct(P) \mbox{  Find pairs related by P that are not symmetrical}\\
&\mid Id_A \mbox{ Identity relation}\\
&\mid Exactly(\mathit{n}, P) \mbox{  Find pairs related by n repetitions of P}\\
&\mid Upto(\mathit{n}, P) \mbox{  Find pairs related by upto n repetitions of P}\\
&\mid FixedPoint(P) \mbox{  Find the transitive closure of P}\\
\end{split}
\end{equation} 
where $n$ denotes a natural number. These queries lookup a set of pairs of objects.\\

\paragraph{FindSingle queries}
\begin{equation}
\label{SDefinition}
\begin{split}
S & \rightarrow Find(F) \mbox{ Find values that match the findable F}\\
&\mid From(S, P) \mbox{ Find values that are reable from results of S via P}\\
&\mid AndS(S, S') \mbox{ Find values that are results of both subqueries}\\
& \mid OrS(S, S') \mbox{ Find values that are results of either subquery}
\end{split}
\end{equation} 
Which lookup sets of single objects.\\


We also define a view of the database and schema, as well as findables 

\todo{Import view + schema semantic definitions here; reword if necessary}

\subsection{Typing}
In order to ensure the correctness of a given query, there is a type system which checks the correctness of queries. Typing rules take two forms. Firstly typing of pair queries:
\[ \Sigma \vdash P\colon (A, B)\]

Which means ``under the schema $\Sigma$, pair query $P$ returns a subset of $A \times B$''.  The second is for single queries:

\[ \Sigma \vdash S \colon A \]

Which means ``under the schema $\Sigma$ single query returns a subset of $A$''

The rules of the first kind are as follows

\[ \begin{array}{c}
\displaystyle\mbox{(Rel)}\frac{\Sigma(r)\downarrow(A, B)}{\typeRule{Rel(r)}{(A, B)}} \\[3ex]

\displaystyle\mbox{(Rev)}\frac{\Sigma(r)\downarrow(B, A)}{\typeRule{Rel(r)}{(A, B)}} \\[3ex]

\displaystyle\mbox{(Id)}\frac{\Sigma(A)\downarrow True}{\typeRule{Id_A}{(A, A)}} \\[3ex]

\displaystyle\mbox{(Chain)}\frac{\typeRule{P}{(A, B)} \indent \typeRule{Q}{(B, C)}}{\typeRule{Chain(P, Q)}{(A, C)}} \\[3ex]

\displaystyle\mbox{(And)}\frac{\typeRule{P}{(A, B)} \indent \typeRule{Q}{(A, B)}}{\typeRule{And(P, Q)}{(A, B)}} \\[3ex]

\displaystyle\mbox{(Or)}\frac{\typeRule{P}{(A, B)} \indent \typeRule{Q}{(A, B)}}{\typeRule{Or(P, Q)}{(A, B)}} \\[3ex]

\displaystyle\mbox{(Distinct)}\frac{\typeRule{P}{(A, B)}}{\typeRule{Distinct(P)}{(A, B)}} \\[3ex]

\displaystyle\mbox{(AndLeft)}\frac{\typeRule{P}{(A, B)} \indent \typeRule{S}{(A)}}{\typeRule{AndLeft(P,S)}{(A, B)}} \\[3ex]

\displaystyle\mbox{(AndRight)}\frac{\typeRule{P}{(A, B)} \indent \typeRule{S}{B}}{\Sigma \vdash \typeRule{AndRight(P, S)}{(A, B)}} \\[3ex]

\displaystyle\mbox{(Exactly)}\frac{\typeRule{P}{(A, A)}}{\typeRule{Exactly(n, P)}{(A, A)}} \\[3ex]

\displaystyle\mbox{(Upto)}\frac{\typeRule{P}{(A, A)}}{\typeRule{Upto(n, P)}{(A, A)}} \\[3ex]

\displaystyle\mbox{(FixedPoint)}\frac{\typeRule{P}{(A, A)}}{\typeRule{FixedPoint(P)}{(A, A)}} \\[3ex]

\end{array} \]


The rules for types of Single queries are similar:

\[ \begin{array}{c}
\displaystyle\mbox{(Find)}\frac{\Sigma(f)\downarrow(A)}{\typeRule{Find(f)}{A}} \\[3ex]

\displaystyle\mbox{(From)}\frac{\typeRule{P}{(A, B)} \indent \typeRule{S}{A}}{\typeRule{From(S, P)}{B}} \\[3ex]

\displaystyle\mbox{(AndS)}\frac{  \typeRule{S}{A} \indent  \typeRule{S'}{A}}{\typeRule{AndS(S, S')}{A}} \\[3ex]

\displaystyle\mbox{(OrS)}\frac{  \typeRule{S}{A} \indent  \typeRule{S'}{A}}{\typeRule{OrS(S, S')}{A}} \\[3ex]
\end{array}
\]
\subsection{Semantics}
    To fully define the query language, we need to define the semantics. I have defined two collections of semantics: operational style and denotational style.
\paragraph{Operational Semantics}   

Now we shall define a set of rules for determining if a pair of objects is a valid result of a query. We're interested in forming a relation $a \triangleleft_A Q$ to mean``a is a valid result of query Q with type A''. This is dependent on the current view $v: View_{\Sigma}$, and the type of the expression. Hence we define $\opRule{(a, b)}{(A, B)}{P}$ for pair queries $P$ and $\opRule{a}{A}{S}$ for single queries $S$.

\[ \begin{array}{c}
\displaystyle\mbox{(Rel)}\frac{(a,b) \in v(r)}{\opRule{(a, b)}{(A, B)}{Rel(r)}} \\[3ex]

\displaystyle\mbox{(Rev)}\frac{(b,a) \in v(r)}{\opRule{(a, b)}{(A, B)}{RevRel(r)}} \\[3ex]

\displaystyle\mbox{(Id)}\frac{a \in v(A)}{\opRule{(a, a)} {(A, A)} {Id_A}} \\[3ex]

\displaystyle\mbox{(Distinct)}\frac{\opRule{(a,b)}{(A, B)}{P} \indent a \neq b}{\opRule{(a,b)}{(A, B)}{Distinct(P)}} \\[3ex]

\displaystyle\mbox{(And)}\frac{\opRule{(a,b)}{(A, B)}{P} \indent \opRule{(a,b)}{(A, B)}{Q}}{\opRule{(a,b)}{(A, B)}{And(P, Q)}} \\[3ex]

\displaystyle\mbox{(Or1)}\frac{\opRule{(a,b)}{(A, B)}{P}}{\opRule{(a,b)}{(A, B)}{Or(P, Q)}} \\[3ex]

\displaystyle\mbox{(Or2)}\frac{\opRule{(a,b)}{(A, B)}{Q}}{\opRule{(a,b)}{(A, B)}{Or(P, Q)}} \\[3ex]

\displaystyle\mbox{(Chain)}\frac{\opRule{(a,b)}{(A, B)}{P} \indent \opRule{(b,c)}{(B, C)}{Q}}{\opRule{(a,c)}{(A, C)}{Chain(P, Q)}} \\[3ex]

\displaystyle\mbox{(AndLeft)}\frac{\opRule{(a,b)}{(A, B)}{P} \indent \opRule{a}{A}{S}}{\opRule{(a,b)}{(A, B)}{AndLeft(P, S)}} \\[3ex]

\displaystyle\mbox{(AndRight)}\frac{\opRule{(a,b)}{(A, B)}{P} \indent \opRule{b}{
B}{S}}{\opRule{(a,b)}{(A, B)}{AndRight(P, S)}} \\[3ex]

\displaystyle\mbox{(Exactly_0)}\frac{\opRule{(a,b)}{(A, A)}{Id_A}}{\opRule{(a, b)}{(A, A)}{Exactly(0, P)}} \\[3ex]

\displaystyle\mbox{(Exactly_{n+1})}\frac{\opRule{(a,b)}{(A, A)}{P} \indent \opRule{(b, c)}{(A, A)}{Exactly(n, P)}}{\opRule{(a, c)}{(A, A)}{Exactly(n + 1, P)}} \\[3ex]

\displaystyle\mbox{(Upto_0)}\frac{\opRule{(a,b)}{(A, A)}{Id_A}}{\opRule{(a, b)}{(A, A)}{Upto(0, P)}} \\[3ex]

\displaystyle\mbox{(Upto_n)}\frac{\opRule{(a, b)}{(A, A)}{Upto(n, P)}}{\opRule{(a, b)}{(A, A)}{Upto(n + 1, P)}} \\[3ex]

\displaystyle\mbox{(Upto_{n+1})}\frac{\opRule{(a,b)}{(A, A)}{P} \indent \opRule{(b, c)}{(A, A)}{Upto(n, P)}}{\opRule{(a, c)}{(A, A)}{Upto(n + 1, P)}} \\[3ex]

\displaystyle\mbox{(fix1)}\frac{\opRule{(a,b)}{(A, A)}{Id_A}}{\opRule{(a, b)}{(A, A)}{FixedPoint(P)}} \\[3ex]

\displaystyle\mbox{(fix2)}\frac{\opRule{(a,b)}{(A, A)}{P} \indent \opRule{(b, c)}{(A, A)}{FixedPoint(P)}}{\opRule{(a, b)}{(A, A)}{FixedPoint(P)}} \\[3ex]

\end{array} \]

And the FindSingle rules

\[ \begin{array}{c}

\displaystyle\mbox{(Find)}\frac{a \in v(A) \indent f(a)\downarrow True}{\opRule{a}{A}{Find(f)}} \\[3ex]

\displaystyle\mbox{(From)}\frac{\opRule{a}{A}{S} \indent \opRule{(a,b)}{)(A, B)}{P}}{\opRule{b}{A}{From(S, P)}} \\[3ex]

\displaystyle\mbox{(AndS)}\frac{\opRule{a}{A}{S} \indent \opRule{a}{A}{S'}}{\opRule{a}{A}{And(S, S')}} \\[3ex]

\displaystyle\mbox{(OrS1)}\frac{\opRule{a}{A}{S}}{\opRule{a}{A}{Or(S, S')}} \\[3ex]

\displaystyle\mbox{(OrS1)}\frac{\opRule{a}{A}{S'}}{\opRule{a}{A}{Or(S, S')}} \\[3ex]
\end{array} \] 
\paragraph{Denotational Semantics}
The operational semantics clearly demonstrate membership of a query, but don't give a means to efficiently generate the results of query. To this end, we introduce denotations $\llbracket P \rrbracket$ and $\llbracket S \rrbracket$ such that \[\Sigma \vdash P \colon (A, B) \Rightarrow\llbracket P \rrbracket \colon View_{\Sigma} \rightarrow \wp(A \times B)\]

and \[\Sigma \vdash S \colon A \Rightarrow\llbracket S \rrbracket \colon View_{\Sigma} \rightarrow \wp(A)\] Such denotations should be compositional and syntax directed, whilst still corresponding to the operational semantics.

\[ \begin{array}{c}
 \deno{Rel(r)} = v(r) \\[3ex]
 \deno{RevRel(r)} = swap(v(r)) \\[3ex]
 \deno{Id_A} = dup(v(A)) \\[3ex]
 \deno{Chain(P, Q)} = join(\deno{P}, \deno{Q}) \\[3ex]
 \deno{And(P, Q)} = \deno{P} \cap \deno{Q} \\[3ex]
 \deno{Or(P, Q)} = \deno{P} \cup \deno{Q} \\[3ex]
 \deno{AndLeft(P, S)} = filterLeft(\deno{P}, \deno{S}) \\[3ex]
 \deno{AndRight(P, S)} = filterRight(\deno{P}, \deno{S}) \\[3ex]
 \deno{Distinct(P)} = distinct(\deno{P}) \\[3ex]
 
 \deno{Exactly(n, P)} = (\lambda pairs. join(\deno{P}, pairs))^n \deno{Id_A} \\[3ex]
 \deno{Upto(n, P)} = (\lambda pairs. join(\deno{P}, pairs) \cup pairs)^n \deno{Id_A} \\[3ex]
 \deno{FixedPoint(P)} = fix (\lambda pairs. join(\deno{P}, pairs) \cup pairs)  \mbox{ in the domain $\clos$}\\[3ex]
\end{array} \]

And similarly with single queries

\[ \begin{array}{c}
\deno{Find(f)} = \setComp{a \in v(A)}{f(a)\downarrow True} \mbox{ for $\Sigma(f) = A$} \\[3ex]

\deno{From(S, P)} =  \setComp{b}{(a, b) \in \deno{P} \wedge a \in \deno{S}}   \\[3ex]

\deno{AndS(S, S')} = \deno{S} \cap \deno{S'} \\[3ex]

\deno{OrS(S, S')} = \deno{S} \cup \deno{S'} \\[3ex]

\end{array}\]

with the following definitions:
\[ swap(s) = \setComp{(b,a)}{(a, b) \i s}\]
\[ dup(s) = \setComp{(a,a)}{a \in s}\]
\[ join(p, q) = \setComp{(a, c)}{ \exists b. (a, b) \in p \wedge (b, c) \in q}\]
\[ distinct(s) = \setComp{(a, b) \in s} { a \neq b} \]
\[ filterLeft(p, s) = \setComp{(a, b) \in p}{a \in s}\]
\[ filterRight(p, s) = \setComp{(a, b) \in p}{b \in s}\]
    
    \todo{ I have also proved in the appendix the correspondence of these two sets of semantics.}
\subsection{Commands}
In addition to the query construction language, there are five commands which make use of the queries. \todo{define and semantics}


\section{Languages and Libraries Used}
\paragraph{Scala} An object-oriented, functional JVM languages interoperable with Java. I chose Scala for its familiarity, advanced type system, ability to use Java libraries, and ease of deployment across many systems. \todo{https://github.com/tperrigo/scala-type-system/wiki/Scala-Type-System-Overview}

\paragraph{Scala Build Tool} A package manager and build tool for Scala.
\todo{https://www.scala-sbt.org/}
\paragraph{PostgreSQL}  An open source, multi platform, performant SQL implementation.
\todo{https://www.postgresql.org/}
\paragraph{LMDB} An open source, highly optimised key-value datastore.
\todo{https://symas.com/lmdb/}
\paragraph{Scalaz} A library for scala providing typeclasses and syntax to aid advanced functional programming. \todo{https://github.com/scalaz/scalaz}
\paragraph{JDBC} Java’s standard library support for SQL connections. Used to interact with a postgres database.
\todo{http://www.oracle.com/technetwork/java/javase/jdbc/index.html}
\paragraph{LMDBJava} A JNI library allowing access to the LMDB datastore. 
\todo{https://github.com/lmdbjava/lmdbjava}
\paragraph{Junit} A unit testing library for JVM languages.
\todo{https://junit.org/junit5/}
\paragraph{SLF4J} A logging library for JVM languages
\todo{https://www.slf4j.org/}
\paragraph{Spray-JSON} A JSON library for Scala, used to generate benchmarking datasets.
\todo{https://github.com/spray/spray-json}
\paragraph{Python} Used to generate test datasets.
\todo{https://www.python.org/}


\section{Software Engineering}
The system was designed to be as modular and re-implementable as possible.

\todo{Figure from Google drive: the architecture map}
\subsection{Front End}
The front end mostly consists of interfaces and syntax for building queries and the schema for the database.
\subsection{Middle End}
The front-end’s DSL translates into the underlying typed ADT. This then has its compile time type information erased to become an un-typed query ADT, which is easier and cleaner to interpret.
\subsection{Back End}
Finally, there are three interfaces that a backend system must implement: DBBackend, DBInstance and DBExecutor. These specify a root backend object that opens an instance which represents a database connection. Each DBInstance has an executor that allows us to execute commands

\section{Scala Techniques}
I have made use of several advanced programming techniques which are specific to the Scala language.

\subsection{Type Enrichment}
The type enrichment feature of the scala language allows retroactive addition of methods to previously defined types. Type enrichment is performed by creating an implicit class, taking a single underlying value of some given type. The methods defined in the implicit class can now be called as if they were methods of the underlying class, provided the implicit class is in scope.
\todo{Type enrichment example from G Drive}
This allows us to add syntax to types where a small number of core methods have been defined.

\subsection{Implicit Parameters}
Another advanced feature of scala that  I have used is that of implicit  parameters to functions and class constructors. Values such as vals, functions, and classes can be declared with a implicit tag. Functions and classes can declare additional parameters as implicit. When these functions are called, the implicit parameter can be omitted if and only if there is an unambiguous implicit value of the correct type in scope.
\todo{Implicit parameters example 1}
This is typically used for purposes such as passing around values that are typically defined once and used many times in a program, such as an ExecutionContext, or a logging framework instance.

Functions can also be implicit and take implicit values
\todo{Implicit function example, caption: When an implicit value of type A is available, then an implicit value of type B is also available.}


This can be made more interesting when we include parameterised generic types. This allows us to get the compiler to do work in the manner of an automated theorem prover (by the curry-howard correspondence) at compile time.

\todo{Implicit parameters example 2}


\subsection{Typeclass Pattern}
A further combination of these two patterns is the typeclass pattern. We define a typeclass for a type by defining as methods on a trait the operations we want on the type. We can then define implicit objects which work as type class instances for the types we want. We can also use implicit functions to generate typeclass instances in a manner similar to a proof tree. \todo{example}

We can then use the type enrichment feature to add methods to values of a type that is a member of the typeclass.

\todo{Monoid typeclass example. Caption: An example of combining the three techniques above to define a Monoid Typeclass, ways to construct instances of the monoid typeclass and a syntax object which enriches types for which moinoid properties can be proven.}

It is clear that these patterns allow for very expressive structures and abstractions to be built in scala. I use these frequently within the project to neaten code and to achieve type safety.



\chapter{Implementation}
\section{Note on Purity and Concurrency}
All of the backends aim to preserve the global immutability of the database. The immutable semantics of the database also mean that queries generally don’t interfere with each other. This means that we can avoid keeping locks or creating large transactions. Hence, the backends don’t require much work to maintain correct concurrency.
\section{Functional Programming Techniques}
	\subsection{Monadic Compilation}
	At several points in building a backend, it becomes necessary to transform one algebraic type to another. This is typically done by walking over a tree, whilst keeping some mutable state representing parts of the tree that are relevant. One example is converting an intermediate representation to SQL output code. Here, we may want to extract common subexpressions into a dictionary, or pick out all the database tables that need to be accessed by the query. This can be encoded by folding a State Monad instance over the tree.

The state monad is an abstraction over functions that chain an immutable state through successive computations.

\todo{State monad definition or link to it}

To define a monadic compiler, we define a recursive function which chains state monad objects together.
\todo{State monad compiler example}

	\subsection{Constrained Future Monad}
	Part of the goal of type safety is the recovery of error cases. Typically, this would be done in a JVM program through the use of exceptions. However the presence of unchecked exceptions on the Java platform makes it difficult to ensure that all error cases are accounted for. A more functional approach is the use of the exception monad. \todo{http://homepages.inf.ed.ac.uk/wadler/papers/marktoberdorf/baastad.pdf} In scala, this manifests as the built in Try monad and Scalaz’s \codeName{\either} (\codeName{Either}) monad. \codeName{Try[A]} has two case classes: \codeName{Failure(e: Throwable)} and \codeName{Success(a: A)}, while \codeName{E \either A} has the case classes \codeName{\eitherL(e: E)} and \codeName{\eitherR(a: A)} (\codeName{Left} and \codeName{Right}). Since \codeName{Try}’s failure case is the unsealed \codeName{Throwable} trait, we don’t really have a way to ensure we have handled all error cases at compile time, whereas \codeName{Either} has a parametrised error type, which can be a sealed type hierarchy, which is then checked by the scala compiler at runtime. For example, consider the simple interpreter below. All error cases are proved to be handled by the typesystem.
\todo{Error interpreter example}	

Typically, we’re dealing with cases which might take a significant amount of time to return. So rather than using a simple \codeName{Either} monad, we lift it into an asynchrony monad. There are several options to choose from for an asynchrony monad. I chose the built in \codeName{Future} over more exotic alternatives, such as the scalaz \codeName{Task}, since it is relatively widely used, and I have some familiarity with it from past projects. \codeName{Futures} also capture thrown unchecked exceptions, which makes handling them a little easier. Hence we’re interested in passing around \codeName{Future[E \either A]} around, for a sealed type hierarchy \codeName{E}. There is also the issue of the java libraries used (for SQL and LMDB access) throwing exceptions, and unexpected exceptions turning up in code. Fortunately, the  Future container catches these, acting like an asynchronous \codeName{Try[E \either A]}. This causes issues as we don’t have the sealed trait property of errors as we have above. To solve this, I introduced the \codeName{ConstrainedFuture[E, A]} monad, which has the requirement that the error case type parameter \codeName{E} implements the \codeName{HasRecovery} typeclass for converting any \codeName{Throwable} to an \codeName{E}.

\todo{Hashrecovery typeclass}

The underlying future is kept private, and can only be accessed via the run method, which calls the recover method on any errors (tail recursively, so any exceptions thrown during execution are also handled). By this construction \todo{appendix} we ensure all non-fatal error cases are contained in a type-safe way.

	\subsection{Operation Monad}
	As stated, \todo{Can I link to it?} typical database operations take a view as a parameter, inspect the view, return a value and may also insert a new view. This requires interplay between the \codeName{ConstrainedFuture} (To handle failure and asynchrony) and \codeName{State} (to chain together updates to the view representing current state) monads. Hence we use the \codeName{Operation[E, A]} monad, which wraps a function \codeName{(ViewId => ConstrainedFuture[E, (ViewId, A)])} in a similar way to how \codeName{State} monad chains together functions \codeName{S => (S, A)}. Each of the commands on a database yields an operation.

	
	\subsection{Local and Global State}
Although it would be preferable to only use purely functional folds, maps, and immutable data structures everywhere within the project, for certain, high frequency, performance critical tasks, using purely immutable structures slows us down. Recursive functions (though my functional style does not make heavy use of them anyway) tend to use more stack space than the JVM has available in many situations. Hence for tasks such as retrieving values for result sets,  pathfinding across large relations, and building indices, I have opted to make use of mutable data structures locally, using builders for collections such as sets. This leads to a dramatic increase in speed, especially for when large numbers of elements are added sequentially \todo{Source?}. In these cases, the mutable state never leaks out of the functions that make use of the mutability.	
	
\section{Schema Implementation}
	One of the goals of the project was to allow close to arbitrary user objects (assuming that they are finite) to be inserted and retrieved from the database. This was achieved using the typeclass pattern.
	\subsection{Schema Hierarchy}
	In order to work with the database, a class needs to have an instance of the \codeName{SchemaObject} typeclass.
	\todo{definition of schema object}
	
	\codeName{SchemaObject} is sealed, so can only be implemented by implementing one of its subclasses. Currently, for the sake of simplicity, there are five: \codeName{SchemaObject0} .. \codeName{SchemaObject4}, with the number indicating the number of underlying database fields required. These are implemented by effectively defining marshalling functions from the type to and from tuples of “\codeName{Storable}”  (read: primitive) types.
	
	\todo{To go in the appendix: The definition of a schema object subclass}
	
	\subsection{DBObjects}
	
	To store objects in the database under various backends, we need to have a type erased (at compile time, but not runtime) version of the objects. Once we have a primitive representation of an object from the \codeName{SchemaObject}, we can fully unerase it by converting it to a \codeName{DBObject}. A \codeName{DBObject} is simply a collection of type tagged fields (\codeName{DBCell}). These can now be easily inserted or retrieved from a database.
	\todo{DBObject and cell definitions}
	
	\subsection{Unerasure}
	In order to correctly retrieve values from the database, we need to be able to undo the erasure process. This can be done using the unmarshalling methods derived from the \codeName{SchemaObject}  for an object type.
	\subsection{Relations}
	    In order for type checking to work, relations need to have type parameters for the object types they link. Hence, to define relations in the schema, the user needs to define objects that extend the relation interface.
	  
	    \todo{Relation definition}

	
	\subsection{SchemaDescription}
	
In order to build the database structures the backends need a definitive collection of schema to include. This is done using a \codeName{SchemaDescription} object, which simply holds a collection of \codeName{SchemaObjects} and \codeName{Relations} which need to be used by the database.	
	
	\subsection{Findables}
	For the sake of simplicity, I have only implemented findables which test if fields of objects match particular values. This allows us to look for specific objects or match particular fields. This also makes indexing easier to implement.
	
\section{Query ADT}
	\todo{This feels like it needs rewriting}
The ADTs described in the preparation section \todo{link}	are implemented in scala each by a pair of ADTs: a typed and type-erased equivalent for each. The typed ADTs are parameterised by the object types which they lookup. Using this parameterisation, I have encoded the type of the typed ADTs such that the scala compiler checks the typing of the any generated query and does type inference for us, according to the type rules of the query language. \todo{Appendix: Class definitions of ADT} The only rules that cannot be checked at compile time are whether the schema description contains the relation in instances of the (\codeName{Rel}) rule, the type \codeName{A} for the (\codeName{Id}) rule, and the (\codeName{Find}) rule, as we cannot predict the contents of the SchemaDescription at compile time without dependent types.  These typed ADTs are erased,  with respect to a schema,  into their unsafe equivalents in order to be executed. If an AST node makes reference to a non-existent table or relation, a runtime error is created in the \codeName{Either} return type. The constructors of the type-erased ADT nodes are private to the enclosing package, meaning that they can only be created by erasing a typed ADT.
	
\section{Commands}
As specified in the previous chapter, each backend needs to implement five commands: 
\begin{itemize}
\item \codeName{find(S)}
\item \codeName{findPairs(P)}
\item \codeName{ShortestPath(start, end, P)}
\item \codeName{allShortestPaths(start, P)}
\item \codeName{insert(relations)}.
\end{itemize}
Each command should return an \codeName{Operation} of the correct type.
\section{DSL}
The DSL mostly consists of syntactic sugar to make queries easier to read. It is implemented using the type enrichment pattern. Both \codeName{Relation} and \codeName{FindPair} implement the trait (interface) \codeName{FindSingleAble}, so we can use type enrichment to write new DSL opertors. The main thing to note in the DSL is the use of arrows to chain relations together. (See \codeName{RelationSyntax.scala} in the appendix for examples of DSL). \todo{Put in appendix}

\section{Common Generic Algorithms}
During construction of the database, several common patterns of problems emerged with slight differences. Hence, I have written relatively optimised generic versions of these algorithms such that different backends can make use of them regardless of the underlying types. These algorithms are found in \codeName{core.utils.algorithms}


	\subsection{Simple Traversal}
	The first set of generic algorithms to look at are the \codeName{SimpleFixedPointTraversal} algorithms. These compute the repetitions of a function for execution of the \codeName{FixedPoint}, \codeName{Upto}, and \codeName{Exactly} expressions of the ADT. They are labelled as “Simple” because they do the search from a single root. They carry out search mutably, and convert their output to an immutable set upon returning.
	
		\paragraph{Exactly}
The simplest algorithm is for computing \codeName{Exactly} query nodes. Here, we simply expand a fringe set of values outwards, by applying the search step to every value in the fringe to get the new fringe. We also memoise the search step function in the case of repetitions. After the required number of repetitions, the remaining fringe is returned. \todo{(Diagram showing expanding fringe)}
    
    		\paragraph{Upto}
The next algorithm is for computing \codeName{Upto}. This is computed in a similar way by flat-mapping the functions over the fringe repeatedly to calculate an expanding fringe. The major differences here are that we keep an accumulator of all the found values. When a new fringe is calculated, we subtract the accumulator set from it to reduce the number of nodes that need to be searched to those that have not yet been searched, and then union the remaining fringe with the accumulator to get the new accumulator. After the required number of repetitions, the accumulator is returned. \todo{diagram}
	
		\paragraph{FixedPoint}
The  final algorithm is to calculate \codeName{FixedPoint}. This works slightly differently. As before, we keep an accumulator of reached nodes, but unlike before, the generation number of each node is not important, only that a node is reachable is important. Hence, the fringe is a queue rather than a set, and we iterate until the fringe is empty. In each iteration, we pop off the top value of the fringe queue, compute all immediately reachable nodes. This reachable set is diffed (define diff) with the accumulator to find the newly reached nodes. These are now added to the fringe queue and the accumulator. When the fringe is empty, we return the accumulator.
	
	\subsection{Full traversal}
		The next set of algorithms build on the \codeName{SimpleFixedPoint} algorithms to return not just those nodes reachable from a single root, but the set of all reachable pairs with the left hand pair derived from a root set (using the left-hand optimisation). As such, the algorithms need to do some more work to reconstruct which nodes are reachable from each root, while still eliminating redundancies.
		\paragraph{Exactly}
The first such algorithm is for computing \codeName{Exactly}. This is similar to the original version, except we now store a fringe for each root in a map of \codeName{Root => Set[Node]}. We also memoise the search function in a Map to avoid computing it redundantly. In each iteration, we simply expand the fringe for each root as before by mapping the fringe expansion loop body over the values of the fringe map. \todo{Diagram}
		
		\paragraph{Upto}
The next algorithm is to compute \codeName{Upto}. This is again done like before, but with a map of root to accumulator set as the accumulator. As with \codeName{Exactly}, the fringes of each root are expanded simultaneously, sharing redundant results via the memo, while the accumulators are unioned with the fringe of the appropriate root with each iteration. \todo{Diagram}
		
		\paragraph{FixedPoint}
Finally, the FixedPoint take a departure from the parallel implementations above. The reachable set of each node is calculated sequentially using a similar algorithm to above. However the memo now contains all nodes reachable from previously processed roots, allowing for fast convergence of dense graphs.	
	
	\subsection{Pathfinding}
	The \codeName{ShortestPath} and \codeName{AllShortestPaths} commands require us to search a subgraph generated by a relation. Since all edges have unit weight, the pathfinding algorithm reduces to breadth-first-search, which I have implemented in an imperative format while wrapping up error cases in an Either. These functions take as a parameter the search step \codeName{A => E \/ Set[A]} which represents the edges going out of a node.  \todo{Is this needed/clear?} There is room for optimisation here, since most most backends focus on returning sets of pairs rather than just the right hand side.
	\subsection{Joins}
	The problem of joining two sets of pairs based on shared intermediate values is a requirement for any backend. \todo{Latex definition of a join}  I have implemented a simple hash join \todo{(http://www.csd.uoc.gr/~hy460/pdf/p63-mishra.pdf)}. This operates by first assuming that the size of distinct leftmost elements in the right set is smaller (known to be a subset of, due to the left-optimisation of backends) than the rightmost elements of the left set. We then build a mutable map to index leftmost values to rightmost values. We then traverse the left set and for each pair generate all new joined pairs, using a \todo{explain that flatmap == bind?} flatmap to collect the values into one set. An issue with this approach is that we have to build the index upon each join call, an issue that is addressed later.
	
	\todo{Join algorithm code?}
	
\section{Views and Commits}
In the memory backend, as will be explained later, views are easy implement as a map of \codeName{ViewId} to \codeName{MemoryTree} and simply updating the \codeName{MemoryTree}, allowing Scala’s immutable collections to handle sharing of data in an efficient way. In the other backends, backed by non-functional technologies, we need other methods of sharing and inserting to immutable structures. A first observation is that with the operation monad model, each view only has one direct predecessor, forming a tree where we’re only interested in the path back to the root from a given node. From this insight, we can think of each new view only needing to store a difference against its parent. Since the current design of the database only allows the addition of values and not deletion, this difference is only going to be positive. Hence, we can store all the added values between two views in a container called a \codeName{Commit}. As an optimisation, we can disregard the parent view, and simply store each view as a collection of its commits. This would also allow us to implement deletion if desired. This would be done by removing commits in a view that are to be modified, and replacing them with a new commit containing the contents of those commits, excluding the deleted values.

\section{Memory Backend}
The first backend that I have implemented is a simple, naive memory-based backend. This backend follows the denotational semantics, and makes very few attempts to improve performance. This backend serves as a test-bench backend, used to create unit tests to test other backends. It also allowed me to practice implementation of type erasure and unerasure according to the schema in a controllable environment (that is, without interference from other languages as libraries as in the SQL and LMDB backends.)
	\subsection{Table Structure}
	A memory instance stores a concurrent map of \codeName{ViewId} to \codeName{MemoryTree}, which itself is a map of \codeName{TableName} (derived from the \codeName{SchemaDescription}) to \codeName{MemoryTable}. There is a \codeName{MemoryTable} for each object class in the \codeName{SchemaDescription}. A \codeName{MemoryTable} provides lookups using maps for \codeName{DBObjects} and \codeName{Findable}s, in the form of an index to set of objects for each column value. These lookups return objects containing a \codeName{DBObject} and the outgoing and incoming relations for the object, indexed by \codeName{RelationName}. 
	\todo{Diagram of View => Tree(TableName => Table(Index => Object)) }

	
	\subsection{Reads}
Reads occur by simply walking over the ADTs recursively, following the denotational semantics \todo{Code is small enough to place in an appendix}. The only real departures from the denotational semantics are the Left-optimisation and use of the generic fixed point algorithms to compute \codeName{Exactly(n, P)}, \codeName{Upto(n, P)}, and \codeName{FixedPoint(P)}. Results are also computed in the \codeName{Either} monad to allow for error checking (for cases such as missing tables).
	
	\subsection{Left Optimisation}
	One of the few optimisations here is the Left optimisation. When we compute the result set of a \codeName{FindPair}, we pass in the subset of left hand side variables we want to compute from. This mostly has an effect when we compute joins (\codeName{Chain}s). Consider joining a query of maybe a few dozen unique rightmost values to a large query with several million unique leftmost values. The pairs of the right relation only need to be joined if their leftmost value is in the set of rightmost values of the left relation. Hence it makes sense to pre-limit our search to pairs with leftmost values in the right most value. \todo{(Diagram showing redundant calculations in the join)}. This pattern also makes an appearance in the original LMDB implementation.
	\subsection{Writes}
	Thanks to Scala’s immutable collections library, updating the database is fairly easy When inserting a collection of relations, we simply add, relation by relation, each object in the relation, if it doesn’t exist, and update the outgoing and incoming relation map of each object. This update creates new immutable object tables and a new memory tree. This is stored to the map of \codeName{ViewId}  to \codeName{MemoryTree} as a new view.

	\subsection{Storage}
	Objects are stored as \codeName{DBObject}s in wrapper \codeName{MemoryObject}s. \codeName{MemoryObject}s  are hashed and compared by their \codeName{DBObject}s. \todo{Memory object diagram}
	
	\subsection{Mutability}
	The only mutability in the Memory implementation is for the views counter and the views lookup table. Access is kept transactional using a lock.

	\subsection{Pathfinding and fixed point traversal}
	The pathfinding and fixed point traversal methods are simply implemented by calls to the appropriate generic algorithms. One redundancy is that the search functions passed calculate all linked pairs, rather than optimising only to look at right hand sides of relations. \todo{(more detail?)}


\section{PostgreSQL backend}
Upon completion of the initial memory backend, I started work on a PostgreSQL based backend. This compiles the ADT intermediate representation into SQL queries that are then executed by a Postgres database.

	\subsection{Table Structure}
	The construction of the underlying database uses several SQL tables. These can be partitioned into a set of control tables that will be present in all database instances and a set  of schema defined tables.

		\paragraph{Control Tables}
		\todo{Insert the table from GDrive}
\note{Note: \codeName{ViewId} is a foreign key into the views registry, \codeName{CommitId} is a foreign key into Commits Registry. The Dummy column is needed to fix postgresql syntax when we try to get the next key for a table with one column.}		
		
		\paragraph{Schema defined Tables}
		\todo{Insert the table from GDrive}
		\note{Note: Object-, Left-, and Right- Ids are foreign keys to the \codeName{ObjectTable}’s ObjectId column.}
	\subsection{Query Structure}
	\subsection{Monadic Compilation}
	In order to generate these SQL queries, we need to convert the ADT query to a lower level intermediate representation \todo{In appendix}, which maps one-to-one to SQL. While doing this, we need to gather and rename all of the relation and auxiliary tables that we extract from so that we can form the CTE queries, we also need to find repeated queries to hoist out. In order to do this, we use the monadic compiler pattern described above. The compiler state is shown below \todo{name the figure}. When the compilation is done, depending on the context of the command, we append different extraction queries. For find pairs we need to extract the fields of both the left hand side and the right hand side objects, whereas for pathfinding operations, we only need to extract the \todo{ObjectId}s along the path as opposed to whole  objects. \todo{Compilation context figure from GDrive}

	\subsection{Writes}
	There are several steps in the implementation of writes, though I have not expended  a great deal of effort making them fast. A significant part of this is the “insert or get” SQL query, which looks up an object in the relevant table, returning its \codeName{ObjectId} if it exists and creating the object and returning the new ID if it does not.
	
	\todo{insertOrGet query from GDrive}
	
	We create a new view and commit, then we run a memoised \codeName{InsertOrGet} over all the leftmost objects to be inserted, then all the right objects to be inserted, yielding tagged relations between \codeName{ObjectIds}. For each inserted relation, the existing relation instances are removed from those to insert, and the remaining inserted to the relevant \codeName{RelationTable} with the correct \codeName{CommitId}. The auxiliary tables are now updated and, on success, the views table is updated.

	\subsection{Mutability}
	As with the memory backend, all mutability except for the availability of views and the default view is hidden from the user. The SQL backend uses commits to manage view mutability.
	\subsection{Pathfinding and Fixed Point Traversal}
Pathfinding is implemented by constructing an SQL query to generate right hand side \codeName{ObjectIds} for a relation given a left hand side \codeName{ObjectId}. This query is used as the search function for the generic pathfinding algorithms. Once paths have been found, their \codeName{ObjectIds} are looked up in the database to find the full values along each path.

Fixed point traversal and repetitions are done natively in SQL. \codeName{FixedPoint} and \codeName{Upto} are done using a recursive CTE, while \codeName{Exactly} is done by explicitly joining together the required number of repetitions of the sub-relation’s query.	
	
	\todo{Recursive CTE example. Caption = An example of a recursive CTE computing \codeName{Upto}. \codeName{FixedPoint} would omit the counter variable and the limit.}
	
	\subsection{Object Storage}
	Objects are stored as in the appropriate \codeName{ObjectTable} in a manner derived from the \codeName{DBObject} of each object. Each \codeName{DBCell} is converted to appropriate SQL type.

\section{LMDB Backends}
The final family of backends are the LMDB backends. LMDB is a simple, efficient memory mapped file based Key-Value datastore. \todo{(https://symas.com/lmdb/)}. It is already the backend for DBMSs \todo{(Source?)}.

	\subsection{Common}
	Although I have written several versions of the LMDB backend, there are several overarching similarities.
		\paragraph{LMDB API}
			\subparagraph{Terminology}
			LMDB terminology differs slightly from that of more mainstream systems. What would be called a database in SQL is known as an \term{Env} and a table known as a \term{DBI}.
			\subparagraph{Transactions}
				LMDB’s transactional model differs from more traditional systems. In short, we need to create a transaction to do reads but not writes. This stems from LMDB delegating as much work as possible to the OS kernel's memory-mapped file system. All writes are immediately written to the database, whereas starting a read transaction gives a view of the \term{Env} as it was at the start of  the transaction, ensuring that concurrent writes don't affect what is read from the database. Writes may be included in transactions to allow atomic get-and-sets. In practice the transactions we actually use are very short lived.
			\subparagraph{JVM API}
			The LMDB backend uses the \codeName{LMDBJava} API \todo{https://github.com/lmdbjava/lmdbjava} to allow access to an LMDB Database from JVM languages (such as Scala). For each \term{DBI}, we have a byte array to byte array key-value map, much like a \codeName{Map[Array[Byte], Array[Byte]]}.
			
		\paragraph{Storage and Keys}
			In order to write to the database using the LMDBJava API, we need to convert the key and value to \codeName{ByteBuffer}. To generify this process I have introduced two type classes: \codeName{Keyable} and \codeName{Storable}.
			
			\todo{figures for these type-classes}
			 \codeName{Keyable} is a type class which shows that an object can be converted into an array of bytes representing a component of a key (Can be concatenated with others to make a full key). \codeName{Storable} is a more general typeclass that shows that objects can be converted into a \codeName{ByteBuffer}. \codeName{Storable} objects are typically more complex than objects used as keys, and have a higher marshalling throughput, hence the lower level, faster, \codeName{ByteBuffer} API. Typeclass instances exist to allow sets and lists of \codeName{Storable} objects to be stored.	
			
		\paragraph{Table Structure}
	
 Similarly to the SQL implementation, there are several tables, some generated based on the \codeName{SchemaSescription} and other, control, tables exist regardless. Each table is represented in memory by a subclass of the \codeName{impl.lmdb.common.tables.interfaces.LMDBTable} trait, which provides utility methods such as transactional reads and computations.
 
 \todo{Import control table table from G-drive}
 \note{Singleton Keys denote tables with single key.}
 \todo{Import schema table table from g-drive}

For each \codeName{SchemaObject} in the \codeName{Schemadescription}, there exists a \codeName{RetrievalTable}, \codeName{EmptyIndexTable}, and a \codeName{ColumnIndexTable} per column in the \codeName{SchemaObject}.
		
		
		\paragraph{Writes}
		Writes occur in a similar way to the SQL implementation, again using commits to manage views. First we lookup all the left and right hand side objects to find their \codeName{ObjectIds} (If needed, we create new entries in the retrieval table and update the index tables to insert the objects.), we then look up existing instances of the relations to insert and only insert new ones to the relation table with the new \codeName{CommitId}. Upon success, we create a new view and insert it to the views table and the available views table.

	\subsection{Original LMDB Implementation}
	\note{impl.lmdb.original}

The original LMDB backend implementation executed queries in much the same way as the in-memory backend, the only difference being the flat LMDB table structure. Interpretation occurs by a very similar to function that of the memory backend, instead passing around a list of Commits to search rather than the memory tree. At the end of a query’s execution, the resulting \codeName{ObjectId}s are looked up in the relevant Retrieval table to extract the actual objects and convert them into user objects. Pathfinding and the repetition operators are handled in the same way as the memory backend by calling the relevant generic algorithms.
 

	\subsection{Batched}
	\note{impl.lmdb.fast}
	This is the first variant upon my simple original LMDB backend, it focuses on making small local optimisations rather than significant algorithmic changes.
	
		\paragraph{Read Batching}
		Firstly, the original implementation made suboptimal uses of reads. For example, when extracting objects at the end of query execution, it would lookup each object individually with its own read transaction. This is sped up by batching together reads to separate keys and commits in the same transaction, allowing for a small speed up.
		
		\paragraph{Pre-Caching}
		The result of executing a \codeName{FindSingle} is constant throughout the whole query, so by lifting out and pre-emptively executing any \codeName{FindSingle}s, we speed up execution by removing redundancy. More importantly, this helps enable the next operation.
		
		\paragraph{FindFrom}
		For the pathfinding commands, the transitive queries and \codeName{From} \codeName{FindSingle} query, we’re not actually interested in finding all pairs matching a query, or interested in which pairs are related, but actually the reachability function \codeName{$A \Rightarrow Set[B]$} (this is a kleisli arrow), which takes a node and finds the directly reachable neighbours. Since we don’t have to do any bookwork to maintain the left hand side of any relation, this greatly simplifies our search. Hence, in these cases, we interpret the query using a new \codeName{FindFrom} method, which is an alternative interpreter. For example, joins now become a concatenation of these arrows (\codeName{flatMap}  in Scala parlance), \codeName{Id} becomes the return function \codeName{($x \Rightarrow Set(x)$)}, \codeName{Distinct} becomes set subtraction as opposed to calling the \codeName{Set.filter} function, and, finally, we can make use of the \codeName{SimpleFixedPointTraversal} algorithms described above. The pre-caching above is useful here, since due to the \codeName{flatMap}s, the computation of \codeName{FindSingle}s would be repeated many times otherwise.

	\subsection{Common Subexpression Elimination}
	\note{impl.lmdb.cse}
	A common performance issue in the previous implementation was that of redundancy and lack of general common subexpression elimination (CSE). When a common sub expression is repeated outside of an \codeName{Upto} or \codeName{Exactly} block, then it is repeatedly recalculated, leading to poor performance. In addition, there is lots of redundancy exposed by the above \codeName{FindFrom} optimisation. Consider calculation $R join (S join T)$. If a node $n$ appears in the results of $S(m)$, $S(m')$ for distinct $\opRule{(?, m)}{A, B}{R}$, $\opRule{(?, m')}{A, B}{R}$ ($m$, $m'$ are right hand results of R), then $T(n)$ is computed redundantly in the overall calculation. These issues of redundancy can be solved using a combination of global CSE and memoization.

		\paragraph{The Memoisation Problem}
		When looking at a graph data structure, memoization is harder than when looking to optimise pure functions. To optimise a pure function $f\colon A \Rightarrow B$, we only need to store a hash table \codeName{Map[A, B]}. To compute $f(a)$, we simply look up $a$ in the map. If it is in the map, we return the mapped value else, we call $f$, and add the result to the map before returning. In a large graph, computing and storing all pairs generated by a query is extremely wasteful, since many of those pairs might never be used (effectively, we would have ignored the left-optimisation). Conversely, memoizing the full left-optimised function, $interpret(Q)\colon Set[A] \Rightarrow Set[(A, B)]$ is also wasteful, since there may be overlap in the sets used as keys. Instead, we memoise the \codeName{FindFrom} function of a query, \todo{get this findfrom to be verbatim}$findFrom(Q)\colon A \Rightarrow Set[B]$, and reconstruct results from the pairs. This allows for the best overlap.

To make the best use of memoization, and to overusing memory by memoizing the same query twice, we need to also apply CSE to group together instances of the same query.

		\paragraph{Retrievers}
		A solution to these parameters is the use of an object called a \codeName{Retriever}. This exposes two methods which mirror those from interpreting a query. 
		\todo{Insert retriever trait here from G-Drive}	
		For each subquery node in a query(e.g. \mathName{And(Id_A, Or(R_1, R_2 ))}), we generate a retriever. Retrievers for primitive relations (e.g. \mathName{Id_A} , \mathName{Rel(R)}) are uncached, as LMDB allows for very fast re-computations of these. In the other cases, we use a CachedRelationRetriever, which memorises an underlying lookup function. Using type-enrichment, I have implemented methods such as \codeName{join}, \codeName{and}, \codeName{or}, \codeName{exactly}, \codeName{fixedPoint} on \codeName{RelationRetrievers}, allowing them to be composed to mirror the query they are generated from. This memoises every node of the query to be executed, yielding a significant reduction of redundancy.
	
		
		\paragraph{Monadic Compilation}
		To avoid storing multiple \codeName{RelationRetriever}s for the same subexpression, it is useful to reuse the same retriever for every occurrence of a subquery. Hence when building a retriever for a query, we want to keep track of all subqueries we’ve seen before. This can be done using the monadic compilation pattern above. The compilation state stores a hashmap of all the subtrees that have already been computed (A bit like when constructing a binary decision diagram in automated theorem proving). 
		
		\todo{Compilation state from G-Drive}
		\note{Compilation state}
		The compilation function checks the memo for precomputed values, and if they are not already computed, recurses to compute subtrees, then composes the subtree results to get a new retriever for the current node, which is added to the compilation state.
		
	\subsection{Complex Common Subexpression Elimination}
	\note{impl.lmdb.fastjoins}
Despite these optimisations, there are still sources of redundancy.

		\paragraph{Index Building}
			Firstly, the original CSE implementation uses the excessively generic join function to join result sets. This function wastes time by computing an index map on every call. So instead we can change the definition of a \codeName{Retriever}'s public functions to mitigate the need to build indices.
			\todo{new retriever trait
			Indeed, the function to join a pair of retrievers becomes a simpler \codeName{flatmap} of one map to another. Functions such as the \codeName{union} and \codeName{intersection} of retrievers also become simpler, \note{(see impl.lmdb.fastjoins.retrievers.RelationRetriever.RelationRetrieverOps)}
}
		\paragraph{Exactly}
		This implementation also explores other formulations of the \mathName{Exactly(n, P)} query. Exactly effectively joins together n repetitions of the underlying query in a linear fashion. This makes relatively poor reuse of the join function, since each join has different operand subqueries, so it cannot be memoised. This formulation does make good reuse of \mathName{P}, however.
		
		\todo{Diagram showing n joins of P, linear}
		
		
		As joins over the set of results of query form a \mathName{Monoid} \todo{Proof in appendix or in evaluation?}, we can use associativity to change the order in which the joins are evaluated, aiming for as much reuse as possible. This can be done in a manner similar to binary exponentiation \todo{(http://computingonline.net/computing/article/viewFile/229/204)}.

We calculate retrievers for $P^{2^i}$ for each $i$ less than the bitlength of $n$. We then join the relevant retrievers to to get a retriever for $P^n$. This only uses $O(Log_2(n))$ distinct joins, and makes very good reuse of the join functions. \todo{Diagram showing join structure;} \todo{Algorithm}

		\paragraph{Upto Optimisation}
			We can also re-formulate an \mathName{Upto(n, P)} as an \mathName{$Exactly(n, Or(P, Id_A))$} \todo{Proof in appendix}, and hence use the same optimisation as above on \mathName{Upto}.  The same cannot be said for \mathName{FixedPoint}, as we do not know statically at what point the underlying relation will converge, so cannot pick an n to compute \mathName{Upto(n, P)}.



\chapter{Evaluation}
\section{Unit Tests}
The correctness of each backend is tested using a suite of 45 unit tests which verify adherence to semantics. These have  wide range, testing over all the commands, all the possible ADT nodes, and the correct usage and separation of views. I have been using the regression test model, in that discovering a non-trivial bug, I’ve written a test case to target that bug, ensuring that it is not leaked into production again.
\section{Performance Tests}
	\subsection{Introduction}
	In order to evaluate the effectiveness of the various optimisations to the LMDB backends described above, it was necessary to run performance tests over wide range of queries. The various LMDB backends were tested against each other as a control and particularly against the SQL backend as a standard to beat. The in-memory backend was omitted as initial tests indicated that it runs with approximately the same algorithmic characteristics as the Original LMDB implementation. The LMDB implementation was able to do indexing faster than Scala’s tree based hash maps, which are used by the in-memory implementation. Further more, the original LMDB implementation is typically around 3x slower than the batched version for most jobs, so we also omit it for most tests, since most of the algorithms it uses are the same.
	\subsection{Hardware}
	Tests were run on the oslo machine belonging to Timothy Jones’ group. Its specifications are shown below. All of the backends ran off of an SSD.
	\todo{Oslo Specs}
	\subsection{Datasets}
	To evaluate tests on non-trivial examples, I sought to construct datasets over which large queries were feasible.

		\paragraph{IMDB}
		The first and most used collection of tests are derived from the IMDB movie database,the most popular 5000 of which were collected from kaggle \todo{https://www.kaggle.com/tmdb/tmdb-movie-metadata}

The CSV data was processed using a python script (\todo{in src/resources/imdb/}) into a simplified JSON format. A separate, larger dataset was constructed from tests for another graph database. \todo{https://github.com/arangodb/example-datasets/tree/master/Graphs/IMDB} and manually (in python) converted to the same JSON format. I then wrote a Scala script which reads the JSON and writes the relations to a given database instance.

Slightly different parameters were given when generating each dataset.
\todo{Table from GDrive with all the data}

The objects in the database are as follows:
\todo{Object and relation tables}

		\paragraph{UFC}
		
		A second dataset, built from UFC fight data, \todo{https://www.kaggle.com/cformey24/ufc-fights-data-1993-2232016/data} was constructed in the same way to produce one JSON dataset.
		
		\todo{object and relation tables}
		
		The \codeName{ShorterThan} and \codeName{LighterThan} relations produce extremely sparse graphs which take a long time to converge under transitive closure. \todo{Size of graph.}
	\subsection{Test Harness}
	In order to run tests against each other, I have written a typesafe test harness to run on oslo. This standardises the interface that individual test instances must implement. Each Test must have a \codeName{setup} method and a \codeName{test} method which is run on a \codeName{DBInstance} with the test method indexed by a \codeName{TestIndex}. The test specification must give a maximum index and a mask to avoid running inappropriate backends (For example, those that might take too long on a large test.) The benchmarks that I have run test only the read speed. The time taken to construct the database is not included.
	
	\subsection{Results}
		\paragraph{Overall Picture}
		\todo{results;}
		\todo{This needs to be more specific, rewrite}
		An overall view of the results is that, as might be expected, the SQL implementation is the fastest, with the most aggressively optimised LMDB typically performing the closest in speed to SQL, \todo{calculate precise values} though occasionally beating SQL’s performance by up to 150\%. Due to a high overhead of initiating individual queries, the SQL instance performed appalling on pathfinding queries. The LMDB optimisations worked very effectively where they were hypothesised to, such as queries with repeated joins and those which made use of \mathName{Exactly} and \mathName{Upto}, yielding orders of magnitude speedup. However, in some tests, the optimisations yielded overheads slowing down performance, especially those queries which make use of \mathName{FindFrom} rather than finding pairs.

Typically, the CSE optimisations alone do not provide a statistically significant speed up, or even slow down operations, however the optimisations also applied to joins and transitive queries do often show a large speedup. 

		The pathfinding implementation, for simplicity, sends queries at each stage of the breadth first search. This seems to particularly punish the SQL implementation, to the extent that it is around 2000x slower than the LMDB reference implementation. Stateful solutions to speed this up might include creating a temporary table in the database to store the subgraph being searched, but this complicates the SQL implementation significantly (clearing up on error becomes a lot harder). Hence, I am ignoring the pathfinding tests for the purpose of backend comparison.


		\paragraph{Redundancy}
			This test is to demonstrate the removal of redundancies in \codeName{FindSingle} commands. There is clearly a major increase in speed due to the introduction of CSE and memoisation. Since this is a \codeName{FindSingle} query, there is no usage of the optimised join formulations, so there is no speed gain by the most optimised LMDB backend. Postgres fails to fully optimise away the redundancy, even with use of the \note{DISTINCT} modifier of queries, and ends up running out of temporary space on this query.
			
			\todo{Graph}
			
\resultTable{Redundancy}{4 each}{\todo{queries}}{ \reference   	& 	381,972 & 1\\
 \cse 			&   379,280 & 0.993 \\
 \batched 		&	9,318,741 & 24.4 \\
 \postgres    	&	Did not finish & Did not finish \\
 } 

		\paragraph{Conjunctions and Disjunctions}
	These two tests make use of the \mathName{And} and  \mathName{Or} operators to combine several subqueries with some underlying common structure. Both resulted in similar performance patterns. The postgres implementation significantly outperformed all LMDB implementations, while the CSE related optimisations appeared to add overheads greater than the redundancy they removed. I expect the reason for this is that there is still not a large amount of redundancy exposed by the underlying sub queries, and that the implementation of unions and intersection in Scala sets is slower than the optimised C implementation used by postgres. In addition, constructing a conjunction and disjunction over the memoisation in the LMDB implementation also adds some overhead and reduces locality of reference.	
				\todo{Graph}
\resultTable{Conjunctions}{40 each}{\todo{queries}}{ 
 \reference 			&   879,984 & 1 \\
 \cse 					&	827,734 & 0.941	\\
 \batched 				&   791,678 & 0.900 \\
 \postgres    			&	106,165 & 0.121 \\
 }
				\todo{Graph}
	\resultTable{Disjunctions}{40 each}{\todo{queries}}{ 
 \reference 			&   853,664 & 1 \\
 \cse 					&	824,799 & 0.966	\\
 \batched				&   800,455 & 0.938 \\
 \postgres    			&	112,488 & 0.132 \\
 }	
		\paragraph{Tests that involve repetitions}
		These are a suite of benchmarks for testing the optimisations and performance on repetitive queries. A general overview is that the join-reordering optimisations strongly sped up benchmarks which ran \codeName{FindPairs} queries, but in \codeName{FindSingles} queries, the overhead of this formulation did not compete as strongly with the fast \codeName{SimpleFixedPointTraversal} methods. SQL performed strongly in \mathName{Upto} queries which use built in, optimised, recursive CTEs to calculate results, but less strongly in \mathName{Exactly} queries which do not have a built-in formulation.

			\subparagraph{Exactly Test}
			This test runs \codeName{FindSingle} \mathName{Exactly} queries of varying lengths over the small movies database. As explained, the \codeName{SimpleFixedPointTraversal} \codeName{exactly} method is faster than any gains by the join optimisations in the optimised LMDB version. Postgres also performed poorly due to the lack of a recursive CTE for self joins.
						\todo{Graph}
	\resultTable{Exactly}{50}{\todo{queries}}{ 
 \reference 			&  	1,752 	& 1 \\
 \cse 					&	1,798 	& 1.03	\\
 \batched				&   1,513 	& 0.834 \\
 \postgres    			&	102,461 & 58.5 \\
 }	
			\subparagraph{Exactly Pairs}
This test instead runs a \codeName{FindPair} variable length \mathName{Exactly} query over the small movies database. Since this test involves keeping track of the root for each pair found, the \mathName{Exactly}-optimised LMDB implementation significantly outperforms the other LMDB implementations based on generic algorithm. Postgres also performs well in this test.
			

			\todo{Graph}
	\resultTable{ExactlyPairs}{50}{\todo{queries}}{ 
 \reference 			&  	83,269 		& 1 \\
 \cse 					&	1,792,388 	& 21.5 \\
 \batched				&   1,787,390 	& 21.5 \\
 \postgres    			&	107,797 	& 1.29 \\
 }	


			\subparagraph{UptoTest}
			
				This test runs \mathName{Upto} based \codeName{FindPair} queries over the large IMDB database. Here the upto-optimisation of the final LMDB backend provides significant speed-ups, such as 225x on the other LMDB backends and 1.5x over postgres.
						\todo{Graph}
				\resultTable{Upto}{10}{\todo{queries}}{ 
 \reference 			&  	126,448 	& 1 \\
 \cse 					&	28,491,910 	& 225 \\
 \batched				&   28,423,658 	& 225 \\
 \postgres    			&	197,880 	& 1.56 \\
 }	
			\subparagraph{UptoLarge}
			
				Since the more naive LMDB instances took seven hours each to complete the above test, I re-ran the benchmark for only the reference and Postgres implementations for a longer period of time, yielding a similar ratio of performance.
			\todo{Graph}
				\resultTable{Upto}{10}{\todo{queries}}{ 
 \reference 			&  	1,059,091 	& 1 \\
 \postgres    			&	1,557,314 	& 1.47 \\
 }	
	
			\paragraph{JoinSpeed}
				This test is  to demonstrate the cost of joins, we interleave queries that calculate two subqueries and then combine them using, firstly, a relatively fast And and, secondly, instead, a higher complexity join. In theory, the difference between these two types of queries should give us an idea of the cost of a join.
			\todo{Graph}
			\resultTable{Join Speed}{150 each}{\todo{queries}}{
			\reference   	& 	368,320 & 1 	\\
 			\cse 			&   525,992 & 1.43   	\\
			\batched 		&	543,909 & 1.48	\\
			\postgres    	&	289,352 & 0.786	\\}
			
						\todo{More detail}
			\paragraph{Closing Thoughts}
			These benchmarks provide some insights into the strengths and weaknesses of the various optimisations I have written. An option to produce a performant back end might be to produce heuristics for estimating graph density and the relative performance of the various LMDB backends upon it to create an adaptive backend. Since all the LMDB backends use the same underlying database format (indeed the code for constructing and writing to the database remains the same), a backend could choose at query time which strategy it wants to use to execute each individual query.
\section{Semantics Proofs}
	\subsection{Denotational == Operational}
	\subsection{Typesafety}
	\subsection{Join as a Monoid}

	
\chapter{Conclusion}

\section{Successes}
To the best of my knowledge, the graph database system I have built over the last seven months is one of the, if not the, first explicitly purely functional graph database systems in existence. This project is a demonstration that complex, performant software projects can be built using purely functional languages, whilst keeping the benefits of strong type systems. I feel that despite having written the project in Scala, a high level, often inefficient language I have achieved an acceptable level of performance for graph traversal. Although the system would need significant work in areas of security, networked access, and tests and tweaks on real world use cases to become commercially viable, it would be interesting to see if companies known for their use of functional programming, especially finance firms, might find use cases for graph databases

\section{Further extensions}
Interesting extensions to the core project that, given more time, I would have been interested to explore include
\begin{itemize}
 \item An increase in the use of laziness to allow larger queries over larger graphs. Currently, the size of queries are limited by how large the result sets of sub-queries. However, the Postgres implementation typically runs out of temporary file space before this happens.
 \item The implementation of deletions and garbage collection of unused views of the database. Garbage collection in particular might be difficult, as the database is not required to store any state about client instances.
 \item Exploration of  improvement of the schema system using a dependent types library, such as shapeless.
 \item Rewriting the lowest layer of the LMDB backends' interpreters in C or another low level language to extract maximum performance from the LMDB system, without having to construct Scala collections at every step.
\end{itemize}

\section{Lessons learned}
	In hindsight, I might aimed to build a narrower project, focusing on a single backend and comparing  performance against a widely used graph database such as neo4j. Although I very much enjoyed the chance to build a large project in a purely functional style, implementing the large number of moving parts in the project distracted from implementing more interesting optimisations and really digging down to extract all redundancy.

\section{Concluding Thoughts}
I hope for another chance to this space as I feel there are still many areas to explore within it.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{domain \clos}

\todo{Do we split this?}
For the various semantics proofs it is necessary to define the Scott domain $\clos$ for some object type $A$ and $View_{\Sigma}$ $v$. This domain is the set of subsets $x$ such that $\deno{Id_A} \subseteq x \subseteq A \times A$ with bottom element $\bot = \deno{Id_A}$ and partial order $x \sqsubseteq y \Leftrightarrow x \subseteq y$. From this point on, I shall use $x \subseteq y$ to mean $y \sqsubseteq x$.

\hfill\begin{minipage}{\dimexpr\textwidth-1cm}
\paragraph{Theorem: $\clos$ is a domain}


Firstly, by definition, $$\forall x. x \in \clos \Rightarrow x \supseteq \deno{Id_A}$$ hence $\deno{Id_A}$ is the bottom element.

Secondly for any chain $ x_1 \subseteq x_2 \subseteq x_3 \subseteq ...$, $x_i \in \clos$, there exists a value $\bigsqcup_n x_n \in \clos$ such that $\forall i. \bigsqcup_n x_n \supseteq x_i$ and $\forall y. (\forall i. x_i \subseteq y) \Rightarrow y \supseteq \bigsqcup_n x_n$

\paragraph{proof:}

Take $\bigsqcup_n x_n = \bigcup_n x_n$. This is in $\clos$, since both $\bigcup_n x_n \supseteq \deno{Id_A}$ due to $\forall i. x_i \supseteq \deno{Id_A}$ by definition and $\bigcup_n x_n \subseteq A \times A$, by $$\forall a. (a \in \bigcup_n x_n \wedge \neg (a \in A \times A)) \Rightarrow (\exists i. a \in x_i \wedge \neg a \in A \times A ) \Rightarrow (\exists i. \neg x_i \subseteq A \times A)$$ yielding a contradiction if $\bigcup_n x_n \subseteq A \times A$ does not hold.
$\\ $
We know $\forall i. \bigcup_n x_n \supseteq x_i$ by definition, so it is an upper bound. $\\ $
To prove it is a least upper bound, consider $y$ such that $(\forall i.) y \supseteq x_i$ 

\begin{equation}
\label{LeastUpperBound}
\begin{split}
\forall a. (\exists i. a \in x_i) & \Rightarrow a \in y \\
\forall a. \bigvee_n (a \in x_n) & \Rightarrow a \in y \\
\forall a. (a \in \bigcup_n x_n) & \Rightarrow a \in y \\
\therefore \bigcup_n x_n & \subseteq y\\
\square
\end{split}
\end{equation}

\xdef\tpd{0pt}
\end{minipage}

\prevdepth\tpd

\end{document}